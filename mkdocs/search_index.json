{
    "docs": [
        {
            "location": "/", 
            "text": "Beam is a highly-general library for accessing any kind of database with\nHaskell. Currently, it supports two SQL backends, Postgres and SQLite. Work is\nunderway for a MySQL backend. For information on creating additional SQL\nbackends, see the \nmanual section\n for more.\n\n\nBeam features\n\n\n\n\nEasy schema generation\n from existing databases\n\n\nA basic migration infrastructure\n for working with multiple versions of your\n  database schema.\n\n\nSupport for most SQL92, SQL99, and SQL2003 features\n across backends that\n  support them, including aggregations, subqueries, and window functions.\n\n\nA straightforward Haskell-friendly query syntax\n. You can use Beam's \nQ\n\n  monad much like you would interact with the \n[]\n monad.\n\n\nNo Template Haskell\n Beam uses the GHC Haskell type system and nothing else.\n  The types have been designed to be easily-inferrable by the compiler, and\n  appropriate functions to refine types have been provided for the where the\n  compiler may need more help.\n\n\n\n\nHow to install\n\n\nBeam is available via both \nHackage\n\nand \nStack\n. For most Haskell\ninstallations, one of the two commands should work.\n\n\ncabal install beam-core beam-migrate \nbackend\n\n\n\n\n\nor\n\n\nstack install beam-core beam-migrate \nbackend\n\n\n\n\n\nYou will also need to install an appropriate backend. Available backends are\n\n\n\n\n\n\nbeam-postgres\n -- A feature-complete backend for the Postgres RDBMS.\n  See \nthe beam-postgres documentation\n\n  for more information.\n\n\n\n\n\n\nbeam-sqlite\n -- An almost feature-complete backend for the Sqlite library.\n  Note that SQLite does not support all SQL92 features, so some of the examples\n  may not work. Refer\n  to \nthe beam-sqlite documentation\n for\n  more information on compatibility.\n\n\n\n\n\n\nQuick Start Guide\n\n\nIf you already have a database schema, you can use the \nbeam-migrate\n command to\nautomatically generate appropriate Beam data definitions.\n\n\nbeam-migrate --backend \nbackend-module\n \nbackend-options\n new \noutput-module-name\n\n\n\n\n\nFor example, to generate a schema for the Postgres database \nemployees\n at\n\nlocalhost:5000\n with the user \nbeam\n, run the following command.\n\n\nbeam-migrate --backend Database.Beam.Postgres.Migrate --pgconnect postgres://beam@localhost:5000/employees new BeamTutorial.Schema\n\n\n\n\nThis will generate a\n\n\nHow to Contribute\n\n\nWe always welcome contributions, especially to cover more database features or\nto add support for a new backend. Help is available on the \nbeam-users\n Google\nGroup. The following is a quick step-by-step guide of contributing a new feature:\n\n\n\n\nFork the github repository at \nhttps://github.com/tathougies/beam\n\n   and clone the fork to a local directory.\n\n\nWork on your feature on your own branch, or pick\n   an \nissue\n.\n\n\nWhen you feel ready to contribute the feature back to \nbeam-core\n, send a\n   Pull Request on Github, with an explanation of what your patch does and\n   whether it breaks the API.\n\n\nRespond to community comments and rework your patch.\n\n\nWhen the maintainer feels comfortable with your patch, he will commit it to\n   the \nmaster\n branch and it will be included in the next minor version.\n   API-breaking changes will not be included until the next major version.\n\n\n\n\nQuestions, Feedback, Discussion\n\n\n\n\nFor frequently asked questions, see the \nFAQ\n.\n\n\nFor general questions, feedback on patches, support, or other concerns, please\n  write to the mailing list\n\n\nFor bugs or feature requests,\n  please \nopen an issue\n\n\n\n\nWhy Beam?\n\n\nBeam is the most feature-complete, turnkey Haskell database solution out there.\nIt supports the entire breadth of the SQL92, SQL99, SQL2003, SQL2006, SQL2008,\nSQL2011, and SQL2016 specifications, as well as the entire breadth of features\nof each of its backends. See\nthe \ncompatibility matrix\n. You will rarely be forced to\nwrite a SQL query 'by hand' when using Beam.\n\n\nAdditionally, Beam plays nice with the rest of the Haskell ecosystem, the\nstandard Beam backends are all implemented in terms of the underlying\n\ndb\n-simple\n packages. Beam provides only minimum support for querying data\nacross multiple databases. It is assumed that you have chosen you RDBMS with\nmuch care, and we want to support you in that. Beam's main purpose is to marshal\ndata back and forth, to serve as the source of truth for the DB schema, and to\ngenerate properly formed SQL from Haskell expressions.", 
            "title": "Home"
        }, 
        {
            "location": "/#how-to-install", 
            "text": "Beam is available via both  Hackage \nand  Stack . For most Haskell\ninstallations, one of the two commands should work.  cabal install beam-core beam-migrate  backend   or  stack install beam-core beam-migrate  backend   You will also need to install an appropriate backend. Available backends are    beam-postgres  -- A feature-complete backend for the Postgres RDBMS.\n  See  the beam-postgres documentation \n  for more information.    beam-sqlite  -- An almost feature-complete backend for the Sqlite library.\n  Note that SQLite does not support all SQL92 features, so some of the examples\n  may not work. Refer\n  to  the beam-sqlite documentation  for\n  more information on compatibility.", 
            "title": "How to install"
        }, 
        {
            "location": "/#quick-start-guide", 
            "text": "If you already have a database schema, you can use the  beam-migrate  command to\nautomatically generate appropriate Beam data definitions.  beam-migrate --backend  backend-module   backend-options  new  output-module-name   For example, to generate a schema for the Postgres database  employees  at localhost:5000  with the user  beam , run the following command.  beam-migrate --backend Database.Beam.Postgres.Migrate --pgconnect postgres://beam@localhost:5000/employees new BeamTutorial.Schema  This will generate a", 
            "title": "Quick Start Guide"
        }, 
        {
            "location": "/#how-to-contribute", 
            "text": "We always welcome contributions, especially to cover more database features or\nto add support for a new backend. Help is available on the  beam-users  Google\nGroup. The following is a quick step-by-step guide of contributing a new feature:   Fork the github repository at  https://github.com/tathougies/beam \n   and clone the fork to a local directory.  Work on your feature on your own branch, or pick\n   an  issue .  When you feel ready to contribute the feature back to  beam-core , send a\n   Pull Request on Github, with an explanation of what your patch does and\n   whether it breaks the API.  Respond to community comments and rework your patch.  When the maintainer feels comfortable with your patch, he will commit it to\n   the  master  branch and it will be included in the next minor version.\n   API-breaking changes will not be included until the next major version.", 
            "title": "How to Contribute"
        }, 
        {
            "location": "/#questions-feedback-discussion", 
            "text": "For frequently asked questions, see the  FAQ .  For general questions, feedback on patches, support, or other concerns, please\n  write to the mailing list  For bugs or feature requests,\n  please  open an issue", 
            "title": "Questions, Feedback, Discussion"
        }, 
        {
            "location": "/#why-beam", 
            "text": "Beam is the most feature-complete, turnkey Haskell database solution out there.\nIt supports the entire breadth of the SQL92, SQL99, SQL2003, SQL2006, SQL2008,\nSQL2011, and SQL2016 specifications, as well as the entire breadth of features\nof each of its backends. See\nthe  compatibility matrix . You will rarely be forced to\nwrite a SQL query 'by hand' when using Beam.  Additionally, Beam plays nice with the rest of the Haskell ecosystem, the\nstandard Beam backends are all implemented in terms of the underlying db -simple  packages. Beam provides only minimum support for querying data\nacross multiple databases. It is assumed that you have chosen you RDBMS with\nmuch care, and we want to support you in that. Beam's main purpose is to marshal\ndata back and forth, to serve as the source of truth for the DB schema, and to\ngenerate properly formed SQL from Haskell expressions.", 
            "title": "Why Beam?"
        }, 
        {
            "location": "/about/faq/", 
            "text": "How does \nbeam\n compare with \nx\n?\n\n\nHelp! The type checker keeps complaining about \nSyntax\n types\n\n\nSuppose you had the following code to run a query over an arbitrary backend that\nsupported the SQL92 syntax.\n\n\nlistEmployees :: (IsSql92Syntax cmd, MonadBeam cmd be hdl m) =\n m [Employee]\nlistEmployees = runSelectReturningList $ select (all_ (employees employeeDb))\n\n\n\n\nYou may get an error message like the following\n\n\nMyQueries.hs:1:1: error:\n    * Could not deduce: Sql92ProjectionExpressionSyntax\n                          (Sql92SelectTableProjectionSyntax\n                             (Sql92SelectSelectTableSyntax (Sql92SelectSyntax cmd)))\n                        ~\n                        Sql92SelectTableExpressionSyntax\n                          (Sql92SelectSelectTableSyntax (Sql92SelectSyntax cmd))\n        arising from a use of 'select'\n\n\n\n\nBeam uses a \nfinally-tagless\n\nencoding for syntaxes. This means we never deal with concrete syntax types\ninternally, just types that fulfill certain constraints (in this case, being a\nvalid description of a SQL92 syntax). This works really nicely for\nextensibility, but makes the types slightly confusing. Here, the type checker is\ncomplaining that it cannot prove that the type of expressions used in\nprojections is the same as the type of expressions used in \nWHERE\n and \nHAVING\n\nclauses. Of course, any sane SQL92 syntax would indeed meet this criteria, but\nthis criteria is difficult to enforce at the type class level (it leads to\ncycles in superclasses, which requires the scary-looking\n\nUndecidableSuperclasses\n extension in GHC).\n\n\nNevertheless, we can avoid all this hullabaloo by using the \nSql92SanityCheck\n\nconstraint synonym. This takes a command syntax and asserts all the type\nequalities that a sane SQL92 syntax would support. Thus the code above becomes.\n\n\nlistEmployees :: ( IsSql92Syntax cmd, Sql92SanityCheck cmd\n                 , MonadBeam cmd be hdl m)\n              =\n m [Employee]\nlistEmployees = runSelectReturningList $ select (all_ (employees employeeDb))\n\n\n\n\nOther database mappers simulate features on databases that lack support, why not beam?\n\n\nBeam assumes that the developer has picked their RDBMS for a reason. Beam does\nnot try to take on features of the RDBMS, because often there is no reasonable\nand equally performant substitution that can be made. Beam tries to follow the\nprinciple of least surprise -- the SQL queries beam generates should be easily\nguessable from the Haskell query DSL (modulo aliasing). Generating complicated\nemulation code which can result in unpredictable performance would violate this\nprinciple.", 
            "title": "Frequently Asked Questions"
        }, 
        {
            "location": "/about/faq/#how-does-beam-compare-with-x", 
            "text": "", 
            "title": "How does beam compare with &lt;x&gt;?"
        }, 
        {
            "location": "/about/faq/#help-the-type-checker-keeps-complaining-about-syntax-types", 
            "text": "Suppose you had the following code to run a query over an arbitrary backend that\nsupported the SQL92 syntax.  listEmployees :: (IsSql92Syntax cmd, MonadBeam cmd be hdl m) =  m [Employee]\nlistEmployees = runSelectReturningList $ select (all_ (employees employeeDb))  You may get an error message like the following  MyQueries.hs:1:1: error:\n    * Could not deduce: Sql92ProjectionExpressionSyntax\n                          (Sql92SelectTableProjectionSyntax\n                             (Sql92SelectSelectTableSyntax (Sql92SelectSyntax cmd)))\n                        ~\n                        Sql92SelectTableExpressionSyntax\n                          (Sql92SelectSelectTableSyntax (Sql92SelectSyntax cmd))\n        arising from a use of 'select'  Beam uses a  finally-tagless \nencoding for syntaxes. This means we never deal with concrete syntax types\ninternally, just types that fulfill certain constraints (in this case, being a\nvalid description of a SQL92 syntax). This works really nicely for\nextensibility, but makes the types slightly confusing. Here, the type checker is\ncomplaining that it cannot prove that the type of expressions used in\nprojections is the same as the type of expressions used in  WHERE  and  HAVING \nclauses. Of course, any sane SQL92 syntax would indeed meet this criteria, but\nthis criteria is difficult to enforce at the type class level (it leads to\ncycles in superclasses, which requires the scary-looking UndecidableSuperclasses  extension in GHC).  Nevertheless, we can avoid all this hullabaloo by using the  Sql92SanityCheck \nconstraint synonym. This takes a command syntax and asserts all the type\nequalities that a sane SQL92 syntax would support. Thus the code above becomes.  listEmployees :: ( IsSql92Syntax cmd, Sql92SanityCheck cmd\n                 , MonadBeam cmd be hdl m)\n              =  m [Employee]\nlistEmployees = runSelectReturningList $ select (all_ (employees employeeDb))", 
            "title": "Help! The type checker keeps complaining about Syntax types"
        }, 
        {
            "location": "/about/faq/#other-database-mappers-simulate-features-on-databases-that-lack-support-why-not-beam", 
            "text": "Beam assumes that the developer has picked their RDBMS for a reason. Beam does\nnot try to take on features of the RDBMS, because often there is no reasonable\nand equally performant substitution that can be made. Beam tries to follow the\nprinciple of least surprise -- the SQL queries beam generates should be easily\nguessable from the Haskell query DSL (modulo aliasing). Generating complicated\nemulation code which can result in unpredictable performance would violate this\nprinciple.", 
            "title": "Other database mappers simulate features on databases that lack support, why not beam?"
        }, 
        {
            "location": "/user-guide/getting-started/", 
            "text": "Beam is made up of several Haskell packages. The main package is \nbeam-core\n,\nwhich provides core type definitions and common combinators for SQL. It also\nprovides a basic interface for using backends in a generic way. Beam also comes\nwith some default backends. Backends for other RDBMSs are available on Hackage\nand GitHub.\n\n\nGetting Started", 
            "title": "Getting Started"
        }, 
        {
            "location": "/user-guide/getting-started/#getting-started", 
            "text": "", 
            "title": "Getting Started"
        }, 
        {
            "location": "/user-guide/models/", 
            "text": "Beam allows you to define models for your data that can be used across multiple\nbackends. Beam models are standard Haskell data structures, but with some extra\nfeatures that allow Beam to introspect them at run-time.\n\n\nA Simple Model\n\n\nLet's define a simple model to represent a person. Open up a file named\n\nSchema.hs\n and add the following.\n\n\n{-# LANGUAGE GADTs #-}\n{-# LANGUAGE DeriveGeneric #-}\nmodule Schema where\n\nimport Database.Beam\nimport Database.Beam.SQLite\nimport Database.SQLite.Simple\n\nimport Data.Text (Text)\n\ndata PersonT f\n    = Person\n    { personFirstName :: Columnar f Text\n    , personLastName  :: Columnar f Text\n    , personAge       :: Columnar f Int\n    } deriving Generic\ninstance Beamable PersonT\n\n\n\n\nBeam also requires that your tables have a primary key that can be used to\nuniquely identify each tuple in your relation. We tell beam about the primary\nkey by implementing the \nTable\n type class for your table.\n\n\ninstance Table PersonT where\n  data PrimaryKey PersonT f\n      = PersonKey\n      { personKeyFirstName :: Columnar f Text\n      , personKeyLastName  :: Columnar f Text\n      } deriving Generic\n  primaryKey person = PersonKey (personFirstName person) (personLastName person)\n\n\n\n\n\n\nNote\n\n\nUsing the first and last name as a primary key is a bad idea, we use it here\nto illustrate using multiple fields as the primary key.\n\n\n\n\n\n\nTip\n\n\nMany people find it useful to use the \nApplicative\n instance for \n(-\n) a\n to\nwrite \nprimaryKey\n. For example, we could have written the above \nprimaryKey\nperson = PersonKey (personFirstName person) (personLastName person)\n as\n\nprimaryKey = PersonKey \n$\n personFirstName \n*\n personLastName\n.\n\n\n\n\nFor ease-of-use purposes we define some type synonyms for \nPersonT\n and\n\nPrimaryKey PersonT\n and some convenient instances. These are not strictly\nrequired but make working with these tables much easier.\n\n\ntype Person = PersonT Identity\ntype PersonKey = PrimaryKey PersonT Identity\nderiving instance Show Person; deriving instance Eq Person\nderiving instance Show PersonKey; deriving instance Eq PersonKey\n\n\n\n\nDue to the magic of the \nColumnar\n type family, the \nPerson\n type can\nbe thought of as having the following definition.\n\n\ndata Person\n    = Person\n    { personFirstName :: Text\n    , personLastName  :: Text\n    , personAge       :: Int\n    } deriving (Show, Eq)\n\n\n\n\nThis allows us to use your type definitions for Beam as regular\nHaskell data structures without wrapping/unwrapping.\n\n\n\n\nTip\n\n\nTyping \nColumnar\n may become tiresome. \nDatabase.Beam\n also exports \nC\n as a\ntype alias for \nColumnar\n, which may make writing models easier. Since \nC\n\nmay cause name clashes, all examples are given using \nColumnar\n.\n\n\n\n\nForeign references\n\n\nForeign references are also easily supported in models by simply\nembedding the \nPrimaryKey\n of the referred to table directly in the\nparent. For example, suppose we want to create a new model\nrepresenting a post by a user.\n\n\ndata PostT f\n    = Post\n    { postId       :: Columnar f (Auto Int)\n    , postPostedAt :: Columnar f LocalTime\n    , postContent  :: Columnar f Text\n    , postPoster   :: PrimaryKey PersonT f\n    } deriving Generic\ninstance Beamable PostT\n\ninstance Table PostT where\n  data PrimaryKey PostT f\n      = PostId (Columnar f (Auto Int)) deriving Generic\n  primaryKey = PostId . postId\n\ntype Post = PostT Identity\ntype PostId = PrimaryKey PostT Identity\nderiving instance Show Post; deriving instance Eq Post\nderiving instance Show PostId; deriving instance Eq PostId\n\n\n\n\nThe \nAuto\n type constructor is provided by \nbeam-core\n for fields that\nare automatically assigned by the database. Internally, \nAuto x\n is\nsimply a newtype over \nMaybe x\n. The guarantee is that all values of\ntype \nAuto x\n returned by beam in the result set will have a value,\nalthough this guarantee is not enforced at the type level (yet).\n\n\nEmbedding\n\n\nSometimes, we want to declare multiple models with fields in common. Beam allows\nyou to simple embed such fields in common types and embed those directly into\nmodels. For example, in the Chinook example schema (see\n\nbeam-sqlite/examples/Chinook/Schema.hs\n), we define the following structure for\naddresses.\n\n\ndata AddressMixin f\n  = Address\n  { address           :: Columnar f (Maybe Text)\n  , addressCity       :: Columnar f (Maybe Text)\n  , addressState      :: Columnar f (Maybe Text)\n  , addressCountry    :: Columnar f (Maybe Text)\n  , addressPostalCode :: Columnar f (Maybe Text)\n  } deriving Generic\ninstance Beamable AddressMixin\ntype Address = AddressMixin Identity\nderiving instance Show (AddressMixin Identity)\n\n\n\n\nWe can then use \nAddressMixin\n in our models.\n\n\ndata EmployeeT f\n  = Employee\n  { employeeId        :: Columnar f Int32\n  , employeeLastName  :: Columnar f Text\n  , employeeFirstName :: Columnar f Text\n  , employeeTitle     :: Columnar f (Maybe Text)\n  , employeeReportsTo :: PrimaryKey EmployeeT (Nullable f)\n  , employeeBirthDate :: Columnar f (Maybe LocalTime)\n  , employeeHireDate  :: Columnar f (Maybe LocalTime)\n  , employeeAddress   :: AddressMixin f\n  , employeePhone     :: Columnar f (Maybe Text)\n  , employeeFax       :: Columnar f (Maybe Text)\n  , employeeEmail     :: Columnar f (Maybe Text)\n  } deriving Generic\n-- ...\ndata CustomerT f\n  = Customer\n  { customerId        :: Columnar f Int32\n  , customerFirstName :: Columnar f Text\n  , customerLastName  :: Columnar f Text\n  , customerCompany   :: Columnar f (Maybe Text)\n  , customerAddress   :: AddressMixin f\n  , customerPhone     :: Columnar f (Maybe Text)\n  , customerFax       :: Columnar f (Maybe Text)\n  , customerEmail     :: Columnar f Text\n  , customerSupportRep :: PrimaryKey EmployeeT (Nullable f)\n  } deriving Generic\n\n\n\n\nDefaults\n\n\nBased on your data type declarations, beam can already guess a lot\nabout your tables. For example, it already assumes that the\n\npersonFirstName\n field is accessible in SQL as \nfirst_name\n. This\ndefaulting behavior makes it very easy to interact with typical\ndatabases.\n\n\nFor the easiest user experience, it's best to follow beam's\nconventions for declaring models. In particular, the defaulting\nmechanisms rely on each table type declaring only one constructor\nwhich has fields named in the camelCase style.\n\n\nWhen defaulting the name of a table field or column, beam\nun-camelCases the field name (after dropping leading underscores) and\ndrops the first word. The remaining words are joined with\nunderscores. If there is only one component, it is not\ndropped. Trailing and internal underscores are preserved in the name\nand if the name consists solely of underscores, beam makes no\nchanges. A summary of these rules is given in the table below.\n\n\n\n\n\n\n\n\nHaskell field name\n\n\nBeam defaulted column name\n\n\n\n\n\n\n\n\n\n\npersonFirstName\n\n\nfirst_name\n\n\n\n\n\n\n_personLastName\n\n\nlast_name\n\n\n\n\n\n\nname\n\n\nname\n\n\n\n\n\n\nfirst_name\n\n\nfirst_name\n\n\n\n\n\n\n_first_name\n\n\nfirst_name\n\n\n\n\n\n\n___\n (three underscores)\n\n\n___\n (no changes)\n\n\n\n\n\n\n\n\nNote that beam only uses lower case in field names. While typically\ncase does not matter for SQL queries, beam always quotes\nidentifiers. Many DBMS's are case-sensitive for quoted\nidentifiers. Thus, queries can sometimes fail if your tables use\nmixtures of lower- and upper-case to distinguish between fields.\n\n\nAll of these defaults can be overriden using the modifications system,\ndescribed in the \nmodel reference\n.\n\n\nWhat about tables without primary keys?\n\n\nTables without primary keys are considered bad style. However,\nsometimes you need to use beam with a schema that you have no control\nover. To declare a table without a primary key, simply instantiate the\n\nTable\n class without overriding the defaults.\n\n\nMore complicated relationships\n\n\nThis is the extent of beam's support for defining models. Although\nsimilar packages in other languages provide support for declaring\none-to-many, many-to-one, and many-to-many relationships, beam's\nfocused is providing a direct mapping of relational database concepts\nto Haskell, not on abstracting away the complexities of database\nquerying. Thus, beam does not use 'lazy-loading' or other tricks that\nobfuscate performance. Because of this, the bulk of the functionality\ndealing with different types of relations is found in the querying\nsupport, rather than in the model declarations.\n\n\nPutting a Database Together\n\n\nBeam also requires you to give a type for your database. The database type\ncontains all the entities (tables or otherwise) that would be present in your\ndatabase. Our database only has one table right now, so it only contains one\nfield.\n\n\ndata ExampleDb f\n    = ExampleDb\n    { persons :: f (TableEntity PersonT)\n    } deriving Generic\ninstance Database ExampleDb\n\nexampleDb :: DatabaseSettings be ExampleDb\nexampleDb = autoDbSettings\n\n\n\n\nUsing your database\n\n\nLet's open up a SQLite database. Open up \nghci\n and import your module.\n\n\nPrelude\n :load Schema.hs\nPrelude Schema\n conn \n- open \nbeam-manual.db\n\n\n\n\n\nA quick note on backends\n\n\nBeam is backend-agnostic and doesn't provide any means to connect to a\ndatabase. Beam backend libraries usually use well-used Haskell\nlibraries to provide database connectivity. For example, the\n\nbeam-sqlite\n backend uses the \nsqlite-simple\n backend.\n\n\nBeam distinguishes each backend via type indexes. Each backend defines\na type that is used to enable backend-specific behavior. For example,\nthe \nbeam-sqlite\n backend ships with the \nSqlite\n type that is used to\ndistinguish sqlite specific constructs with generic or other\nbackend-specific ones.\n\n\nEach backend can have one or more 'syntaxes', which are particular\nways to query the database. While the \nbeam-core\n library ships with a\nstandard ANSI SQL builder, few real-world database implementations\nfully follow the standard. Most backends use their own custom syntax\ntype. Internally, beam uses a finally-tagless representation for\nsyntax trees that allow straightforward construction against any\nbackend.\n\n\nBeam offers backend-generic functions for the most common operations\nagainst databases. These functions are meant to fit the lowest common\ndenominator. For example, no control is offered over streaming results\nfrom SELECT statements. While these backend-generic functions are\nuseful for ad-hoc querying and development, it is wisest to use\nbackend-specific functions in production for maximum control. Refer to\nbackend-specific documentation for more information.\n\n\nFor our examples, we will use the \nbeam-sqlite\n backend and demonstrate\nusage of the beam standard query functions.\n\n\nInserting data\n\n\nFirst, let's connect to a sqlite database, and create our schema. The\n\nbeam-core\n does not offer any support for the SQL DDL language. There\nis a separate core library \nbeam-migrate\n that offers complete support\nfor ANSI-standard SQL DDL operations, as well as tools to manipulate\ndatabase schemas. See the section on migrations for more information.\n\n\nFor our example, we will simply issue a \nCREATE TABLE\n command\ndirectly against the database using \nsqlite-simple\n functionality:\n\n\nPrelude Schema\n execute_ conn \nCREATE TABLE persons ( first_name TEXT NOT NULL, last_name TEXT NOT NULL, age INT NOT NULL, PRIMARY KEY(first_name, last_name) )\n\n\n\n\n\nNow we can insert some data into our database. Beam ships with a\nfunction \nwithDatabase\n, with the following signature:\n\n\nwithDatabase :: MonadBeam syntax be m =\n DbHandle be -\n m a -\n IO a\n\n\n\n\nDbHandle be\n is a type family that refers to a backend-specific type\nfor referring to a particular database connection. For the\n\nbeam-sqlite\n backend \nDbHandle Sqlite ~\nDatabase.Sqlite.Simple.Connection\n. \n\n\nMonadBeam\n is a type class relating a particular syntax and backend\nto a monad we can use to execute data query and manipulation commands.\n\n\nLet's insert some data into our database. We are going to use the\n\nrunInsert\n function from \nMonadBeam\n.\n\n\nPrelude Schema\n :{\nPrelude Schema| withDatabase conn $ do\nPrelude Schema|   runInsert $ insert (persons exampleDb) $\nPrelude Schema|               insertValues [ Person \nBob\n \nSmith\n 50\nPrelude Schema|                            , Person \nAlice\n \nWong\n 55\nPrelude Schema|                            , Person \nJohn\n \nQuincy\n 30 ]\nPrelude Schema| :}\n\n\n\n\nThe \nrunInsert\n function has the type signature\n\n\nrunInsert :: MonadBeam syntax be m =\n SqlInsert syntax -\n m ()\n\n\n\n\nSqlInsert syntax\n represents a SQL \nINSERT\n command in the given\n\nsyntax\n. We construct this value using the \ninsert\n function from\n\nDatabase.Beam.Query\n.\n\n\ninsert :: IsSql92InsertSyntax syntax =\n\n          DatabaseEntity be db (TableEntity table)\n       -\n Sql92InsertValuesSyntax syntax\n       -\n SqlInsert syntax\n\n\n\n\nIntuitively, \ninsert\n takes a database table descriptor and some\nvalues (particular to the given syntax) and returns a statement to\ninsert these values. \nSql92InsertValuesSyntax syntax\n always\nimplements the \nIsSql92InsertValuesSyntax\n typeclass, which is where\nwe get the \ninsertValues\n function from. \nIsSql92InsertValuesSyntax\n\nalso defines the \ninsertSelect\n function for inserting values from the\nresult of a \nSELECT\n statement. Other backends may provide other ways\nof specifying the source of values. This brings us to another point\n\n\nPrelude Schema\n runSelect (select (all_ (persons exampleDb)))\n[ Person { personFirstName = \nBob\n, personLastName=\nSmith\n, personAge=50 }, ... ]", 
            "title": "Models"
        }, 
        {
            "location": "/user-guide/models/#a-simple-model", 
            "text": "Let's define a simple model to represent a person. Open up a file named Schema.hs  and add the following.  {-# LANGUAGE GADTs #-}\n{-# LANGUAGE DeriveGeneric #-}\nmodule Schema where\n\nimport Database.Beam\nimport Database.Beam.SQLite\nimport Database.SQLite.Simple\n\nimport Data.Text (Text)\n\ndata PersonT f\n    = Person\n    { personFirstName :: Columnar f Text\n    , personLastName  :: Columnar f Text\n    , personAge       :: Columnar f Int\n    } deriving Generic\ninstance Beamable PersonT  Beam also requires that your tables have a primary key that can be used to\nuniquely identify each tuple in your relation. We tell beam about the primary\nkey by implementing the  Table  type class for your table.  instance Table PersonT where\n  data PrimaryKey PersonT f\n      = PersonKey\n      { personKeyFirstName :: Columnar f Text\n      , personKeyLastName  :: Columnar f Text\n      } deriving Generic\n  primaryKey person = PersonKey (personFirstName person) (personLastName person)   Note  Using the first and last name as a primary key is a bad idea, we use it here\nto illustrate using multiple fields as the primary key.    Tip  Many people find it useful to use the  Applicative  instance for  (- ) a  to\nwrite  primaryKey . For example, we could have written the above  primaryKey\nperson = PersonKey (personFirstName person) (personLastName person)  as primaryKey = PersonKey  $  personFirstName  *  personLastName .   For ease-of-use purposes we define some type synonyms for  PersonT  and PrimaryKey PersonT  and some convenient instances. These are not strictly\nrequired but make working with these tables much easier.  type Person = PersonT Identity\ntype PersonKey = PrimaryKey PersonT Identity\nderiving instance Show Person; deriving instance Eq Person\nderiving instance Show PersonKey; deriving instance Eq PersonKey  Due to the magic of the  Columnar  type family, the  Person  type can\nbe thought of as having the following definition.  data Person\n    = Person\n    { personFirstName :: Text\n    , personLastName  :: Text\n    , personAge       :: Int\n    } deriving (Show, Eq)  This allows us to use your type definitions for Beam as regular\nHaskell data structures without wrapping/unwrapping.   Tip  Typing  Columnar  may become tiresome.  Database.Beam  also exports  C  as a\ntype alias for  Columnar , which may make writing models easier. Since  C \nmay cause name clashes, all examples are given using  Columnar .", 
            "title": "A Simple Model"
        }, 
        {
            "location": "/user-guide/models/#foreign-references", 
            "text": "Foreign references are also easily supported in models by simply\nembedding the  PrimaryKey  of the referred to table directly in the\nparent. For example, suppose we want to create a new model\nrepresenting a post by a user.  data PostT f\n    = Post\n    { postId       :: Columnar f (Auto Int)\n    , postPostedAt :: Columnar f LocalTime\n    , postContent  :: Columnar f Text\n    , postPoster   :: PrimaryKey PersonT f\n    } deriving Generic\ninstance Beamable PostT\n\ninstance Table PostT where\n  data PrimaryKey PostT f\n      = PostId (Columnar f (Auto Int)) deriving Generic\n  primaryKey = PostId . postId\n\ntype Post = PostT Identity\ntype PostId = PrimaryKey PostT Identity\nderiving instance Show Post; deriving instance Eq Post\nderiving instance Show PostId; deriving instance Eq PostId  The  Auto  type constructor is provided by  beam-core  for fields that\nare automatically assigned by the database. Internally,  Auto x  is\nsimply a newtype over  Maybe x . The guarantee is that all values of\ntype  Auto x  returned by beam in the result set will have a value,\nalthough this guarantee is not enforced at the type level (yet).", 
            "title": "Foreign references"
        }, 
        {
            "location": "/user-guide/models/#embedding", 
            "text": "Sometimes, we want to declare multiple models with fields in common. Beam allows\nyou to simple embed such fields in common types and embed those directly into\nmodels. For example, in the Chinook example schema (see beam-sqlite/examples/Chinook/Schema.hs ), we define the following structure for\naddresses.  data AddressMixin f\n  = Address\n  { address           :: Columnar f (Maybe Text)\n  , addressCity       :: Columnar f (Maybe Text)\n  , addressState      :: Columnar f (Maybe Text)\n  , addressCountry    :: Columnar f (Maybe Text)\n  , addressPostalCode :: Columnar f (Maybe Text)\n  } deriving Generic\ninstance Beamable AddressMixin\ntype Address = AddressMixin Identity\nderiving instance Show (AddressMixin Identity)  We can then use  AddressMixin  in our models.  data EmployeeT f\n  = Employee\n  { employeeId        :: Columnar f Int32\n  , employeeLastName  :: Columnar f Text\n  , employeeFirstName :: Columnar f Text\n  , employeeTitle     :: Columnar f (Maybe Text)\n  , employeeReportsTo :: PrimaryKey EmployeeT (Nullable f)\n  , employeeBirthDate :: Columnar f (Maybe LocalTime)\n  , employeeHireDate  :: Columnar f (Maybe LocalTime)\n  , employeeAddress   :: AddressMixin f\n  , employeePhone     :: Columnar f (Maybe Text)\n  , employeeFax       :: Columnar f (Maybe Text)\n  , employeeEmail     :: Columnar f (Maybe Text)\n  } deriving Generic\n-- ...\ndata CustomerT f\n  = Customer\n  { customerId        :: Columnar f Int32\n  , customerFirstName :: Columnar f Text\n  , customerLastName  :: Columnar f Text\n  , customerCompany   :: Columnar f (Maybe Text)\n  , customerAddress   :: AddressMixin f\n  , customerPhone     :: Columnar f (Maybe Text)\n  , customerFax       :: Columnar f (Maybe Text)\n  , customerEmail     :: Columnar f Text\n  , customerSupportRep :: PrimaryKey EmployeeT (Nullable f)\n  } deriving Generic", 
            "title": "Embedding"
        }, 
        {
            "location": "/user-guide/models/#defaults", 
            "text": "Based on your data type declarations, beam can already guess a lot\nabout your tables. For example, it already assumes that the personFirstName  field is accessible in SQL as  first_name . This\ndefaulting behavior makes it very easy to interact with typical\ndatabases.  For the easiest user experience, it's best to follow beam's\nconventions for declaring models. In particular, the defaulting\nmechanisms rely on each table type declaring only one constructor\nwhich has fields named in the camelCase style.  When defaulting the name of a table field or column, beam\nun-camelCases the field name (after dropping leading underscores) and\ndrops the first word. The remaining words are joined with\nunderscores. If there is only one component, it is not\ndropped. Trailing and internal underscores are preserved in the name\nand if the name consists solely of underscores, beam makes no\nchanges. A summary of these rules is given in the table below.     Haskell field name  Beam defaulted column name      personFirstName  first_name    _personLastName  last_name    name  name    first_name  first_name    _first_name  first_name    ___  (three underscores)  ___  (no changes)     Note that beam only uses lower case in field names. While typically\ncase does not matter for SQL queries, beam always quotes\nidentifiers. Many DBMS's are case-sensitive for quoted\nidentifiers. Thus, queries can sometimes fail if your tables use\nmixtures of lower- and upper-case to distinguish between fields.  All of these defaults can be overriden using the modifications system,\ndescribed in the  model reference .", 
            "title": "Defaults"
        }, 
        {
            "location": "/user-guide/models/#what-about-tables-without-primary-keys", 
            "text": "Tables without primary keys are considered bad style. However,\nsometimes you need to use beam with a schema that you have no control\nover. To declare a table without a primary key, simply instantiate the Table  class without overriding the defaults.", 
            "title": "What about tables without primary keys?"
        }, 
        {
            "location": "/user-guide/models/#more-complicated-relationships", 
            "text": "This is the extent of beam's support for defining models. Although\nsimilar packages in other languages provide support for declaring\none-to-many, many-to-one, and many-to-many relationships, beam's\nfocused is providing a direct mapping of relational database concepts\nto Haskell, not on abstracting away the complexities of database\nquerying. Thus, beam does not use 'lazy-loading' or other tricks that\nobfuscate performance. Because of this, the bulk of the functionality\ndealing with different types of relations is found in the querying\nsupport, rather than in the model declarations.", 
            "title": "More complicated relationships"
        }, 
        {
            "location": "/user-guide/models/#putting-a-database-together", 
            "text": "Beam also requires you to give a type for your database. The database type\ncontains all the entities (tables or otherwise) that would be present in your\ndatabase. Our database only has one table right now, so it only contains one\nfield.  data ExampleDb f\n    = ExampleDb\n    { persons :: f (TableEntity PersonT)\n    } deriving Generic\ninstance Database ExampleDb\n\nexampleDb :: DatabaseSettings be ExampleDb\nexampleDb = autoDbSettings", 
            "title": "Putting a Database Together"
        }, 
        {
            "location": "/user-guide/models/#using-your-database", 
            "text": "Let's open up a SQLite database. Open up  ghci  and import your module.  Prelude  :load Schema.hs\nPrelude Schema  conn  - open  beam-manual.db", 
            "title": "Using your database"
        }, 
        {
            "location": "/user-guide/models/#a-quick-note-on-backends", 
            "text": "Beam is backend-agnostic and doesn't provide any means to connect to a\ndatabase. Beam backend libraries usually use well-used Haskell\nlibraries to provide database connectivity. For example, the beam-sqlite  backend uses the  sqlite-simple  backend.  Beam distinguishes each backend via type indexes. Each backend defines\na type that is used to enable backend-specific behavior. For example,\nthe  beam-sqlite  backend ships with the  Sqlite  type that is used to\ndistinguish sqlite specific constructs with generic or other\nbackend-specific ones.  Each backend can have one or more 'syntaxes', which are particular\nways to query the database. While the  beam-core  library ships with a\nstandard ANSI SQL builder, few real-world database implementations\nfully follow the standard. Most backends use their own custom syntax\ntype. Internally, beam uses a finally-tagless representation for\nsyntax trees that allow straightforward construction against any\nbackend.  Beam offers backend-generic functions for the most common operations\nagainst databases. These functions are meant to fit the lowest common\ndenominator. For example, no control is offered over streaming results\nfrom SELECT statements. While these backend-generic functions are\nuseful for ad-hoc querying and development, it is wisest to use\nbackend-specific functions in production for maximum control. Refer to\nbackend-specific documentation for more information.  For our examples, we will use the  beam-sqlite  backend and demonstrate\nusage of the beam standard query functions.", 
            "title": "A quick note on backends"
        }, 
        {
            "location": "/user-guide/models/#inserting-data", 
            "text": "First, let's connect to a sqlite database, and create our schema. The beam-core  does not offer any support for the SQL DDL language. There\nis a separate core library  beam-migrate  that offers complete support\nfor ANSI-standard SQL DDL operations, as well as tools to manipulate\ndatabase schemas. See the section on migrations for more information.  For our example, we will simply issue a  CREATE TABLE  command\ndirectly against the database using  sqlite-simple  functionality:  Prelude Schema  execute_ conn  CREATE TABLE persons ( first_name TEXT NOT NULL, last_name TEXT NOT NULL, age INT NOT NULL, PRIMARY KEY(first_name, last_name) )   Now we can insert some data into our database. Beam ships with a\nfunction  withDatabase , with the following signature:  withDatabase :: MonadBeam syntax be m =  DbHandle be -  m a -  IO a  DbHandle be  is a type family that refers to a backend-specific type\nfor referring to a particular database connection. For the beam-sqlite  backend  DbHandle Sqlite ~\nDatabase.Sqlite.Simple.Connection .   MonadBeam  is a type class relating a particular syntax and backend\nto a monad we can use to execute data query and manipulation commands.  Let's insert some data into our database. We are going to use the runInsert  function from  MonadBeam .  Prelude Schema  :{\nPrelude Schema| withDatabase conn $ do\nPrelude Schema|   runInsert $ insert (persons exampleDb) $\nPrelude Schema|               insertValues [ Person  Bob   Smith  50\nPrelude Schema|                            , Person  Alice   Wong  55\nPrelude Schema|                            , Person  John   Quincy  30 ]\nPrelude Schema| :}  The  runInsert  function has the type signature  runInsert :: MonadBeam syntax be m =  SqlInsert syntax -  m ()  SqlInsert syntax  represents a SQL  INSERT  command in the given syntax . We construct this value using the  insert  function from Database.Beam.Query .  insert :: IsSql92InsertSyntax syntax = \n          DatabaseEntity be db (TableEntity table)\n       -  Sql92InsertValuesSyntax syntax\n       -  SqlInsert syntax  Intuitively,  insert  takes a database table descriptor and some\nvalues (particular to the given syntax) and returns a statement to\ninsert these values.  Sql92InsertValuesSyntax syntax  always\nimplements the  IsSql92InsertValuesSyntax  typeclass, which is where\nwe get the  insertValues  function from.  IsSql92InsertValuesSyntax \nalso defines the  insertSelect  function for inserting values from the\nresult of a  SELECT  statement. Other backends may provide other ways\nof specifying the source of values. This brings us to another point  Prelude Schema  runSelect (select (all_ (persons exampleDb)))\n[ Person { personFirstName =  Bob , personLastName= Smith , personAge=50 }, ... ]", 
            "title": "Inserting data"
        }, 
        {
            "location": "/user-guide/databases/", 
            "text": "In addition to defining types for each of your tables, beam also\nrequires you to declare your database as a type with fields for\nholding all entities in your database. This includes more than just\ntables. For example, user-defined types that you would like to work\nwith must also be included in your database type.\n\n\nA simple database type\n\n\nLike tables, a database type takes a functor and applies it to each\nentity in the database. For example, a database type for the two\ntables defined above has the form.\n\n\ndata ExampleDb f\n    = ExampleDb\n    { persons :: f (TableEntity PersonT)\n    , posts   :: f (TableEntity PersonT)\n    } deriving Generic\ninstance Database ExampleDb\n\nexampleDb :: DatabaseSettings be ExampleDb\nexampleDb = defaultDbSettings\n\n\n\n\nOther database entities\n\n\nViews\n\n\n\n\nNote\n\n\nViews have not yet been implemented, but this is the expected syntax\n\n\n\n\nSome databases also offer the concept of 'views' -- pseudo-tables that\nare built from a pre-defined query. Suppose we wanted to create a view\nthat returned the latest comments and their respective posters.\n\n\ndata PostAndPosterView f\n    = PostAndPosterView\n    { post   :: PostT f\n    , poster :: PersonT f\n    } deriving Generic\ninstance Beamable PostAndPosterView\n\n\n\n\nWe can include this in our database:\n\n\ndata ExampleDb f\n    = ExampleDb\n    { persons        :: f (TableEntity PersonT)\n    , posts          :: f (TableEntity PersonT)\n    , postAndPosters :: f (ViewEntity PostAndPosterView)\n    } deriving Generic\n\n\n\n\nNow we can use \npostAndPosters\n wherever we'd use a table. Note that you do not\nneed to specify the definition of the view. The definition is not important to\naccess the view, so beam does not need to know about it at the type-level. If\nyou want to manipulate view definitions, use the migrations packgae.\n\n\nUnique constraints\n\n\n\n\nNote\n\n\nThis is the current implementation plan. Uniques are not currently implemented.\n\n\n\n\nThe \nTableEntityWithUnique\n database entity allows you to declare\ntables with additional uniqueness constraints (the primary key is\nconsidered to be unique by default).\n\n\nFor example, suppose you wanted to re-define the \nPersonT\n table with\nan additional unique e-mail and another unique phone column.\n\n\ndata PersonT f\n    = Person\n    { personFirstName :: Columnar f Text\n    , personLastName  :: Columnar f Text\n    , personAge       :: Columnar f Int\n    , personEmail     :: Columnar f Text\n    , personPhone     :: Columnar f Text\n    } deriving Generic\n\ndata PersonByEmail f = PersonByEmail (Columnar f Text)\ndata PersonByPhone f = PersonByPhone (Columnar f Text)\n\n\n\n\nNow, use \nTableEntityWithUnique\n to declare the table.\n\n\ndata ExampleDb f\n    = ExampleDb\n    { persons        :: f (TableEntityWithUnique PersonT '[PersonByEmail, PersonByPhone])\n    , posts          :: f (TableEntity PersonT)\n    , postAndPosters :: f (ViewEntity PostAndPosterView)\n    } deriving Generic\n\n\n\n\nBeam will not complain about this definition, but you will need to\ndeclare additional instances in order to actually use the unique\nconstraints.\n\n\ninstance Unique PersonT PersonByEmail where\n  mkUnique = PersonByEmail . personEmail\n\ninstance Unique PersonT PersonByPhone where\n  mkUnique = PersonByPhone . personPhone\n\n\n\n\nTODO\n: Should the unique constraints be declared at the database or table level?\n\n\nDomain types\n\n\nDomain types are a way of creating new database types with additional\nconstraints. Beam supports declaring these types as part of your\ndatabase, so they can be used anywhere a data type can. In order to\nuse your domain type, you need to supply beam a Haskell newtype that\nis used to represent values of this type in Haskell.\n\n\nTriggers\n\n\nTODO\n\n\nCharacter sets\n\n\nTODO\n\n\nCollations\n\n\nTODO\n\n\nTranslations\n\n\nTODO\n\n\nDatabase descriptors\n\n\nIn order to interact with the database, beam needs to know more about\nthe data structure, it also needs to know how to refer to each entity\nin your database. For the most part, beam can figure out the names for\nyou using its Generics-based defaulting mechanims. Once you have a\ndatabase type defined, you can create a database descriptor using the\n\ndefaultDbSettings\n function.\n\n\nFor example, to create a backend-agnostic database descriptor for the\n\nExampleDb\n type:\n\n\nexampleDb :: DatabaseSettings be ExampleDb\nexampleDb = defaultDbSettings\n\n\n\n\nNow, we can use the entities in \nexampleDb\n to write queries. The\nrules for name defaulting for database entities are the same as those\nfor \ntable fields", 
            "title": "Databases"
        }, 
        {
            "location": "/user-guide/databases/#a-simple-database-type", 
            "text": "Like tables, a database type takes a functor and applies it to each\nentity in the database. For example, a database type for the two\ntables defined above has the form.  data ExampleDb f\n    = ExampleDb\n    { persons :: f (TableEntity PersonT)\n    , posts   :: f (TableEntity PersonT)\n    } deriving Generic\ninstance Database ExampleDb\n\nexampleDb :: DatabaseSettings be ExampleDb\nexampleDb = defaultDbSettings", 
            "title": "A simple database type"
        }, 
        {
            "location": "/user-guide/databases/#other-database-entities", 
            "text": "", 
            "title": "Other database entities"
        }, 
        {
            "location": "/user-guide/databases/#views", 
            "text": "Note  Views have not yet been implemented, but this is the expected syntax   Some databases also offer the concept of 'views' -- pseudo-tables that\nare built from a pre-defined query. Suppose we wanted to create a view\nthat returned the latest comments and their respective posters.  data PostAndPosterView f\n    = PostAndPosterView\n    { post   :: PostT f\n    , poster :: PersonT f\n    } deriving Generic\ninstance Beamable PostAndPosterView  We can include this in our database:  data ExampleDb f\n    = ExampleDb\n    { persons        :: f (TableEntity PersonT)\n    , posts          :: f (TableEntity PersonT)\n    , postAndPosters :: f (ViewEntity PostAndPosterView)\n    } deriving Generic  Now we can use  postAndPosters  wherever we'd use a table. Note that you do not\nneed to specify the definition of the view. The definition is not important to\naccess the view, so beam does not need to know about it at the type-level. If\nyou want to manipulate view definitions, use the migrations packgae.", 
            "title": "Views"
        }, 
        {
            "location": "/user-guide/databases/#unique-constraints", 
            "text": "Note  This is the current implementation plan. Uniques are not currently implemented.   The  TableEntityWithUnique  database entity allows you to declare\ntables with additional uniqueness constraints (the primary key is\nconsidered to be unique by default).  For example, suppose you wanted to re-define the  PersonT  table with\nan additional unique e-mail and another unique phone column.  data PersonT f\n    = Person\n    { personFirstName :: Columnar f Text\n    , personLastName  :: Columnar f Text\n    , personAge       :: Columnar f Int\n    , personEmail     :: Columnar f Text\n    , personPhone     :: Columnar f Text\n    } deriving Generic\n\ndata PersonByEmail f = PersonByEmail (Columnar f Text)\ndata PersonByPhone f = PersonByPhone (Columnar f Text)  Now, use  TableEntityWithUnique  to declare the table.  data ExampleDb f\n    = ExampleDb\n    { persons        :: f (TableEntityWithUnique PersonT '[PersonByEmail, PersonByPhone])\n    , posts          :: f (TableEntity PersonT)\n    , postAndPosters :: f (ViewEntity PostAndPosterView)\n    } deriving Generic  Beam will not complain about this definition, but you will need to\ndeclare additional instances in order to actually use the unique\nconstraints.  instance Unique PersonT PersonByEmail where\n  mkUnique = PersonByEmail . personEmail\n\ninstance Unique PersonT PersonByPhone where\n  mkUnique = PersonByPhone . personPhone  TODO : Should the unique constraints be declared at the database or table level?", 
            "title": "Unique constraints"
        }, 
        {
            "location": "/user-guide/databases/#domain-types", 
            "text": "Domain types are a way of creating new database types with additional\nconstraints. Beam supports declaring these types as part of your\ndatabase, so they can be used anywhere a data type can. In order to\nuse your domain type, you need to supply beam a Haskell newtype that\nis used to represent values of this type in Haskell.", 
            "title": "Domain types"
        }, 
        {
            "location": "/user-guide/databases/#triggers", 
            "text": "TODO", 
            "title": "Triggers"
        }, 
        {
            "location": "/user-guide/databases/#character-sets", 
            "text": "TODO", 
            "title": "Character sets"
        }, 
        {
            "location": "/user-guide/databases/#collations", 
            "text": "TODO", 
            "title": "Collations"
        }, 
        {
            "location": "/user-guide/databases/#translations", 
            "text": "TODO", 
            "title": "Translations"
        }, 
        {
            "location": "/user-guide/databases/#database-descriptors", 
            "text": "In order to interact with the database, beam needs to know more about\nthe data structure, it also needs to know how to refer to each entity\nin your database. For the most part, beam can figure out the names for\nyou using its Generics-based defaulting mechanims. Once you have a\ndatabase type defined, you can create a database descriptor using the defaultDbSettings  function.  For example, to create a backend-agnostic database descriptor for the ExampleDb  type:  exampleDb :: DatabaseSettings be ExampleDb\nexampleDb = defaultDbSettings  Now, we can use the entities in  exampleDb  to write queries. The\nrules for name defaulting for database entities are the same as those\nfor  table fields", 
            "title": "Database descriptors"
        }, 
        {
            "location": "/user-guide/queries/basic/", 
            "text": "Given our database definition and database descriptor, we can query database\nentities and retrieve data. Before we discuss writing queries, we will take a\nlook at some of the important query types.\n\n\nData types\n\n\nThe \nQ\n data type\n\n\nBeam queries are built using the \nQ\n data type. \nQ\n's signature is as follows\n\n\ndata Q syntax db s a\n\n\n\n\nIn this definition\n\n\n\n\n\n\nsyntax\n is the particular dialect of SQL this \nQ\n monad will evaluate to.\n  Often times, this is any instance of \nIsSql92SelectSyntax\n, but sometimes you\n  use syntax-specific features. For example, if you want to use named windows in\n  postgres, you'll likely have to specialize this to \nPgSelectSyntax\n from\n  \nDatabase.Beam.Postgres.Syntax\n.\n\n\n\n\n\n\ndb\n is the type of the database (as we defined above). This is used to ensure\n  you only query database entities that are in scope in this database.\n\n\n\n\n\n\ns\n is the scope parameter. For the most part, you'll write your queries so\n  that they work over all \ns\n. Beam manipulates this parameter internally to\n  ensure that the fields in your expressions are always in scope at run-time.\n\n\n\n\n\n\na\n is the type of the result of the query.\n\n\n\n\n\n\nThe \nQGenExpr\n type\n\n\nWhile \nQ\n represents the result of whole queries (entire \nSELECT\ns for example),\n\nQGenExpr\n represents the type of SQL expressions. \nQGenExpr\n also takes some\ntype parameters:\n\n\ndata QGenExpr context syntax s a\n\n\n\n\n\n\n\n\ncontext\n is the particular way in which this expression is being used. For\n  example, expressions containing aggregates have \ncontext ~ QAggregateContext\n.\n  Expressions returning scalar values have \ncontext ~ QValueContext\n.\n\n\n\n\n\n\nsyntax\n is the particular SQL dialect this expression is written in. Note\n  that this is usually different than the \nsyntax\n for \nQ\n, because \nQ\n's syntax\n  refers to a particular syntax for \nSELECT\n expressions (a type implementing\n  \nIsSql92SelectSyntax\n), while \nQGenExpr\n's syntax usually refers to an\n  expression syntax (a type implementing \nIsSql92ExpressionSyntax\n). Of course,\n  since syntaxes are related, you can get from a \nQ\n \nSELECT\n syntax to a\n  \nQGenExpr\n \nsyntax\n with the \nSql92SelectExpressionSyntax\n type family.\n\n\n\n\n\n\nThus, a \nQGenExpr\n with syntax \nSql92SelectExpressionSyntax select\n can be\n  used in the \nFILTER\n clause of a query with type \nQ select db s a\n.\n\n\n\n\n\n\ns\n is a scoping parameter, which will match the \ns\n in \nQ\n.\n\n\n\n\n\n\na\n is the type of this expression. For example, expressions returning SQL\n  \nint\n values, will have Haskell type \nInt\n. This ensures that your SQL query\n  won't fail at run-time with a type error.\n\n\n\n\n\n\nBeam defines some specializations of \nQGenExpr\n for common uses.\n\n\ntype QExpr = QGenExpr QValueContext\ntype QAgg = QGenExpr QAggregateContext\ntype QOrd = QGenExpr QOrderingContext\ntype QWindowExpr = QGenExpr QWindowingContext\ntype QWindowFrame = QGenExpr QWindowFrameContext\ntype QGroupExpr = QGenExpr QGroupingContext\n\n\n\n\nThus, value expressions can be given the simpler type of \nQExpr syntax s a\n.\nExpressions containing aggregates are typed as \nQAgg syntax s a\n.\n\n\nA note on type inference\n\n\nThese types may seem incredibly complicated. Indeed, the safety that beam tries\nto provide requires these scary-looking types.\n\n\nBut alas, do not fear! Beam is also designed to assist type inference. For the\nmost part, you will rarely need to annotate these types in your code.\nOccassionally you will need to provide a type for the result of an expression.\nFor example, \nSELECT\ning just the literal \n1\n may cause an ambiguity, because\nthe compiler won't know which \nIntegral\n type to use. Beam provides an easy\nutility function \nas_\n for this. With \n-XTypeApplications\n enabled,\n\n\nas_ @Int (ambiguous expression)\n\n\n\n\nensures that \nambiguous expression\n has the type \nQGenExpr ctxt syntax s Int\n\nwith the \nctxt\n, \nsyntax\n, and \ns\n types appropriately inferred.\n\n\nSimple queries\n\n\nThe easiest query is simply getting all the rows in a specific table. If you\nhave a database object (something with type \nDatabaseSettings be db\n) with some\ntable or view entities, you can use the \nall_\n function to retrieve all rows in\na specific table or view.\n\n\nFor example, to retrieve all \nPersonT\n entries in the \nexampleDb\n we defined in\nthe last section, we can say \n\n\nall_ (persons exampleDb) :: Q syntax ExampleDb s (PersonT (QExpr s))\n\n\n\n\n\n\nNote\n\n\nWe give the full type of the query here for illustrative purposes only. There \nis no need to do so in your own code\n\n\n\n\nTwo things to note. Firstly, here \nPersonT\n is parameterized over the \nQExpr s\n\nhigher-kinded type. This means that each field in \nPersonT\n now contains a SQL\nexpression instead of a Haskell value. This is the magic that our parameterized\ntypes allow.\n\n\nThus,\n\n\npersonFirstName (all_ (persons exampleDb)) :: QExpr s Text\n\n\n\n\nand\n\n\npersonFirstName (Person \nJohn\n \nSmith\n 23 \njohn.smith@example.com\n \n8888888888\n :: Person) :: Text\n\n\n\n\nSecondly, the field type has the same scope variable as the entire query. This\nmeans, it can only be used in the scope of this query. You will never be able to\ninspect the type of \ns\n from outside \nQ\n.\n\n\nOnce we have a query in terms of \nQ\n, we can use the \nselect\n function from\n\nDatabase.Beam.Query\n to turn it into a select statement that can be run against\nthe backend.\n\n\nselect (all_ (persons exampleDb)) :: (...) =\n SqlSelect syntax Person\n\n\n\n\nThe \n...\n in the context represents a bunch of requirements for \nsyntax\n that\nGHC will generate.\n\n\nNormally, you'd ship this select statement off to a backend to run, but for the\npurposes of this tutorial, we can also ask beam to dump what the standard SQL\nexpression this query encodes.\n\n\ndumpSqlSelect (all_ (persons exampleDb))\nSELECT \nt0\n.\nfirst_name\n AS \nres0\n, \nt0\n.\nlast_name\n AS \nres1\n, \nt0\n.\nage\n AS \nres2\n, \nt0\n.\nemail\n AS \nres3\n, \nt0\n.\nphone\n AS \nres4\n FROM \nperson\n AS \nt0\n\n\n\n\n\nInternally, \ndumpSqlSelect\n uses a \nbeam-core\n provided syntax to generate\nstandard ANSI SQL expressions. Note that these expressions should not be shipped\nto a backend directly, as they may not be escaped properly. Still, it is useful\nto see what would run.\n\n\nA note on composability\n\n\nAll beam queries are \ncomposable\n. This means that you can freely mix values of\ntype \nQ\n in whichever way typechecks and expect a reasonable SQL query. This\ndiffers from the behavior of SQL, where the syntax for composing queries depends\non the structure of that query.\n\n\nFor example, suppose you wanted to fetch all rows of a table, filter them by a\ncondition, and then limit the amount of rows returned. In beam, \n\n\nConnecting to a database\n\n\nOkay, so we can print out a SQL statement, but how do we execute it against a\ndatabase? Beam provides a convenient \nMonadBeam\n type class that allows us to\nwrite queries in a backend agnostic manner. This is good-enough for most\napplications and preserves portability across databases. However, \nMonadBeam\n\ndoes not support features specific to each backend, nor does it guarantee the\nhighest-performance. Most backends provide additional methods to query a\ndatabase, and you should prefer these if you've committed to a particular\nbackend. For tutorial purposes, we will use the \nbeam-sqlite\n backend.\n\n\nFirst, install \nbeam-sqlite\n with \ncabal\n or \nstack\n:\n\n\n$ cabal install beam-sqlite\n# or\n$ stack install beam-sqlite\n\n\n\n\nNow, load \nbeam-sqlite\n in GHCi. \n\n\nPrelude\n import Database.Beam.Sqlite\nPrelude Database.Beam.Sqlite\n \n\n\n\n\nNow, in another terminal, load the example database provided. \n\n\n$ sqlite3 basics.db \n beam-sqlite/examples/basics.sql\n\n\n\n\nNow, back in GHCi, we can create a connection to this database.\n\n\nPrelude Database.Beam.Sqlite\n basics \n- open \nbasics.db\n\nPrelude Database.Beam.Sqlite\n withDatabase basics $ runSelectReturningList (select (all_ (persons exampleDb)))\n[ .. ]\n\n\n\n\nThe \nrunSelectReturningList\n function takes a \nSqlSelect\n for the given syntax\nand returns the results via a list.\n\n\nVoil\u00e0! We've successfully created our first query and run it against an example\ndatabase. We have now seen the major functionalities of the beam library. In the\nnext section we'll explore more advanced querying and using relationships\nbetween tables.", 
            "title": "Basic Queries"
        }, 
        {
            "location": "/user-guide/queries/basic/#data-types", 
            "text": "", 
            "title": "Data types"
        }, 
        {
            "location": "/user-guide/queries/basic/#the-q-data-type", 
            "text": "Beam queries are built using the  Q  data type.  Q 's signature is as follows  data Q syntax db s a  In this definition    syntax  is the particular dialect of SQL this  Q  monad will evaluate to.\n  Often times, this is any instance of  IsSql92SelectSyntax , but sometimes you\n  use syntax-specific features. For example, if you want to use named windows in\n  postgres, you'll likely have to specialize this to  PgSelectSyntax  from\n   Database.Beam.Postgres.Syntax .    db  is the type of the database (as we defined above). This is used to ensure\n  you only query database entities that are in scope in this database.    s  is the scope parameter. For the most part, you'll write your queries so\n  that they work over all  s . Beam manipulates this parameter internally to\n  ensure that the fields in your expressions are always in scope at run-time.    a  is the type of the result of the query.", 
            "title": "The Q data type"
        }, 
        {
            "location": "/user-guide/queries/basic/#the-qgenexpr-type", 
            "text": "While  Q  represents the result of whole queries (entire  SELECT s for example), QGenExpr  represents the type of SQL expressions.  QGenExpr  also takes some\ntype parameters:  data QGenExpr context syntax s a    context  is the particular way in which this expression is being used. For\n  example, expressions containing aggregates have  context ~ QAggregateContext .\n  Expressions returning scalar values have  context ~ QValueContext .    syntax  is the particular SQL dialect this expression is written in. Note\n  that this is usually different than the  syntax  for  Q , because  Q 's syntax\n  refers to a particular syntax for  SELECT  expressions (a type implementing\n   IsSql92SelectSyntax ), while  QGenExpr 's syntax usually refers to an\n  expression syntax (a type implementing  IsSql92ExpressionSyntax ). Of course,\n  since syntaxes are related, you can get from a  Q   SELECT  syntax to a\n   QGenExpr   syntax  with the  Sql92SelectExpressionSyntax  type family.    Thus, a  QGenExpr  with syntax  Sql92SelectExpressionSyntax select  can be\n  used in the  FILTER  clause of a query with type  Q select db s a .    s  is a scoping parameter, which will match the  s  in  Q .    a  is the type of this expression. For example, expressions returning SQL\n   int  values, will have Haskell type  Int . This ensures that your SQL query\n  won't fail at run-time with a type error.    Beam defines some specializations of  QGenExpr  for common uses.  type QExpr = QGenExpr QValueContext\ntype QAgg = QGenExpr QAggregateContext\ntype QOrd = QGenExpr QOrderingContext\ntype QWindowExpr = QGenExpr QWindowingContext\ntype QWindowFrame = QGenExpr QWindowFrameContext\ntype QGroupExpr = QGenExpr QGroupingContext  Thus, value expressions can be given the simpler type of  QExpr syntax s a .\nExpressions containing aggregates are typed as  QAgg syntax s a .", 
            "title": "The QGenExpr type"
        }, 
        {
            "location": "/user-guide/queries/basic/#a-note-on-type-inference", 
            "text": "These types may seem incredibly complicated. Indeed, the safety that beam tries\nto provide requires these scary-looking types.  But alas, do not fear! Beam is also designed to assist type inference. For the\nmost part, you will rarely need to annotate these types in your code.\nOccassionally you will need to provide a type for the result of an expression.\nFor example,  SELECT ing just the literal  1  may cause an ambiguity, because\nthe compiler won't know which  Integral  type to use. Beam provides an easy\nutility function  as_  for this. With  -XTypeApplications  enabled,  as_ @Int (ambiguous expression)  ensures that  ambiguous expression  has the type  QGenExpr ctxt syntax s Int \nwith the  ctxt ,  syntax , and  s  types appropriately inferred.", 
            "title": "A note on type inference"
        }, 
        {
            "location": "/user-guide/queries/basic/#simple-queries", 
            "text": "The easiest query is simply getting all the rows in a specific table. If you\nhave a database object (something with type  DatabaseSettings be db ) with some\ntable or view entities, you can use the  all_  function to retrieve all rows in\na specific table or view.  For example, to retrieve all  PersonT  entries in the  exampleDb  we defined in\nthe last section, we can say   all_ (persons exampleDb) :: Q syntax ExampleDb s (PersonT (QExpr s))   Note  We give the full type of the query here for illustrative purposes only. There \nis no need to do so in your own code   Two things to note. Firstly, here  PersonT  is parameterized over the  QExpr s \nhigher-kinded type. This means that each field in  PersonT  now contains a SQL\nexpression instead of a Haskell value. This is the magic that our parameterized\ntypes allow.  Thus,  personFirstName (all_ (persons exampleDb)) :: QExpr s Text  and  personFirstName (Person  John   Smith  23  john.smith@example.com   8888888888  :: Person) :: Text  Secondly, the field type has the same scope variable as the entire query. This\nmeans, it can only be used in the scope of this query. You will never be able to\ninspect the type of  s  from outside  Q .  Once we have a query in terms of  Q , we can use the  select  function from Database.Beam.Query  to turn it into a select statement that can be run against\nthe backend.  select (all_ (persons exampleDb)) :: (...) =  SqlSelect syntax Person  The  ...  in the context represents a bunch of requirements for  syntax  that\nGHC will generate.  Normally, you'd ship this select statement off to a backend to run, but for the\npurposes of this tutorial, we can also ask beam to dump what the standard SQL\nexpression this query encodes.  dumpSqlSelect (all_ (persons exampleDb))\nSELECT  t0 . first_name  AS  res0 ,  t0 . last_name  AS  res1 ,  t0 . age  AS  res2 ,  t0 . email  AS  res3 ,  t0 . phone  AS  res4  FROM  person  AS  t0   Internally,  dumpSqlSelect  uses a  beam-core  provided syntax to generate\nstandard ANSI SQL expressions. Note that these expressions should not be shipped\nto a backend directly, as they may not be escaped properly. Still, it is useful\nto see what would run.", 
            "title": "Simple queries"
        }, 
        {
            "location": "/user-guide/queries/basic/#a-note-on-composability", 
            "text": "All beam queries are  composable . This means that you can freely mix values of\ntype  Q  in whichever way typechecks and expect a reasonable SQL query. This\ndiffers from the behavior of SQL, where the syntax for composing queries depends\non the structure of that query.  For example, suppose you wanted to fetch all rows of a table, filter them by a\ncondition, and then limit the amount of rows returned. In beam,", 
            "title": "A note on composability"
        }, 
        {
            "location": "/user-guide/queries/basic/#connecting-to-a-database", 
            "text": "Okay, so we can print out a SQL statement, but how do we execute it against a\ndatabase? Beam provides a convenient  MonadBeam  type class that allows us to\nwrite queries in a backend agnostic manner. This is good-enough for most\napplications and preserves portability across databases. However,  MonadBeam \ndoes not support features specific to each backend, nor does it guarantee the\nhighest-performance. Most backends provide additional methods to query a\ndatabase, and you should prefer these if you've committed to a particular\nbackend. For tutorial purposes, we will use the  beam-sqlite  backend.  First, install  beam-sqlite  with  cabal  or  stack :  $ cabal install beam-sqlite\n# or\n$ stack install beam-sqlite  Now, load  beam-sqlite  in GHCi.   Prelude  import Database.Beam.Sqlite\nPrelude Database.Beam.Sqlite    Now, in another terminal, load the example database provided.   $ sqlite3 basics.db   beam-sqlite/examples/basics.sql  Now, back in GHCi, we can create a connection to this database.  Prelude Database.Beam.Sqlite  basics  - open  basics.db \nPrelude Database.Beam.Sqlite  withDatabase basics $ runSelectReturningList (select (all_ (persons exampleDb)))\n[ .. ]  The  runSelectReturningList  function takes a  SqlSelect  for the given syntax\nand returns the results via a list.  Voil\u00e0! We've successfully created our first query and run it against an example\ndatabase. We have now seen the major functionalities of the beam library. In the\nnext section we'll explore more advanced querying and using relationships\nbetween tables.", 
            "title": "Connecting to a database"
        }, 
        {
            "location": "/user-guide/queries/select/", 
            "text": "We've seen how to create simple queries from our schema. Beam supports other\nclauses in the SQL SELECT statement.\n\n\nFor these examples, we're going to use the \nbeam-sqlite\n backend with the\nprovided sample Chinook database. The Chinook database schema is modeled after a\nfictional record store. It provides several tables containing information on the\nmusic as well as the billing operations. Thus, it provides a good 'real-world'\ndemonstration of beam's capabalities.\n\n\nFirst, create a SQLite database from the included example.\n\n\n$ sqlite3 chinook.db \n beam-sqlite/examples/chinook.sql\n\n\n\n\nNow, load the chinook database schema in GHCi.\n\n\nPrelude Database.Beam.Sqlite\n :load beam-sqlite/examples/Chinook/Schema.hs\nPrelude Chinook.Schema\n chinook \n- open \nchinook.db\n\n\n\n\n\nOne more thing, before we see more complex examples, let's define a quick\nutility function.\n\n\nPrelude Chinook.Schema\n let withConnectionTutorial = withDatabaseDebug putStrLn chinook\n\n\n\n\nLet's test it!\n\n\nWe can run all our queries like:\n\n\nwithConnectionTutorial $ runSelectReturningList $ select $ \nquery\n\n\n\n\n\nLet's select all the tracks.\n\n\nwithConnectionTutorial $ runSelectReturningList $ select $ all_ (track chinookDb)\n\n\n\n\nFor the rest of the guide, we will also show the generated SQL code for both\nsqlite and postgres.\n\n\n\n                \n\n                    \n        \n\n            \n        \n\n            \nHaskell\n\n        \n\n    \n        \n\n            \nSqlite3\n\n        \n\n    \n        \n\n            \nPostgres\n\n        \n\n    \n        \n\n    \n        \n\n            \n        \n\n            \nall_ (track chinookDb)\n\n\n        \n\n    \n        \n\n            \nSELECT \nt0\n.\nTrackId\n AS \nres0\n,\n       \nt0\n.\nName\n AS \nres1\n,\n       \nt0\n.\nAlbumId\n AS \nres2\n,\n       \nt0\n.\nMediaTypeId\n AS \nres3\n,\n       \nt0\n.\nGenreId\n AS \nres4\n,\n       \nt0\n.\nComposer\n AS \nres5\n,\n       \nt0\n.\nMilliseconds\n AS \nres6\n,\n       \nt0\n.\nBytes\n AS \nres7\n,\n       \nt0\n.\nUnitPrice\n AS \nres8\n\nFROM \nTrack\n AS \nt0\n\n\n\n        \n\n    \n        \n\n            \nSELECT \nt0\n.\nTrackId\n AS \nres0\n,\n       \nt0\n.\nName\n AS \nres1\n,\n       \nt0\n.\nAlbumId\n AS \nres2\n,\n       \nt0\n.\nMediaTypeId\n AS \nres3\n,\n       \nt0\n.\nGenreId\n AS \nres4\n,\n       \nt0\n.\nComposer\n AS \nres5\n,\n       \nt0\n.\nMilliseconds\n AS \nres6\n,\n       \nt0\n.\nBytes\n AS \nres7\n,\n       \nt0\n.\nUnitPrice\n AS \nres8\n\nFROM \nTrack\n AS \nt0\n\n\n\n        \n\n    \n        \n\n    \n                \n\n                    \n\n\nWHERE\n clause\n\n\nWe've seen how to use \nall_\n to select all rows of a table. Sometimes, you would\nlike to filter results based on the result of some condition. For example,\nperhaps you would like to fetch all customers whose names start with \"Jo\". We\ncan filter over results using the \nfilter_\n function.\n\n\n\n                \n\n                    \n        \n\n            \n        \n\n            \nHaskell\n\n        \n\n    \n        \n\n            \nSqlite3\n\n        \n\n    \n        \n\n            \nPostgres\n\n        \n\n    \n        \n\n    \n        \n\n            \n        \n\n            \nfilter_ (\\customer -\n customerFirstName customer `like_` \nJo%\n) $\nall_ (customer chinookDb)\n\n\n        \n\n    \n        \n\n            \nSELECT \nt0\n.\nCustomerId\n AS \nres0\n,\n       \nt0\n.\nFirstName\n AS \nres1\n,\n       \nt0\n.\nLastName\n AS \nres2\n,\n       \nt0\n.\nCompany\n AS \nres3\n,\n       \nt0\n.\nAddress\n AS \nres4\n,\n       \nt0\n.\nCity\n AS \nres5\n,\n       \nt0\n.\nState\n AS \nres6\n,\n       \nt0\n.\nCountry\n AS \nres7\n,\n       \nt0\n.\nPostalCode\n AS \nres8\n,\n       \nt0\n.\nPhone\n AS \nres9\n,\n       \nt0\n.\nFax\n AS \nres10\n,\n       \nt0\n.\nEmail\n AS \nres11\n,\n       \nt0\n.\nSupportRepId\n AS \nres12\n\nFROM \nCustomer\n AS \nt0\n\nWHERE (\nt0\n.\nFirstName\n) LIKE (?)\n\n\n        \n\n    \n        \n\n            \nSELECT \nt0\n.\nCustomerId\n AS \nres0\n,\n       \nt0\n.\nFirstName\n AS \nres1\n,\n       \nt0\n.\nLastName\n AS \nres2\n,\n       \nt0\n.\nCompany\n AS \nres3\n,\n       \nt0\n.\nAddress\n AS \nres4\n,\n       \nt0\n.\nCity\n AS \nres5\n,\n       \nt0\n.\nState\n AS \nres6\n,\n       \nt0\n.\nCountry\n AS \nres7\n,\n       \nt0\n.\nPostalCode\n AS \nres8\n,\n       \nt0\n.\nPhone\n AS \nres9\n,\n       \nt0\n.\nFax\n AS \nres10\n,\n       \nt0\n.\nEmail\n AS \nres11\n,\n       \nt0\n.\nSupportRepId\n AS \nres12\n\nFROM \nCustomer\n AS \nt0\n\nWHERE (\nt0\n.\nFirstName\n) LIKE ('Jo%')\n\n\n        \n\n    \n        \n\n    \n                \n\n                    \n\n\nYou can use \n(\n.)\n and \n(||.)\n to combine boolean expressions, as you'd expect.\nFor example, to select all customers whose first name begins with \"Jo\", last\nname begins with \"S\", and who live in either California or Washington:\n\n\n\n                \n\n                    \n        \n\n            \n        \n\n            \nHaskell\n\n        \n\n    \n        \n\n            \nSqlite3\n\n        \n\n    \n        \n\n            \nPostgres\n\n        \n\n    \n        \n\n    \n        \n\n            \n        \n\n            \nfilter_ (\\customer -\n ((customerFirstName customer `like_` \nJo%\n) \n. (customerLastName customer `like_` \nS%\n)) \n.\n                      (addressState (customerAddress customer) ==. just_ \nCA\n ||. addressState (customerAddress customer) ==. just_ \nWA\n)) $\n        all_ (customer chinookDb)\n\n\n        \n\n    \n        \n\n            \nSELECT \nt0\n.\nCustomerId\n AS \nres0\n,\n       \nt0\n.\nFirstName\n AS \nres1\n,\n       \nt0\n.\nLastName\n AS \nres2\n,\n       \nt0\n.\nCompany\n AS \nres3\n,\n       \nt0\n.\nAddress\n AS \nres4\n,\n       \nt0\n.\nCity\n AS \nres5\n,\n       \nt0\n.\nState\n AS \nres6\n,\n       \nt0\n.\nCountry\n AS \nres7\n,\n       \nt0\n.\nPostalCode\n AS \nres8\n,\n       \nt0\n.\nPhone\n AS \nres9\n,\n       \nt0\n.\nFax\n AS \nres10\n,\n       \nt0\n.\nEmail\n AS \nres11\n,\n       \nt0\n.\nSupportRepId\n AS \nres12\n\nFROM \nCustomer\n AS \nt0\n\nWHERE (((\nt0\n.\nFirstName\n) LIKE (?))\n       AND ((\nt0\n.\nLastName\n) LIKE (?)))\n  AND (((\nt0\n.\nState\n)=(?))\n       OR ((\nt0\n.\nState\n)=(?)))\n\n\n        \n\n    \n        \n\n            \nSELECT \nt0\n.\nCustomerId\n AS \nres0\n,\n       \nt0\n.\nFirstName\n AS \nres1\n,\n       \nt0\n.\nLastName\n AS \nres2\n,\n       \nt0\n.\nCompany\n AS \nres3\n,\n       \nt0\n.\nAddress\n AS \nres4\n,\n       \nt0\n.\nCity\n AS \nres5\n,\n       \nt0\n.\nState\n AS \nres6\n,\n       \nt0\n.\nCountry\n AS \nres7\n,\n       \nt0\n.\nPostalCode\n AS \nres8\n,\n       \nt0\n.\nPhone\n AS \nres9\n,\n       \nt0\n.\nFax\n AS \nres10\n,\n       \nt0\n.\nEmail\n AS \nres11\n,\n       \nt0\n.\nSupportRepId\n AS \nres12\n\nFROM \nCustomer\n AS \nt0\n\nWHERE (((\nt0\n.\nFirstName\n) LIKE ('Jo%'))\n       AND ((\nt0\n.\nLastName\n) LIKE ('S%')))\n  AND (((\nt0\n.\nState\n) = ('CA'))\n       OR ((\nt0\n.\nState\n) = ('WA')))\n\n\n        \n\n    \n        \n\n    \n                \n\n                    \n\n\n\n\nNote\n\n\nWe had to use the \njust_\n function above to compare\n\naddressState (customerAddress customer)\n. This is because \naddressState\n(customerAddress customer)\n represents a nullable column which beam types as\n\nMaybe Text\n. Just as in Haskell, we need to explicitly unwrap the \nMaybe\n\ntype. This is an example of beam offering stronger typing than SQL itself.\n\n\n\n\nLIMIT\n/\nOFFSET\n support\n\n\nThe \nlimit_\n and \noffset_\n functions can be used to truncate the result set at a\ncertain length and fetch different portions of the result. They correspond to\nthe \nLIMIT\n and \nOFFSET\n SQL constructs.\n\n\n\n                \n\n                    \n        \n\n            \n        \n\n            \nHaskell\n\n        \n\n    \n        \n\n            \nSqlite3\n\n        \n\n    \n        \n\n            \nPostgres\n\n        \n\n    \n        \n\n    \n        \n\n            \n        \n\n            \nlimit_ 10 $ offset_ 100 $\nfilter_ (\\customer -\n ((customerFirstName customer `like_` \nJo%\n) \n. (customerLastName customer `like_` \nS%\n)) \n.\n                      (addressState (customerAddress customer) ==. just_ \nCA\n ||. addressState (customerAddress customer) ==. just_ \nWA\n)) $\n        all_ (customer chinookDb)\n\n\n        \n\n    \n        \n\n            \nSELECT \nt0\n.\nCustomerId\n AS \nres0\n,\n       \nt0\n.\nFirstName\n AS \nres1\n,\n       \nt0\n.\nLastName\n AS \nres2\n,\n       \nt0\n.\nCompany\n AS \nres3\n,\n       \nt0\n.\nAddress\n AS \nres4\n,\n       \nt0\n.\nCity\n AS \nres5\n,\n       \nt0\n.\nState\n AS \nres6\n,\n       \nt0\n.\nCountry\n AS \nres7\n,\n       \nt0\n.\nPostalCode\n AS \nres8\n,\n       \nt0\n.\nPhone\n AS \nres9\n,\n       \nt0\n.\nFax\n AS \nres10\n,\n       \nt0\n.\nEmail\n AS \nres11\n,\n       \nt0\n.\nSupportRepId\n AS \nres12\n\nFROM \nCustomer\n AS \nt0\n\nWHERE (((\nt0\n.\nFirstName\n) LIKE (?))\n       AND ((\nt0\n.\nLastName\n) LIKE (?)))\n  AND (((\nt0\n.\nState\n)=(?))\n       OR ((\nt0\n.\nState\n)=(?)))\nLIMIT 10\nOFFSET 100\n\n\n        \n\n    \n        \n\n            \nSELECT \nt0\n.\nCustomerId\n AS \nres0\n,\n       \nt0\n.\nFirstName\n AS \nres1\n,\n       \nt0\n.\nLastName\n AS \nres2\n,\n       \nt0\n.\nCompany\n AS \nres3\n,\n       \nt0\n.\nAddress\n AS \nres4\n,\n       \nt0\n.\nCity\n AS \nres5\n,\n       \nt0\n.\nState\n AS \nres6\n,\n       \nt0\n.\nCountry\n AS \nres7\n,\n       \nt0\n.\nPostalCode\n AS \nres8\n,\n       \nt0\n.\nPhone\n AS \nres9\n,\n       \nt0\n.\nFax\n AS \nres10\n,\n       \nt0\n.\nEmail\n AS \nres11\n,\n       \nt0\n.\nSupportRepId\n AS \nres12\n\nFROM \nCustomer\n AS \nt0\n\nWHERE (((\nt0\n.\nFirstName\n) LIKE ('Jo%'))\n       AND ((\nt0\n.\nLastName\n) LIKE ('S%')))\n  AND (((\nt0\n.\nState\n) = ('CA'))\n       OR ((\nt0\n.\nState\n) = ('WA')))\nLIMIT 10\nOFFSET 100\n\n\n        \n\n    \n        \n\n    \n                \n\n                    \n\n\n\n\nNote\n\n\nNested \nlimit_\ns and \noffset_\ns compose in the way you'd expect without\ngenerating extraneous subqueries.\n\n\n\n\n\n\nWarning\n\n\nNote that the order of the \nlimit_\n and \noffset_\n functions matter.\nOffseting an already limited result is not the same as limiting an offseted\nresult. For example, if you offset three rows into a limited set of five\nresults, you will get at most two rows. On the other hand, if you offset\nthree rows and then limit the result to the next five, you may get up to\nfive. Beam will generate exactly the query you specify. Notice the\ndifference below, where the order of the clauses made beam generate a query\nthat returns no results.\n\n\n\n\n\n                \n\n                    \n        \n\n            \n        \n\n            \nHaskell\n\n        \n\n    \n        \n\n            \nSqlite3\n\n        \n\n    \n        \n\n            \nPostgres\n\n        \n\n    \n        \n\n    \n        \n\n            \n        \n\n            \noffset_ 100 $ limit_ 10 $\nfilter_ (\\customer -\n ((customerFirstName customer `like_` \nJo%\n) \n. (customerLastName customer `like_` \nS%\n)) \n.\n                      (addressState (customerAddress customer) ==. just_ \nCA\n ||. addressState (customerAddress customer) ==. just_ \nWA\n)) $\n        all_ (customer chinookDb)\n\n\n        \n\n    \n        \n\n            \nSELECT \nt0\n.\nCustomerId\n AS \nres0\n,\n       \nt0\n.\nFirstName\n AS \nres1\n,\n       \nt0\n.\nLastName\n AS \nres2\n,\n       \nt0\n.\nCompany\n AS \nres3\n,\n       \nt0\n.\nAddress\n AS \nres4\n,\n       \nt0\n.\nCity\n AS \nres5\n,\n       \nt0\n.\nState\n AS \nres6\n,\n       \nt0\n.\nCountry\n AS \nres7\n,\n       \nt0\n.\nPostalCode\n AS \nres8\n,\n       \nt0\n.\nPhone\n AS \nres9\n,\n       \nt0\n.\nFax\n AS \nres10\n,\n       \nt0\n.\nEmail\n AS \nres11\n,\n       \nt0\n.\nSupportRepId\n AS \nres12\n\nFROM \nCustomer\n AS \nt0\n\nWHERE (((\nt0\n.\nFirstName\n) LIKE (?))\n       AND ((\nt0\n.\nLastName\n) LIKE (?)))\n  AND (((\nt0\n.\nState\n)=(?))\n       OR ((\nt0\n.\nState\n)=(?)))\nLIMIT 0\nOFFSET 100\n\n\n        \n\n    \n        \n\n            \nSELECT \nt0\n.\nCustomerId\n AS \nres0\n,\n       \nt0\n.\nFirstName\n AS \nres1\n,\n       \nt0\n.\nLastName\n AS \nres2\n,\n       \nt0\n.\nCompany\n AS \nres3\n,\n       \nt0\n.\nAddress\n AS \nres4\n,\n       \nt0\n.\nCity\n AS \nres5\n,\n       \nt0\n.\nState\n AS \nres6\n,\n       \nt0\n.\nCountry\n AS \nres7\n,\n       \nt0\n.\nPostalCode\n AS \nres8\n,\n       \nt0\n.\nPhone\n AS \nres9\n,\n       \nt0\n.\nFax\n AS \nres10\n,\n       \nt0\n.\nEmail\n AS \nres11\n,\n       \nt0\n.\nSupportRepId\n AS \nres12\n\nFROM \nCustomer\n AS \nt0\n\nWHERE (((\nt0\n.\nFirstName\n) LIKE ('Jo%'))\n       AND ((\nt0\n.\nLastName\n) LIKE ('S%')))\n  AND (((\nt0\n.\nState\n) = ('CA'))\n       OR ((\nt0\n.\nState\n) = ('WA')))\nLIMIT 0\nOFFSET 100", 
            "title": "More complex SELECTs"
        }, 
        {
            "location": "/user-guide/queries/select/#where-clause", 
            "text": "We've seen how to use  all_  to select all rows of a table. Sometimes, you would\nlike to filter results based on the result of some condition. For example,\nperhaps you would like to fetch all customers whose names start with \"Jo\". We\ncan filter over results using the  filter_  function.  \n                 \n                    \n         \n            \n         \n             Haskell \n         \n    \n         \n             Sqlite3 \n         \n    \n         \n             Postgres \n         \n    \n         \n    \n         \n            \n         \n             filter_ (\\customer -  customerFirstName customer `like_`  Jo% ) $\nall_ (customer chinookDb) \n         \n    \n         \n             SELECT  t0 . CustomerId  AS  res0 ,\n        t0 . FirstName  AS  res1 ,\n        t0 . LastName  AS  res2 ,\n        t0 . Company  AS  res3 ,\n        t0 . Address  AS  res4 ,\n        t0 . City  AS  res5 ,\n        t0 . State  AS  res6 ,\n        t0 . Country  AS  res7 ,\n        t0 . PostalCode  AS  res8 ,\n        t0 . Phone  AS  res9 ,\n        t0 . Fax  AS  res10 ,\n        t0 . Email  AS  res11 ,\n        t0 . SupportRepId  AS  res12 \nFROM  Customer  AS  t0 \nWHERE ( t0 . FirstName ) LIKE (?) \n         \n    \n         \n             SELECT  t0 . CustomerId  AS  res0 ,\n        t0 . FirstName  AS  res1 ,\n        t0 . LastName  AS  res2 ,\n        t0 . Company  AS  res3 ,\n        t0 . Address  AS  res4 ,\n        t0 . City  AS  res5 ,\n        t0 . State  AS  res6 ,\n        t0 . Country  AS  res7 ,\n        t0 . PostalCode  AS  res8 ,\n        t0 . Phone  AS  res9 ,\n        t0 . Fax  AS  res10 ,\n        t0 . Email  AS  res11 ,\n        t0 . SupportRepId  AS  res12 \nFROM  Customer  AS  t0 \nWHERE ( t0 . FirstName ) LIKE ('Jo%') \n         \n    \n         \n    \n                 \n                      You can use  ( .)  and  (||.)  to combine boolean expressions, as you'd expect.\nFor example, to select all customers whose first name begins with \"Jo\", last\nname begins with \"S\", and who live in either California or Washington:  \n                 \n                    \n         \n            \n         \n             Haskell \n         \n    \n         \n             Sqlite3 \n         \n    \n         \n             Postgres \n         \n    \n         \n    \n         \n            \n         \n             filter_ (\\customer -  ((customerFirstName customer `like_`  Jo% )  . (customerLastName customer `like_`  S% ))  .\n                      (addressState (customerAddress customer) ==. just_  CA  ||. addressState (customerAddress customer) ==. just_  WA )) $\n        all_ (customer chinookDb) \n         \n    \n         \n             SELECT  t0 . CustomerId  AS  res0 ,\n        t0 . FirstName  AS  res1 ,\n        t0 . LastName  AS  res2 ,\n        t0 . Company  AS  res3 ,\n        t0 . Address  AS  res4 ,\n        t0 . City  AS  res5 ,\n        t0 . State  AS  res6 ,\n        t0 . Country  AS  res7 ,\n        t0 . PostalCode  AS  res8 ,\n        t0 . Phone  AS  res9 ,\n        t0 . Fax  AS  res10 ,\n        t0 . Email  AS  res11 ,\n        t0 . SupportRepId  AS  res12 \nFROM  Customer  AS  t0 \nWHERE ((( t0 . FirstName ) LIKE (?))\n       AND (( t0 . LastName ) LIKE (?)))\n  AND ((( t0 . State )=(?))\n       OR (( t0 . State )=(?))) \n         \n    \n         \n             SELECT  t0 . CustomerId  AS  res0 ,\n        t0 . FirstName  AS  res1 ,\n        t0 . LastName  AS  res2 ,\n        t0 . Company  AS  res3 ,\n        t0 . Address  AS  res4 ,\n        t0 . City  AS  res5 ,\n        t0 . State  AS  res6 ,\n        t0 . Country  AS  res7 ,\n        t0 . PostalCode  AS  res8 ,\n        t0 . Phone  AS  res9 ,\n        t0 . Fax  AS  res10 ,\n        t0 . Email  AS  res11 ,\n        t0 . SupportRepId  AS  res12 \nFROM  Customer  AS  t0 \nWHERE ((( t0 . FirstName ) LIKE ('Jo%'))\n       AND (( t0 . LastName ) LIKE ('S%')))\n  AND ((( t0 . State ) = ('CA'))\n       OR (( t0 . State ) = ('WA'))) \n         \n    \n         \n    \n                 \n                       Note  We had to use the  just_  function above to compare addressState (customerAddress customer) . This is because  addressState\n(customerAddress customer)  represents a nullable column which beam types as Maybe Text . Just as in Haskell, we need to explicitly unwrap the  Maybe \ntype. This is an example of beam offering stronger typing than SQL itself.", 
            "title": "WHERE clause"
        }, 
        {
            "location": "/user-guide/queries/select/#limitoffset-support", 
            "text": "The  limit_  and  offset_  functions can be used to truncate the result set at a\ncertain length and fetch different portions of the result. They correspond to\nthe  LIMIT  and  OFFSET  SQL constructs.  \n                 \n                    \n         \n            \n         \n             Haskell \n         \n    \n         \n             Sqlite3 \n         \n    \n         \n             Postgres \n         \n    \n         \n    \n         \n            \n         \n             limit_ 10 $ offset_ 100 $\nfilter_ (\\customer -  ((customerFirstName customer `like_`  Jo% )  . (customerLastName customer `like_`  S% ))  .\n                      (addressState (customerAddress customer) ==. just_  CA  ||. addressState (customerAddress customer) ==. just_  WA )) $\n        all_ (customer chinookDb) \n         \n    \n         \n             SELECT  t0 . CustomerId  AS  res0 ,\n        t0 . FirstName  AS  res1 ,\n        t0 . LastName  AS  res2 ,\n        t0 . Company  AS  res3 ,\n        t0 . Address  AS  res4 ,\n        t0 . City  AS  res5 ,\n        t0 . State  AS  res6 ,\n        t0 . Country  AS  res7 ,\n        t0 . PostalCode  AS  res8 ,\n        t0 . Phone  AS  res9 ,\n        t0 . Fax  AS  res10 ,\n        t0 . Email  AS  res11 ,\n        t0 . SupportRepId  AS  res12 \nFROM  Customer  AS  t0 \nWHERE ((( t0 . FirstName ) LIKE (?))\n       AND (( t0 . LastName ) LIKE (?)))\n  AND ((( t0 . State )=(?))\n       OR (( t0 . State )=(?)))\nLIMIT 10\nOFFSET 100 \n         \n    \n         \n             SELECT  t0 . CustomerId  AS  res0 ,\n        t0 . FirstName  AS  res1 ,\n        t0 . LastName  AS  res2 ,\n        t0 . Company  AS  res3 ,\n        t0 . Address  AS  res4 ,\n        t0 . City  AS  res5 ,\n        t0 . State  AS  res6 ,\n        t0 . Country  AS  res7 ,\n        t0 . PostalCode  AS  res8 ,\n        t0 . Phone  AS  res9 ,\n        t0 . Fax  AS  res10 ,\n        t0 . Email  AS  res11 ,\n        t0 . SupportRepId  AS  res12 \nFROM  Customer  AS  t0 \nWHERE ((( t0 . FirstName ) LIKE ('Jo%'))\n       AND (( t0 . LastName ) LIKE ('S%')))\n  AND ((( t0 . State ) = ('CA'))\n       OR (( t0 . State ) = ('WA')))\nLIMIT 10\nOFFSET 100 \n         \n    \n         \n    \n                 \n                       Note  Nested  limit_ s and  offset_ s compose in the way you'd expect without\ngenerating extraneous subqueries.    Warning  Note that the order of the  limit_  and  offset_  functions matter.\nOffseting an already limited result is not the same as limiting an offseted\nresult. For example, if you offset three rows into a limited set of five\nresults, you will get at most two rows. On the other hand, if you offset\nthree rows and then limit the result to the next five, you may get up to\nfive. Beam will generate exactly the query you specify. Notice the\ndifference below, where the order of the clauses made beam generate a query\nthat returns no results.   \n                 \n                    \n         \n            \n         \n             Haskell \n         \n    \n         \n             Sqlite3 \n         \n    \n         \n             Postgres \n         \n    \n         \n    \n         \n            \n         \n             offset_ 100 $ limit_ 10 $\nfilter_ (\\customer -  ((customerFirstName customer `like_`  Jo% )  . (customerLastName customer `like_`  S% ))  .\n                      (addressState (customerAddress customer) ==. just_  CA  ||. addressState (customerAddress customer) ==. just_  WA )) $\n        all_ (customer chinookDb) \n         \n    \n         \n             SELECT  t0 . CustomerId  AS  res0 ,\n        t0 . FirstName  AS  res1 ,\n        t0 . LastName  AS  res2 ,\n        t0 . Company  AS  res3 ,\n        t0 . Address  AS  res4 ,\n        t0 . City  AS  res5 ,\n        t0 . State  AS  res6 ,\n        t0 . Country  AS  res7 ,\n        t0 . PostalCode  AS  res8 ,\n        t0 . Phone  AS  res9 ,\n        t0 . Fax  AS  res10 ,\n        t0 . Email  AS  res11 ,\n        t0 . SupportRepId  AS  res12 \nFROM  Customer  AS  t0 \nWHERE ((( t0 . FirstName ) LIKE (?))\n       AND (( t0 . LastName ) LIKE (?)))\n  AND ((( t0 . State )=(?))\n       OR (( t0 . State )=(?)))\nLIMIT 0\nOFFSET 100 \n         \n    \n         \n             SELECT  t0 . CustomerId  AS  res0 ,\n        t0 . FirstName  AS  res1 ,\n        t0 . LastName  AS  res2 ,\n        t0 . Company  AS  res3 ,\n        t0 . Address  AS  res4 ,\n        t0 . City  AS  res5 ,\n        t0 . State  AS  res6 ,\n        t0 . Country  AS  res7 ,\n        t0 . PostalCode  AS  res8 ,\n        t0 . Phone  AS  res9 ,\n        t0 . Fax  AS  res10 ,\n        t0 . Email  AS  res11 ,\n        t0 . SupportRepId  AS  res12 \nFROM  Customer  AS  t0 \nWHERE ((( t0 . FirstName ) LIKE ('Jo%'))\n       AND (( t0 . LastName ) LIKE ('S%')))\n  AND ((( t0 . State ) = ('CA'))\n       OR (( t0 . State ) = ('WA')))\nLIMIT 0\nOFFSET 100", 
            "title": "LIMIT/OFFSET support"
        }, 
        {
            "location": "/user-guide/queries/ordering/", 
            "text": "Usually, queries are ordered before \nLIMIT\n and \nOFFSET\n are applied. Beam\nsupports the standard SQL \nORDER BY\n construct through the \norderBy_\n function.\n\n\norderBy_\n works like the Haskell function \nsortBy\n, wich some restructions. Its\nfirst argument is a function which takes as input the output of the given query.\nThe function should return a sorting key, which is either a single sort ordering\nor a tuple of them. A sort ordering specifies an expression and a direction by\nwhich to sort. The result is then sorted lexicographically based on these sort\nexpressions. The second argument to \norderBy_\n is the query whose results to\nsort.\n\n\nUse the \nasc_\n and \ndesc_\n functions to specify the sort ordering over an\narbitrary expression.\n\n\nFor example, to get the first ten albums when sorted lexicographically, use\n\n\n\n                \n\n                    \n        \n\n            \n        \n\n            \nHaskell\n\n        \n\n    \n        \n\n            \nSqlite3\n\n        \n\n    \n        \n\n            \nPostgres\n\n        \n\n    \n        \n\n    \n        \n\n            \n        \n\n            \nlimit_ 10 $\norderBy_ (asc_ . albumTitle) $\nall_ (album chinookDb)\n\n\n        \n\n    \n        \n\n            \nSELECT \nt0\n.\nAlbumId\n AS \nres0\n,\n       \nt0\n.\nTitle\n AS \nres1\n,\n       \nt0\n.\nArtistId\n AS \nres2\n\nFROM \nAlbum\n AS \nt0\n\nORDER BY \nt0\n.\nTitle\n ASC\nLIMIT 10\n\n\n        \n\n    \n        \n\n            \nSELECT \nt0\n.\nAlbumId\n AS \nres0\n,\n       \nt0\n.\nTitle\n AS \nres1\n,\n       \nt0\n.\nArtistId\n AS \nres2\n\nFROM \nAlbum\n AS \nt0\n\nORDER BY \nt0\n.\nTitle\n ASC\nLIMIT 10\n\n\n        \n\n    \n        \n\n    \n                \n\n                    \n\n\nAgain, note that the ordering in which you apply the \nlimit_\n and \norderBy_\n\nmatters. In general, you want to sort before you limit or offset, to keep your\nresult set stable. However, if you really want to sort a limited number of\narbitrarily chosen rows, you can use a different ordering.\n\n\n\n                \n\n                    \n        \n\n            \n        \n\n            \nHaskell\n\n        \n\n    \n        \n\n            \nSqlite3\n\n        \n\n    \n        \n\n            \nPostgres\n\n        \n\n    \n        \n\n    \n        \n\n            \n        \n\n            \norderBy_ (asc_ . albumTitle) $\nlimit_ 10 $\nall_ (album chinookDb)\n\n\n        \n\n    \n        \n\n            \nSELECT \nt0\n.\nres0\n AS \nres0\n,\n       \nt0\n.\nres1\n AS \nres1\n,\n       \nt0\n.\nres2\n AS \nres2\n\nFROM\n  (SELECT \nt0\n.\nAlbumId\n AS \nres0\n,\n          \nt0\n.\nTitle\n AS \nres1\n,\n          \nt0\n.\nArtistId\n AS \nres2\n\n   FROM \nAlbum\n AS \nt0\n\n   LIMIT 10) AS \nt0\n\nORDER BY \nt0\n.\nres1\n ASC\n\n\n        \n\n    \n        \n\n            \nSELECT \nt0\n.\nres0\n AS \nres0\n,\n       \nt0\n.\nres1\n AS \nres1\n,\n       \nt0\n.\nres2\n AS \nres2\n\nFROM\n  (SELECT \nt0\n.\nAlbumId\n AS \nres0\n,\n          \nt0\n.\nTitle\n AS \nres1\n,\n          \nt0\n.\nArtistId\n AS \nres2\n\n   FROM \nAlbum\n AS \nt0\n\n   LIMIT 10) AS \nt0\n\nORDER BY \nt0\n.\nres1\n ASC", 
            "title": "Ordering"
        }, 
        {
            "location": "/user-guide/queries/relationships/", 
            "text": "Relational databases are so-named because they're good at expressing relations\namong data and providing related data in queries. Beam exposes these features in\nits DSL.\n\n\nFor these examples, we're going to use the \nbeam-sqlite\n backend with the\nprovided sample Chinook database.\n\n\nFirst, create a SQLite database from the included example.\n\n\n sqlite3 chinook.db \n beam-sqlite/examples/chinook.sql\n\n\n\n\nNow, load the chinook database schema in GHCi.\n\n\nPrelude Database.Beam.Sqlite\n :load beam-sqlite/examples/Chinook/Schema.hs\nPrelude Chinook.Schema\n chinook \n- open \nchinook.db\n\n\n\n\n\nOne more thing, before we explore how beam handles relationships. Before we do, let's define a quick utility function.\n\n\nPrelude Chinook.Schema\n let withConnectionTutorial = withDatabaseDebug putStrLn chinook\n\n\n\n\nThis function prints each of our queries to standard output before running them.\nUsing this function will let us see what SQL is executing.\n\n\nOne-to-many\n\n\nBeam supports querying for one-to-many joins. For example, to get every\n\nInvoiceLine\n for each \nInvoice\n, use the \noneToMany_\n combinator.\n\n\n\n                \n\n                    \n        \n\n            \n        \n\n            \nHaskell\n\n        \n\n    \n        \n\n            \nSqlite3\n\n        \n\n    \n        \n\n            \nPostgres\n\n        \n\n    \n        \n\n    \n        \n\n            \n        \n\n            \ndo i \n- all_ (invoice chinookDb)\n   ln \n- oneToMany_ (invoiceLine chinookDb) invoiceLineInvoice i\n   pure (i, ln)\n\n\n        \n\n    \n        \n\n            \nSELECT \nt0\n.\nInvoiceId\n AS \nres0\n,\n       \nt0\n.\nCustomerId\n AS \nres1\n,\n       \nt0\n.\nInvoiceDate\n AS \nres2\n,\n       \nt0\n.\nBillingAddress\n AS \nres3\n,\n       \nt0\n.\nBillingCity\n AS \nres4\n,\n       \nt0\n.\nBillingState\n AS \nres5\n,\n       \nt0\n.\nBillingCountry\n AS \nres6\n,\n       \nt0\n.\nBillingPostalCode\n AS \nres7\n,\n       \nt0\n.\nTotal\n AS \nres8\n,\n       \nt1\n.\nInvoiceLineId\n AS \nres9\n,\n       \nt1\n.\nInvoiceId\n AS \nres10\n,\n       \nt1\n.\nTrackId\n AS \nres11\n,\n       \nt1\n.\nUnitPrice\n AS \nres12\n,\n       \nt1\n.\nQuantity\n AS \nres13\n\nFROM \nInvoice\n AS \nt0\n\nINNER JOIN \nInvoiceLine\n AS \nt1\n ON (\nt1\n.\nInvoiceId\n)=(\nt0\n.\nInvoiceId\n)\n\n\n        \n\n    \n        \n\n            \nSELECT \nt0\n.\nInvoiceId\n AS \nres0\n,\n       \nt0\n.\nCustomerId\n AS \nres1\n,\n       \nt0\n.\nInvoiceDate\n AS \nres2\n,\n       \nt0\n.\nBillingAddress\n AS \nres3\n,\n       \nt0\n.\nBillingCity\n AS \nres4\n,\n       \nt0\n.\nBillingState\n AS \nres5\n,\n       \nt0\n.\nBillingCountry\n AS \nres6\n,\n       \nt0\n.\nBillingPostalCode\n AS \nres7\n,\n       \nt0\n.\nTotal\n AS \nres8\n,\n       \nt1\n.\nInvoiceLineId\n AS \nres9\n,\n       \nt1\n.\nInvoiceId\n AS \nres10\n,\n       \nt1\n.\nTrackId\n AS \nres11\n,\n       \nt1\n.\nUnitPrice\n AS \nres12\n,\n       \nt1\n.\nQuantity\n AS \nres13\n\nFROM \nInvoice\n AS \nt0\n\nINNER JOIN \nInvoiceLine\n AS \nt1\n ON (\nt1\n.\nInvoiceId\n) = (\nt0\n.\nInvoiceId\n)\n\n\n        \n\n    \n        \n\n    \n                \n\n                    \n\n\nOr, if you have an actual \nInvoice\n (called \noneInvoice\n) and you want all the\nassociated \nInvoiceLine\ns, you can use \nval_\n to convert \noneInvoice\n to the SQL\nexpression level.\n\n\noneToMany_ (invoiceLine chinookDb) invoiceLineInvoice (val_ i)\n\n\n\n\nIf you find yourself repeating yourself constantly, you can define a helper.\n\n\ninvoiceLines_ :: OneToMany InvoiceT InvoiceLineT\ninvoiceLines_ = oneToMany_ (invoiceLine chinookDb) invoiceLineInvoice\n\n\n\n\nThen the above queries become\n\n\ndo i \n- all_ (invoice chinookDb)\n   ln \n- invoiceLines_ i\n\n\n\n\nand\n\n\ninvoiceLines (val_ i)\n\n\n\n\nNullable columns\n\n\nIf you have a nullable foreign key in your many table, you can use\n\noneToManyOptional_\n and \nOneToManyOptional\n, respectively. For example, \n\n\nOne-to-one\n\n\nOne to one relationships are a special case of one to many relationships, save\nfor a unique constraint on one column. Thus, there are no special constructs for\none-to-one relationships.\n\n\nFor convenience, \noneToOne_\n and \nOneToOne\n are equivalent to \noneToMany_\n and\n\nOneToMany\n. Additionally, \noneToMaybe_\n and \nOneToMaybe\n correspond to\n\noneToManyOptional_\n and \nOneToManyOptional\n.\n\n\nMany-to-many\n\n\nMany to many relationships require a linking table, with foreign keys to each\ntable part of the relationship.\n\n\nThe \nmanyToMany_\n construct can be used to fetch both, one, or no sides of a\nmany-to-many relationship.\n\n\nmanyToMany_ :: ( Database db, Table joinThrough\n               , Table left, Table right\n               , Sql92SelectSanityCheck syntax\n               , IsSql92SelectSyntax syntax\n\n               , SqlOrd (QExpr (Sql92SelectExpressionSyntax syntax) s) (PrimaryKey left g)\n               , SqlOrd (QExpr (Sql92SelectExpressionSyntax syntax) s) (PrimaryKey right h) )\n            =\n DatabaseEntity be db (TableEntity joinThrough)\n            -\n (joinThrough (QExpr (Sql92SelectExpressionSyntax syntax) s) -\n PrimaryKey left g)\n            -\n (joinThrough (QExpr (Sql92SelectExpressionSyntax syntax) s) -\n PrimaryKey right h)\n            -\n Q syntax db s (left g) -\n Q syntax db s (right h)\n            -\n Q syntax db s (left g, right h)\n\n\n\n\nThis reads: for any database \ndb\n; tables \njoinThrough\n, \nleft\n, and \nright\n;\nand sane select syntax \nsyntax\n, where the primary keys of \nleft\n and \nright\n\nare comparable as value expressions and we have some way of extracting a primary\nkey of \nleft\n and \nright\n from \njoinThrough\n, associate all entries of \nleft\n\nwith those of \nright\n through \njoinThrough\n and return the results of \nleft\n and\n\nright\n.\n\n\nFor example, \n\n\nMany-to-many with arbitrary data\n\n\nSometimes you want to have additional data for each relationship. For this, use\n\nmanyToManyPassthrough_\n.\n\n\nmanyToManyPassthrough_ \n    :: ( Database db, Table joinThrough\n       , Table left, Table right\n       , Sql92SelectSanityCheck syntax\n       , IsSql92SelectSyntax syntax\n\n       , SqlOrd (QExpr (Sql92SelectExpressionSyntax syntax) s) (PrimaryKey left g)\n       , SqlOrd (QExpr (Sql92SelectExpressionSyntax syntax) s) (PrimaryKey right h) )\n    =\n DatabaseEntity be db (TableEntity joinThrough)\n    -\n (joinThrough (QExpr (Sql92SelectExpressionSyntax syntax) s) -\n PrimaryKey left g)\n    -\n (joinThrough (QExpr (Sql92SelectExpressionSyntax syntax) s) -\n PrimaryKey right h)\n    -\n Q syntax db s (left g) -\n Q syntax db s (right h)\n    -\n Q syntax db s (joinThrough (QExpr (Sql92SelectExpressionSyntax syntax) s), left g, right h)\n\n\n\n\nUnder the hood \nmanyToMany_\n is defined simply as \n\n\nmanyToMany_ = fmap (\\(_, left, right) -\n (left, right)) manyToManyPassthrough_\n\n\n\n\n\n\nTODO\n\n\nIt would be nice to have a \nManyToMany\n type or some equivalent.\n\n\n\n\nArbitrary Joins\n\n\nJoins with arbitrary conditions can be specified using the \njoin_\n construct.\n\n\nOuter joins\n\n\nLeft and right joins\n\n\nLeft joins with arbitrary conditions can be specified with the \nleftJoin_\n\nconstruct. \nleftJoin_\n takes a table and a join condition. It associates each\nresult record with a record of the table given or an fully NULL row of that\ntable in case no row matches. For this reason, the result of \nleftJoin_\n has an\nextra \nNullable\n column tag, which converts each field into the corresponding\n\nMaybe\n type.\n\n\n\n\nNote\n\n\nThe table parameter passed in as the join condition does not have a \n\nNullable\n column tag. The join condition should be written as if a \nconcrete row from that table exists.\n\n\n\n\nFor example, to get every artist along with their albums, but always including\nevery artist, use \nleftJoin_\n as follows.\n\n\n\n                \n\n                    \n        \n\n            \n        \n\n            \nHaskell\n\n        \n\n    \n        \n\n            \nSqlite3\n\n        \n\n    \n        \n\n            \nPostgres\n\n        \n\n    \n        \n\n    \n        \n\n            \n        \n\n            \ndo artist \n- all_ (artist chinookDb)\n   album  \n- leftJoin_ (album chinookDb) (\\album -\n albumArtist album ==. primaryKey artist)\n   pure (artist, album)\n\n\n        \n\n    \n        \n\n            \nSELECT \nt0\n.\nArtistId\n AS \nres0\n,\n       \nt0\n.\nName\n AS \nres1\n,\n       \nt1\n.\nAlbumId\n AS \nres2\n,\n       \nt1\n.\nTitle\n AS \nres3\n,\n       \nt1\n.\nArtistId\n AS \nres4\n\nFROM \nArtist\n AS \nt0\n\nLEFT JOIN \nAlbum\n AS \nt1\n ON (\nt1\n.\nArtistId\n)=(\nt0\n.\nArtistId\n)\n\n\n        \n\n    \n        \n\n            \nSELECT \nt0\n.\nArtistId\n AS \nres0\n,\n       \nt0\n.\nName\n AS \nres1\n,\n       \nt1\n.\nAlbumId\n AS \nres2\n,\n       \nt1\n.\nTitle\n AS \nres3\n,\n       \nt1\n.\nArtistId\n AS \nres4\n\nFROM \nArtist\n AS \nt0\n\nLEFT JOIN \nAlbum\n AS \nt1\n ON (\nt1\n.\nArtistId\n) = (\nt0\n.\nArtistId\n)\n\n\n        \n\n    \n        \n\n    \n                \n\n                    \n\n\nRight joins are not yet supported. They can always be rewritten as left joins.\nIf you have a compelling use case, please file an issue!\n\n\nFull Outer joins\n\n\n\n\nTODO\n\n\nouterJoin_\n not yet supported\n\n\n\n\nFull outer joins are supported via the \nouterJoin_\n construct.\n\n\nSubqueries\n\n\nSometimes you want to join against a \nsubquery\n rather than a table. For the\nmost part, beam will automatically figure out when certain queries need to be\nwritten using subqueries. For example, to join two result sets cointaining a SQL\nLIMIT, you would normally have to write both queries as subqueries. In beam, you\ncan write such queries as you'd expect. The library takes care of creating\nsubqueries as expected.\n\n\nFor example, the following query generates the code you'd expect.\n\n\n\n                \n\n                    \n        \n\n            \n        \n\n            \nHaskell\n\n        \n\n    \n        \n\n            \nSqlite3\n\n        \n\n    \n        \n\n            \nPostgres\n\n        \n\n    \n        \n\n    \n        \n\n            \n        \n\n            \ndo i \n- limit_ 10 $ all_ (invoice chinookDb)\n   line \n- invoiceLines i\n   pure (i, line)\n\n\n        \n\n    \n        \n\n            \nSELECT \nt0\n.\nres0\n AS \nres0\n,\n       \nt0\n.\nres1\n AS \nres1\n,\n       \nt0\n.\nres2\n AS \nres2\n,\n       \nt0\n.\nres3\n AS \nres3\n,\n       \nt0\n.\nres4\n AS \nres4\n,\n       \nt0\n.\nres5\n AS \nres5\n,\n       \nt0\n.\nres6\n AS \nres6\n,\n       \nt0\n.\nres7\n AS \nres7\n,\n       \nt0\n.\nres8\n AS \nres8\n,\n       \nt1\n.\nInvoiceLineId\n AS \nres9\n,\n       \nt1\n.\nInvoiceId\n AS \nres10\n,\n       \nt1\n.\nTrackId\n AS \nres11\n,\n       \nt1\n.\nUnitPrice\n AS \nres12\n,\n       \nt1\n.\nQuantity\n AS \nres13\n\nFROM\n  (SELECT \nt0\n.\nInvoiceId\n AS \nres0\n,\n          \nt0\n.\nCustomerId\n AS \nres1\n,\n          \nt0\n.\nInvoiceDate\n AS \nres2\n,\n          \nt0\n.\nBillingAddress\n AS \nres3\n,\n          \nt0\n.\nBillingCity\n AS \nres4\n,\n          \nt0\n.\nBillingState\n AS \nres5\n,\n          \nt0\n.\nBillingCountry\n AS \nres6\n,\n          \nt0\n.\nBillingPostalCode\n AS \nres7\n,\n          \nt0\n.\nTotal\n AS \nres8\n\n   FROM \nInvoice\n AS \nt0\n\n   LIMIT 10) AS \nt0\n\nINNER JOIN \nInvoiceLine\n AS \nt1\n ON (\nt1\n.\nInvoiceId\n)=(\nt0\n.\nres0\n)\n\n\n        \n\n    \n        \n\n            \nSELECT \nt0\n.\nres0\n AS \nres0\n,\n       \nt0\n.\nres1\n AS \nres1\n,\n       \nt0\n.\nres2\n AS \nres2\n,\n       \nt0\n.\nres3\n AS \nres3\n,\n       \nt0\n.\nres4\n AS \nres4\n,\n       \nt0\n.\nres5\n AS \nres5\n,\n       \nt0\n.\nres6\n AS \nres6\n,\n       \nt0\n.\nres7\n AS \nres7\n,\n       \nt0\n.\nres8\n AS \nres8\n,\n       \nt1\n.\nInvoiceLineId\n AS \nres9\n,\n       \nt1\n.\nInvoiceId\n AS \nres10\n,\n       \nt1\n.\nTrackId\n AS \nres11\n,\n       \nt1\n.\nUnitPrice\n AS \nres12\n,\n       \nt1\n.\nQuantity\n AS \nres13\n\nFROM\n  (SELECT \nt0\n.\nInvoiceId\n AS \nres0\n,\n          \nt0\n.\nCustomerId\n AS \nres1\n,\n          \nt0\n.\nInvoiceDate\n AS \nres2\n,\n          \nt0\n.\nBillingAddress\n AS \nres3\n,\n          \nt0\n.\nBillingCity\n AS \nres4\n,\n          \nt0\n.\nBillingState\n AS \nres5\n,\n          \nt0\n.\nBillingCountry\n AS \nres6\n,\n          \nt0\n.\nBillingPostalCode\n AS \nres7\n,\n          \nt0\n.\nTotal\n AS \nres8\n\n   FROM \nInvoice\n AS \nt0\n\n   LIMIT 10) AS \nt0\n\nINNER JOIN \nInvoiceLine\n AS \nt1\n ON (\nt1\n.\nInvoiceId\n) = (\nt0\n.\nres0\n)\n\n\n        \n\n    \n        \n\n    \n                \n\n                    \n\n\nIf you need to (for efficiency for example), you can also generate subqueries\nexplicitly, using \nsubselect_\n. The \nsubselect_\n will force a new query to be\noutput in most cases. For simple queries, such as \nall_\n, \nsubselect_\n will have\nno effect.\n\n\n\n                \n\n                    \n        \n\n            \n        \n\n            \nHaskell\n\n        \n\n    \n        \n\n            \nSqlite3\n\n        \n\n    \n        \n\n            \nPostgres\n\n        \n\n    \n        \n\n    \n        \n\n            \n        \n\n            \n-- Same as above, but with explicit sub select\ndo i \n- subselect_ $ limit_ 10 $ all_ (invoice chinookDb)\n   line \n- invoiceLines i\n   pure (i, line)\n\n\n        \n\n    \n        \n\n            \nSELECT \nt0\n.\nres0\n AS \nres0\n,\n       \nt0\n.\nres1\n AS \nres1\n,\n       \nt0\n.\nres2\n AS \nres2\n,\n       \nt0\n.\nres3\n AS \nres3\n,\n       \nt0\n.\nres4\n AS \nres4\n,\n       \nt0\n.\nres5\n AS \nres5\n,\n       \nt0\n.\nres6\n AS \nres6\n,\n       \nt0\n.\nres7\n AS \nres7\n,\n       \nt0\n.\nres8\n AS \nres8\n,\n       \nt1\n.\nInvoiceLineId\n AS \nres9\n,\n       \nt1\n.\nInvoiceId\n AS \nres10\n,\n       \nt1\n.\nTrackId\n AS \nres11\n,\n       \nt1\n.\nUnitPrice\n AS \nres12\n,\n       \nt1\n.\nQuantity\n AS \nres13\n\nFROM\n  (SELECT \nt0\n.\nInvoiceId\n AS \nres0\n,\n          \nt0\n.\nCustomerId\n AS \nres1\n,\n          \nt0\n.\nInvoiceDate\n AS \nres2\n,\n          \nt0\n.\nBillingAddress\n AS \nres3\n,\n          \nt0\n.\nBillingCity\n AS \nres4\n,\n          \nt0\n.\nBillingState\n AS \nres5\n,\n          \nt0\n.\nBillingCountry\n AS \nres6\n,\n          \nt0\n.\nBillingPostalCode\n AS \nres7\n,\n          \nt0\n.\nTotal\n AS \nres8\n\n   FROM \nInvoice\n AS \nt0\n\n   LIMIT 10) AS \nt0\n\nINNER JOIN \nInvoiceLine\n AS \nt1\n ON (\nt1\n.\nInvoiceId\n)=(\nt0\n.\nres0\n)\n\n\n        \n\n    \n        \n\n            \nSELECT \nt0\n.\nres0\n AS \nres0\n,\n       \nt0\n.\nres1\n AS \nres1\n,\n       \nt0\n.\nres2\n AS \nres2\n,\n       \nt0\n.\nres3\n AS \nres3\n,\n       \nt0\n.\nres4\n AS \nres4\n,\n       \nt0\n.\nres5\n AS \nres5\n,\n       \nt0\n.\nres6\n AS \nres6\n,\n       \nt0\n.\nres7\n AS \nres7\n,\n       \nt0\n.\nres8\n AS \nres8\n,\n       \nt1\n.\nInvoiceLineId\n AS \nres9\n,\n       \nt1\n.\nInvoiceId\n AS \nres10\n,\n       \nt1\n.\nTrackId\n AS \nres11\n,\n       \nt1\n.\nUnitPrice\n AS \nres12\n,\n       \nt1\n.\nQuantity\n AS \nres13\n\nFROM\n  (SELECT \nt0\n.\nInvoiceId\n AS \nres0\n,\n          \nt0\n.\nCustomerId\n AS \nres1\n,\n          \nt0\n.\nInvoiceDate\n AS \nres2\n,\n          \nt0\n.\nBillingAddress\n AS \nres3\n,\n          \nt0\n.\nBillingCity\n AS \nres4\n,\n          \nt0\n.\nBillingState\n AS \nres5\n,\n          \nt0\n.\nBillingCountry\n AS \nres6\n,\n          \nt0\n.\nBillingPostalCode\n AS \nres7\n,\n          \nt0\n.\nTotal\n AS \nres8\n\n   FROM \nInvoice\n AS \nt0\n\n   LIMIT 10) AS \nt0\n\nINNER JOIN \nInvoiceLine\n AS \nt1\n ON (\nt1\n.\nInvoiceId\n) = (\nt0\n.\nres0\n)", 
            "title": "Relationships"
        }, 
        {
            "location": "/user-guide/queries/relationships/#one-to-many", 
            "text": "Beam supports querying for one-to-many joins. For example, to get every InvoiceLine  for each  Invoice , use the  oneToMany_  combinator.  \n                 \n                    \n         \n            \n         \n             Haskell \n         \n    \n         \n             Sqlite3 \n         \n    \n         \n             Postgres \n         \n    \n         \n    \n         \n            \n         \n             do i  - all_ (invoice chinookDb)\n   ln  - oneToMany_ (invoiceLine chinookDb) invoiceLineInvoice i\n   pure (i, ln) \n         \n    \n         \n             SELECT  t0 . InvoiceId  AS  res0 ,\n        t0 . CustomerId  AS  res1 ,\n        t0 . InvoiceDate  AS  res2 ,\n        t0 . BillingAddress  AS  res3 ,\n        t0 . BillingCity  AS  res4 ,\n        t0 . BillingState  AS  res5 ,\n        t0 . BillingCountry  AS  res6 ,\n        t0 . BillingPostalCode  AS  res7 ,\n        t0 . Total  AS  res8 ,\n        t1 . InvoiceLineId  AS  res9 ,\n        t1 . InvoiceId  AS  res10 ,\n        t1 . TrackId  AS  res11 ,\n        t1 . UnitPrice  AS  res12 ,\n        t1 . Quantity  AS  res13 \nFROM  Invoice  AS  t0 \nINNER JOIN  InvoiceLine  AS  t1  ON ( t1 . InvoiceId )=( t0 . InvoiceId ) \n         \n    \n         \n             SELECT  t0 . InvoiceId  AS  res0 ,\n        t0 . CustomerId  AS  res1 ,\n        t0 . InvoiceDate  AS  res2 ,\n        t0 . BillingAddress  AS  res3 ,\n        t0 . BillingCity  AS  res4 ,\n        t0 . BillingState  AS  res5 ,\n        t0 . BillingCountry  AS  res6 ,\n        t0 . BillingPostalCode  AS  res7 ,\n        t0 . Total  AS  res8 ,\n        t1 . InvoiceLineId  AS  res9 ,\n        t1 . InvoiceId  AS  res10 ,\n        t1 . TrackId  AS  res11 ,\n        t1 . UnitPrice  AS  res12 ,\n        t1 . Quantity  AS  res13 \nFROM  Invoice  AS  t0 \nINNER JOIN  InvoiceLine  AS  t1  ON ( t1 . InvoiceId ) = ( t0 . InvoiceId ) \n         \n    \n         \n    \n                 \n                      Or, if you have an actual  Invoice  (called  oneInvoice ) and you want all the\nassociated  InvoiceLine s, you can use  val_  to convert  oneInvoice  to the SQL\nexpression level.  oneToMany_ (invoiceLine chinookDb) invoiceLineInvoice (val_ i)  If you find yourself repeating yourself constantly, you can define a helper.  invoiceLines_ :: OneToMany InvoiceT InvoiceLineT\ninvoiceLines_ = oneToMany_ (invoiceLine chinookDb) invoiceLineInvoice  Then the above queries become  do i  - all_ (invoice chinookDb)\n   ln  - invoiceLines_ i  and  invoiceLines (val_ i)", 
            "title": "One-to-many"
        }, 
        {
            "location": "/user-guide/queries/relationships/#nullable-columns", 
            "text": "If you have a nullable foreign key in your many table, you can use oneToManyOptional_  and  OneToManyOptional , respectively. For example,", 
            "title": "Nullable columns"
        }, 
        {
            "location": "/user-guide/queries/relationships/#one-to-one", 
            "text": "One to one relationships are a special case of one to many relationships, save\nfor a unique constraint on one column. Thus, there are no special constructs for\none-to-one relationships.  For convenience,  oneToOne_  and  OneToOne  are equivalent to  oneToMany_  and OneToMany . Additionally,  oneToMaybe_  and  OneToMaybe  correspond to oneToManyOptional_  and  OneToManyOptional .", 
            "title": "One-to-one"
        }, 
        {
            "location": "/user-guide/queries/relationships/#many-to-many", 
            "text": "Many to many relationships require a linking table, with foreign keys to each\ntable part of the relationship.  The  manyToMany_  construct can be used to fetch both, one, or no sides of a\nmany-to-many relationship.  manyToMany_ :: ( Database db, Table joinThrough\n               , Table left, Table right\n               , Sql92SelectSanityCheck syntax\n               , IsSql92SelectSyntax syntax\n\n               , SqlOrd (QExpr (Sql92SelectExpressionSyntax syntax) s) (PrimaryKey left g)\n               , SqlOrd (QExpr (Sql92SelectExpressionSyntax syntax) s) (PrimaryKey right h) )\n            =  DatabaseEntity be db (TableEntity joinThrough)\n            -  (joinThrough (QExpr (Sql92SelectExpressionSyntax syntax) s) -  PrimaryKey left g)\n            -  (joinThrough (QExpr (Sql92SelectExpressionSyntax syntax) s) -  PrimaryKey right h)\n            -  Q syntax db s (left g) -  Q syntax db s (right h)\n            -  Q syntax db s (left g, right h)  This reads: for any database  db ; tables  joinThrough ,  left , and  right ;\nand sane select syntax  syntax , where the primary keys of  left  and  right \nare comparable as value expressions and we have some way of extracting a primary\nkey of  left  and  right  from  joinThrough , associate all entries of  left \nwith those of  right  through  joinThrough  and return the results of  left  and right .  For example,", 
            "title": "Many-to-many"
        }, 
        {
            "location": "/user-guide/queries/relationships/#many-to-many-with-arbitrary-data", 
            "text": "Sometimes you want to have additional data for each relationship. For this, use manyToManyPassthrough_ .  manyToManyPassthrough_ \n    :: ( Database db, Table joinThrough\n       , Table left, Table right\n       , Sql92SelectSanityCheck syntax\n       , IsSql92SelectSyntax syntax\n\n       , SqlOrd (QExpr (Sql92SelectExpressionSyntax syntax) s) (PrimaryKey left g)\n       , SqlOrd (QExpr (Sql92SelectExpressionSyntax syntax) s) (PrimaryKey right h) )\n    =  DatabaseEntity be db (TableEntity joinThrough)\n    -  (joinThrough (QExpr (Sql92SelectExpressionSyntax syntax) s) -  PrimaryKey left g)\n    -  (joinThrough (QExpr (Sql92SelectExpressionSyntax syntax) s) -  PrimaryKey right h)\n    -  Q syntax db s (left g) -  Q syntax db s (right h)\n    -  Q syntax db s (joinThrough (QExpr (Sql92SelectExpressionSyntax syntax) s), left g, right h)  Under the hood  manyToMany_  is defined simply as   manyToMany_ = fmap (\\(_, left, right) -  (left, right)) manyToManyPassthrough_   TODO  It would be nice to have a  ManyToMany  type or some equivalent.", 
            "title": "Many-to-many with arbitrary data"
        }, 
        {
            "location": "/user-guide/queries/relationships/#arbitrary-joins", 
            "text": "Joins with arbitrary conditions can be specified using the  join_  construct.", 
            "title": "Arbitrary Joins"
        }, 
        {
            "location": "/user-guide/queries/relationships/#outer-joins", 
            "text": "", 
            "title": "Outer joins"
        }, 
        {
            "location": "/user-guide/queries/relationships/#left-and-right-joins", 
            "text": "Left joins with arbitrary conditions can be specified with the  leftJoin_ \nconstruct.  leftJoin_  takes a table and a join condition. It associates each\nresult record with a record of the table given or an fully NULL row of that\ntable in case no row matches. For this reason, the result of  leftJoin_  has an\nextra  Nullable  column tag, which converts each field into the corresponding Maybe  type.   Note  The table parameter passed in as the join condition does not have a  Nullable  column tag. The join condition should be written as if a \nconcrete row from that table exists.   For example, to get every artist along with their albums, but always including\nevery artist, use  leftJoin_  as follows.  \n                 \n                    \n         \n            \n         \n             Haskell \n         \n    \n         \n             Sqlite3 \n         \n    \n         \n             Postgres \n         \n    \n         \n    \n         \n            \n         \n             do artist  - all_ (artist chinookDb)\n   album   - leftJoin_ (album chinookDb) (\\album -  albumArtist album ==. primaryKey artist)\n   pure (artist, album) \n         \n    \n         \n             SELECT  t0 . ArtistId  AS  res0 ,\n        t0 . Name  AS  res1 ,\n        t1 . AlbumId  AS  res2 ,\n        t1 . Title  AS  res3 ,\n        t1 . ArtistId  AS  res4 \nFROM  Artist  AS  t0 \nLEFT JOIN  Album  AS  t1  ON ( t1 . ArtistId )=( t0 . ArtistId ) \n         \n    \n         \n             SELECT  t0 . ArtistId  AS  res0 ,\n        t0 . Name  AS  res1 ,\n        t1 . AlbumId  AS  res2 ,\n        t1 . Title  AS  res3 ,\n        t1 . ArtistId  AS  res4 \nFROM  Artist  AS  t0 \nLEFT JOIN  Album  AS  t1  ON ( t1 . ArtistId ) = ( t0 . ArtistId ) \n         \n    \n         \n    \n                 \n                      Right joins are not yet supported. They can always be rewritten as left joins.\nIf you have a compelling use case, please file an issue!", 
            "title": "Left and right joins"
        }, 
        {
            "location": "/user-guide/queries/relationships/#full-outer-joins", 
            "text": "TODO  outerJoin_  not yet supported   Full outer joins are supported via the  outerJoin_  construct.", 
            "title": "Full Outer joins"
        }, 
        {
            "location": "/user-guide/queries/relationships/#subqueries", 
            "text": "Sometimes you want to join against a  subquery  rather than a table. For the\nmost part, beam will automatically figure out when certain queries need to be\nwritten using subqueries. For example, to join two result sets cointaining a SQL\nLIMIT, you would normally have to write both queries as subqueries. In beam, you\ncan write such queries as you'd expect. The library takes care of creating\nsubqueries as expected.  For example, the following query generates the code you'd expect.  \n                 \n                    \n         \n            \n         \n             Haskell \n         \n    \n         \n             Sqlite3 \n         \n    \n         \n             Postgres \n         \n    \n         \n    \n         \n            \n         \n             do i  - limit_ 10 $ all_ (invoice chinookDb)\n   line  - invoiceLines i\n   pure (i, line) \n         \n    \n         \n             SELECT  t0 . res0  AS  res0 ,\n        t0 . res1  AS  res1 ,\n        t0 . res2  AS  res2 ,\n        t0 . res3  AS  res3 ,\n        t0 . res4  AS  res4 ,\n        t0 . res5  AS  res5 ,\n        t0 . res6  AS  res6 ,\n        t0 . res7  AS  res7 ,\n        t0 . res8  AS  res8 ,\n        t1 . InvoiceLineId  AS  res9 ,\n        t1 . InvoiceId  AS  res10 ,\n        t1 . TrackId  AS  res11 ,\n        t1 . UnitPrice  AS  res12 ,\n        t1 . Quantity  AS  res13 \nFROM\n  (SELECT  t0 . InvoiceId  AS  res0 ,\n           t0 . CustomerId  AS  res1 ,\n           t0 . InvoiceDate  AS  res2 ,\n           t0 . BillingAddress  AS  res3 ,\n           t0 . BillingCity  AS  res4 ,\n           t0 . BillingState  AS  res5 ,\n           t0 . BillingCountry  AS  res6 ,\n           t0 . BillingPostalCode  AS  res7 ,\n           t0 . Total  AS  res8 \n   FROM  Invoice  AS  t0 \n   LIMIT 10) AS  t0 \nINNER JOIN  InvoiceLine  AS  t1  ON ( t1 . InvoiceId )=( t0 . res0 ) \n         \n    \n         \n             SELECT  t0 . res0  AS  res0 ,\n        t0 . res1  AS  res1 ,\n        t0 . res2  AS  res2 ,\n        t0 . res3  AS  res3 ,\n        t0 . res4  AS  res4 ,\n        t0 . res5  AS  res5 ,\n        t0 . res6  AS  res6 ,\n        t0 . res7  AS  res7 ,\n        t0 . res8  AS  res8 ,\n        t1 . InvoiceLineId  AS  res9 ,\n        t1 . InvoiceId  AS  res10 ,\n        t1 . TrackId  AS  res11 ,\n        t1 . UnitPrice  AS  res12 ,\n        t1 . Quantity  AS  res13 \nFROM\n  (SELECT  t0 . InvoiceId  AS  res0 ,\n           t0 . CustomerId  AS  res1 ,\n           t0 . InvoiceDate  AS  res2 ,\n           t0 . BillingAddress  AS  res3 ,\n           t0 . BillingCity  AS  res4 ,\n           t0 . BillingState  AS  res5 ,\n           t0 . BillingCountry  AS  res6 ,\n           t0 . BillingPostalCode  AS  res7 ,\n           t0 . Total  AS  res8 \n   FROM  Invoice  AS  t0 \n   LIMIT 10) AS  t0 \nINNER JOIN  InvoiceLine  AS  t1  ON ( t1 . InvoiceId ) = ( t0 . res0 ) \n         \n    \n         \n    \n                 \n                      If you need to (for efficiency for example), you can also generate subqueries\nexplicitly, using  subselect_ . The  subselect_  will force a new query to be\noutput in most cases. For simple queries, such as  all_ ,  subselect_  will have\nno effect.  \n                 \n                    \n         \n            \n         \n             Haskell \n         \n    \n         \n             Sqlite3 \n         \n    \n         \n             Postgres \n         \n    \n         \n    \n         \n            \n         \n             -- Same as above, but with explicit sub select\ndo i  - subselect_ $ limit_ 10 $ all_ (invoice chinookDb)\n   line  - invoiceLines i\n   pure (i, line) \n         \n    \n         \n             SELECT  t0 . res0  AS  res0 ,\n        t0 . res1  AS  res1 ,\n        t0 . res2  AS  res2 ,\n        t0 . res3  AS  res3 ,\n        t0 . res4  AS  res4 ,\n        t0 . res5  AS  res5 ,\n        t0 . res6  AS  res6 ,\n        t0 . res7  AS  res7 ,\n        t0 . res8  AS  res8 ,\n        t1 . InvoiceLineId  AS  res9 ,\n        t1 . InvoiceId  AS  res10 ,\n        t1 . TrackId  AS  res11 ,\n        t1 . UnitPrice  AS  res12 ,\n        t1 . Quantity  AS  res13 \nFROM\n  (SELECT  t0 . InvoiceId  AS  res0 ,\n           t0 . CustomerId  AS  res1 ,\n           t0 . InvoiceDate  AS  res2 ,\n           t0 . BillingAddress  AS  res3 ,\n           t0 . BillingCity  AS  res4 ,\n           t0 . BillingState  AS  res5 ,\n           t0 . BillingCountry  AS  res6 ,\n           t0 . BillingPostalCode  AS  res7 ,\n           t0 . Total  AS  res8 \n   FROM  Invoice  AS  t0 \n   LIMIT 10) AS  t0 \nINNER JOIN  InvoiceLine  AS  t1  ON ( t1 . InvoiceId )=( t0 . res0 ) \n         \n    \n         \n             SELECT  t0 . res0  AS  res0 ,\n        t0 . res1  AS  res1 ,\n        t0 . res2  AS  res2 ,\n        t0 . res3  AS  res3 ,\n        t0 . res4  AS  res4 ,\n        t0 . res5  AS  res5 ,\n        t0 . res6  AS  res6 ,\n        t0 . res7  AS  res7 ,\n        t0 . res8  AS  res8 ,\n        t1 . InvoiceLineId  AS  res9 ,\n        t1 . InvoiceId  AS  res10 ,\n        t1 . TrackId  AS  res11 ,\n        t1 . UnitPrice  AS  res12 ,\n        t1 . Quantity  AS  res13 \nFROM\n  (SELECT  t0 . InvoiceId  AS  res0 ,\n           t0 . CustomerId  AS  res1 ,\n           t0 . InvoiceDate  AS  res2 ,\n           t0 . BillingAddress  AS  res3 ,\n           t0 . BillingCity  AS  res4 ,\n           t0 . BillingState  AS  res5 ,\n           t0 . BillingCountry  AS  res6 ,\n           t0 . BillingPostalCode  AS  res7 ,\n           t0 . Total  AS  res8 \n   FROM  Invoice  AS  t0 \n   LIMIT 10) AS  t0 \nINNER JOIN  InvoiceLine  AS  t1  ON ( t1 . InvoiceId ) = ( t0 . res0 )", 
            "title": "Subqueries"
        }, 
        {
            "location": "/user-guide/queries/aggregates/", 
            "text": "You can use the \naggregate_\n function to group your result set and compute\naggregates within the group. You can think of \naggregate_\n as a souped up\nversion of Haskell's \ngroupBy\n.\n\n\nYou use \naggregate_\n by specifying an underlying query to run and a function\nthat produces an aggregation projection. An aggregation projection is either a\nvalue of type \nQAgg syntax s a\n, a value of type \nQGroupExpr syntax s a\n, or a\ntuple of such values. Any \nQGenExpr\n that uses an aggregate function is\nautomatically assigned the \nQAgg syntax s a\n type. Any \nQGenExpr\n that contains\nthe \ngroup_\n combinator is given the type \nQGroupExpr\n.\n\n\nDuring query generation, the expressions of type \nQGroupExpr\n are added to the\n\nGROUP BY\n clause, and expressions of type \nQAgg\n are treated as aggregation to\nbe computed.\n\n\nThe result of the \naggregate_\n lifts all the \nQAgg\ns and \nQGroupExpr\ns to\n'regular' value-level \nQExpr\ns, so the result of \naggregate_\n can be used in\nexpressions as usual.\n\n\nSimple aggregate usage\n\n\nSuppose we wanted to count the number of genres in our database.\n\n\n\n                \n\n                    \n        \n\n            \n        \n\n            \nHaskell\n\n        \n\n    \n        \n\n            \nSqlite3\n\n        \n\n    \n        \n\n            \nPostgres\n\n        \n\n    \n        \n\n    \n        \n\n            \n        \n\n            \naggregate_ (\\_ -\n countAll_) (all_ (genre chinookDb))\n\n\n        \n\n    \n        \n\n            \nSELECT COUNT(*) AS \nres0\n\nFROM \nGenre\n AS \nt0\n\n\n\n        \n\n    \n        \n\n            \nSELECT COUNT(*) AS \nres0\n\nFROM \nGenre\n AS \nt0\n\n\n\n        \n\n    \n        \n\n    \n                \n\n                    \n\n\nAdding a GROUP BY clause\n\n\nAbove, SQL used the default grouping, which puts all rows in one group. We can\nalso specify columns and expressions to group by. For example, if we wanted to\ncount the number of tracks for each genre, we can use the \ngroup_\n function to\ngroup by the genre.\n\n\n\n                \n\n                    \n        \n\n            \n        \n\n            \nHaskell\n\n        \n\n    \n        \n\n            \nSqlite3\n\n        \n\n    \n        \n\n            \nPostgres\n\n        \n\n    \n        \n\n    \n        \n\n            \n        \n\n            \naggregate_ (\\(genre, track) -\n (group_ genre, as_ @Int $ count_ (trackId track)))\n           ((,) \n$\n all_ (genre chinookDb) \n*\n all_ (track chinookDb))\n\n\n        \n\n    \n        \n\n            \nSELECT \nt0\n.\nGenreId\n AS \nres0\n,\n       \nt0\n.\nName\n AS \nres1\n,\n       COUNT(\nt1\n.\nTrackId\n) AS \nres2\n\nFROM \nGenre\n AS \nt0\n\nINNER JOIN \nTrack\n AS \nt1\n\nGROUP BY \nt0\n.\nGenreId\n,\n         \nt0\n.\nName\n\n\n\n        \n\n    \n        \n\n            \nSELECT \nt0\n.\nGenreId\n AS \nres0\n,\n       \nt0\n.\nName\n AS \nres1\n,\n       COUNT(\nt1\n.\nTrackId\n) AS \nres2\n\nFROM \nGenre\n AS \nt0\n\nINNER JOIN \nTrack\n AS \nt1\n\nGROUP BY \nt0\n.\nGenreId\n,\n         \nt0\n.\nName\n\n\n\n        \n\n    \n        \n\n    \n                \n\n                    \n\n\n\n\nTip\n\n\ncount_\n can return any \nIntegral\n type. Adding the explicit \nas_ @Int\n above \nprevents an ambiguous type error.\n\n\n\n\nSQL compatibility\n\n\nAbove, we demonstrated the use of \ncount_\n and \ncountAll_\n which map to the\nappropriate SQL aggregates. Beam supports all of the other standard SQL92\naggregates.\n\n\nIn general, SQL aggregates are named similarly in beam and SQL. As usual, the\naggregate function in beam is suffixed by an underscore. For example, \nsum_\n\ncorresponds to the SQL aggregate \nSUM\n.\n\n\nSQL also allows you to specify set quantifiers for each aggregate. Beam supports\nthese as well. By convention, versions of aggregates that take in an optional\nset quantifier are suffixed by \nOver\n. For example \nSUM(DISTINCT x)\n can be\nwritten \nsumOver_ distinctInGroup_ x\n. The universally quantified version of\neach aggregate is obtained by using the \nallInGroup_\n quantifier. Thus, \nsum_ ==\nsumOver_ allInGroup_\n. Because \nALL\n is the default set quantifier, beam does\nnot typically generate it in queries. If, for some reason, you would like beam\nto be explicit about it, you can use the \nallInGroupExplicitly_\n quantifier.\n\n\n\n                \n\n                    \n        \n\n            \n        \n\n            \nHaskell\n\n        \n\n    \n        \n\n            \nSqlite3\n\n        \n\n    \n        \n\n            \nPostgres\n\n        \n\n    \n        \n\n    \n        \n\n            \n        \n\n            \naggregate_ (\\(genre, track) -\n\n              ( group_ genre\n              , as_ @Int $ countOver_ distinctInGroup_ (trackUnitPrice track)\n              , sumOver_ allInGroupExplicitly_ (trackMilliseconds track) `div_` 1000 )) $\n           ((,) \n$\n all_ (genre chinookDb) \n*\n all_ (track chinookDb))\n\n\n        \n\n    \n        \n\n            \nSELECT \nt0\n.\nGenreId\n AS \nres0\n,\n       \nt0\n.\nName\n AS \nres1\n,\n       COUNT(DISTINCT \nt1\n.\nUnitPrice\n) AS \nres2\n,\n       (SUM(ALL \nt1\n.\nMilliseconds\n)) / (?) AS \nres3\n\nFROM \nGenre\n AS \nt0\n\nINNER JOIN \nTrack\n AS \nt1\n\nGROUP BY \nt0\n.\nGenreId\n,\n         \nt0\n.\nName\n\n\n\n        \n\n    \n        \n\n            \nSELECT \nt0\n.\nGenreId\n AS \nres0\n,\n       \nt0\n.\nName\n AS \nres1\n,\n       COUNT(DISTINCT \nt1\n.\nUnitPrice\n) AS \nres2\n,\n       (SUM(ALL \nt1\n.\nMilliseconds\n)) / (1000) AS \nres3\n\nFROM \nGenre\n AS \nt0\n\nINNER JOIN \nTrack\n AS \nt1\n\nGROUP BY \nt0\n.\nGenreId\n,\n         \nt0\n.\nName\n\n\n\n        \n\n    \n        \n\n    \n                \n\n                    \n\n\nThe \nbeam-core\n library supports the standard SQL aggregation functions.\nIndividual backends are likely to support the full range of aggregates available\non that backend (if not, please send a bug report).\n\n\n\n\n\n\n\n\nSQL Aggregate\n\n\nRelevant standard\n\n\nUnquantified beam function\n\n\nQuantified beam function\n\n\n\n\n\n\n\n\n\n\nSUM\n\n\nSQL92\n\n\nsum_\n\n\nsumOver_\n\n\n\n\n\n\nMIN\n\n\nSQL92\n\n\nmin_\n\n\nminOver_\n\n\n\n\n\n\nMAX\n\n\nSQL92\n\n\nmax_\n\n\nmaxOver_\n\n\n\n\n\n\nAVG\n\n\nSQL92\n\n\navg_\n\n\navgOver_\n\n\n\n\n\n\nCOUNT(x)\n\n\nSQL92\n\n\ncount_\n\n\ncountOver_\n\n\n\n\n\n\nCOUNT(*)\n\n\nSQL92\n\n\ncountAll_\n\n\nN/A\n\n\n\n\n\n\nEVERY(x)\n\n\nSQL99\n\n\nevery_\n\n\neveryOver_\n\n\n\n\n\n\nANY(x)/SOME(x)\n\n\nSQL99\n\n\nany_\n, \nsome_\n\n\nanyOver_\n, \nsomeOver_\n\n\n\n\n\n\n\n\nThe \nHAVING\n clause\n\n\nSQL allows users to specify a \nHAVING\n condition to filter results based on the\ncomputed result of an aggregate. Beam fully supports \nHAVIVG\n clauses, but does\nnot use any special syntax. Simply use \nfilter_\n or \nguard_\n as usual, and beam\nwill add a \nHAVING\n clause if it forms legal SQL. Otherwise, beam will create a\nsubselect and add a \nWHERE\n clause. Either way, this is transparent to the user.\n\n\n\n                \n\n                    \n        \n\n            \n        \n\n            \nHaskell\n\n        \n\n    \n        \n\n            \nSqlite3\n\n        \n\n    \n        \n\n            \nPostgres\n\n        \n\n    \n        \n\n    \n        \n\n            \n        \n\n            \n-- Only return results for genres whose total track length is over 5 minutes\nfilter_ (\\(genre, distinctPriceCount, totalTrackLength) -\n totalTrackLength \n=. 300000) $\naggregate_ (\\(genre, track) -\n\n              ( group_ genre\n              , as_ @Int $ countOver_ distinctInGroup_ (trackUnitPrice track)\n              , sumOver_ allInGroupExplicitly_ (trackMilliseconds track) `div_` 1000 )) $\n           ((,) \n$\n all_ (genre chinookDb) \n*\n all_ (track chinookDb))\n\n\n        \n\n    \n        \n\n            \nSELECT \nt0\n.\nGenreId\n AS \nres0\n,\n       \nt0\n.\nName\n AS \nres1\n,\n       COUNT(DISTINCT \nt1\n.\nUnitPrice\n) AS \nres2\n,\n       (SUM(ALL \nt1\n.\nMilliseconds\n)) / (?) AS \nres3\n\nFROM \nGenre\n AS \nt0\n\nINNER JOIN \nTrack\n AS \nt1\n\nGROUP BY \nt0\n.\nGenreId\n,\n         \nt0\n.\nName\n\nHAVING ((SUM(ALL \nt1\n.\nMilliseconds\n)) / (?))\n=(?)\n\n\n        \n\n    \n        \n\n            \nSELECT \nt0\n.\nGenreId\n AS \nres0\n,\n       \nt0\n.\nName\n AS \nres1\n,\n       COUNT(DISTINCT \nt1\n.\nUnitPrice\n) AS \nres2\n,\n       (SUM(ALL \nt1\n.\nMilliseconds\n)) / (1000) AS \nres3\n\nFROM \nGenre\n AS \nt0\n\nINNER JOIN \nTrack\n AS \nt1\n\nGROUP BY \nt0\n.\nGenreId\n,\n         \nt0\n.\nName\n\nHAVING ((SUM(ALL \nt1\n.\nMilliseconds\n)) / (1000)) \n= (300000)\n\n\n        \n\n    \n        \n\n    \n                \n\n                    \n\n\nBeam will also handle the \nfilter_\n correctly in the presence of more\ncomplicated queries. For example, we can now join our aggregate on genres back\nover tracks.\n\n\n\n                \n\n                    \n        \n\n            \n        \n\n            \nHaskell\n\n        \n\n    \n        \n\n            \nSqlite3\n\n        \n\n    \n        \n\n            \nPostgres\n\n        \n\n    \n        \n\n    \n        \n\n            \n        \n\n            \n-- Only return results for genres whose total track length is over 5 minutes\nfilter_ (\\(genre, track, distinctPriceCount, totalTrackLength) -\n totalTrackLength \n=. 300000) $\ndo (genre, priceCnt, trackLength) \n-\n            aggregate_ (\\(genre, track) -\n\n                          ( group_ genre\n                          , as_ @Int $ countOver_ distinctInGroup_ (trackUnitPrice track)\n                          , sumOver_ allInGroupExplicitly_ (trackMilliseconds track) `div_` 1000 )) $\n            ((,) \n$\n all_ (genre chinookDb) \n*\n all_ (track chinookDb))\n   track \n- join_ (track chinookDb) (\\track -\n trackGenreId track ==. just_ (pk genre))\n   pure (genre, track, priceCnt, trackLength)\n\n\n        \n\n    \n        \n\n            \nSELECT \nt0\n.\nres0\n AS \nres0\n,\n       \nt0\n.\nres1\n AS \nres1\n,\n       \nt1\n.\nTrackId\n AS \nres2\n,\n       \nt1\n.\nName\n AS \nres3\n,\n       \nt1\n.\nAlbumId\n AS \nres4\n,\n       \nt1\n.\nMediaTypeId\n AS \nres5\n,\n       \nt1\n.\nGenreId\n AS \nres6\n,\n       \nt1\n.\nComposer\n AS \nres7\n,\n       \nt1\n.\nMilliseconds\n AS \nres8\n,\n       \nt1\n.\nBytes\n AS \nres9\n,\n       \nt1\n.\nUnitPrice\n AS \nres10\n,\n       \nt0\n.\nres2\n AS \nres11\n,\n       \nt0\n.\nres3\n AS \nres12\n\nFROM\n  (SELECT \nt0\n.\nGenreId\n AS \nres0\n,\n          \nt0\n.\nName\n AS \nres1\n,\n          COUNT(DISTINCT \nt1\n.\nUnitPrice\n) AS \nres2\n,\n          (SUM(ALL \nt1\n.\nMilliseconds\n)) / (?) AS \nres3\n\n   FROM \nGenre\n AS \nt0\n\n   INNER JOIN \nTrack\n AS \nt1\n\n   GROUP BY \nt0\n.\nGenreId\n,\n            \nt0\n.\nName\n) AS \nt0\n\nINNER JOIN \nTrack\n AS \nt1\n ON (\nt1\n.\nGenreId\n)=(\nt0\n.\nres0\n)\nWHERE (\nt0\n.\nres3\n)\n=(?)\n\n\n        \n\n    \n        \n\n            \nSELECT \nt0\n.\nres0\n AS \nres0\n,\n       \nt0\n.\nres1\n AS \nres1\n,\n       \nt1\n.\nTrackId\n AS \nres2\n,\n       \nt1\n.\nName\n AS \nres3\n,\n       \nt1\n.\nAlbumId\n AS \nres4\n,\n       \nt1\n.\nMediaTypeId\n AS \nres5\n,\n       \nt1\n.\nGenreId\n AS \nres6\n,\n       \nt1\n.\nComposer\n AS \nres7\n,\n       \nt1\n.\nMilliseconds\n AS \nres8\n,\n       \nt1\n.\nBytes\n AS \nres9\n,\n       \nt1\n.\nUnitPrice\n AS \nres10\n,\n       \nt0\n.\nres2\n AS \nres11\n,\n       \nt0\n.\nres3\n AS \nres12\n\nFROM\n  (SELECT \nt0\n.\nGenreId\n AS \nres0\n,\n          \nt0\n.\nName\n AS \nres1\n,\n          COUNT(DISTINCT \nt1\n.\nUnitPrice\n) AS \nres2\n,\n          (SUM(ALL \nt1\n.\nMilliseconds\n)) / (1000) AS \nres3\n\n   FROM \nGenre\n AS \nt0\n\n   INNER JOIN \nTrack\n AS \nt1\n\n   GROUP BY \nt0\n.\nGenreId\n,\n            \nt0\n.\nName\n) AS \nt0\n\nINNER JOIN \nTrack\n AS \nt1\n ON (\nt1\n.\nGenreId\n) = (\nt0\n.\nres0\n)\nWHERE (\nt0\n.\nres3\n) \n= (300000)\n\n\n        \n\n    \n        \n\n    \n                \n\n                    \n\n\nThe position of \nfilter_\n changes the code generated. Above, the \nfilter_\n\nproduced a \nWHERE\n clause on the outermost \nSELECT\n. If instead, we put the\n\nfilter_\n clause right outside the \naggregate_\n, beam will produce a \nHAVING\n clause instead.\n\n\n\n                \n\n                    \n        \n\n            \n        \n\n            \nHaskell\n\n        \n\n    \n        \n\n            \nSqlite3\n\n        \n\n    \n        \n\n            \nPostgres\n\n        \n\n    \n        \n\n    \n        \n\n            \n        \n\n            \n-- Only return results for genres whose total track length is over 5 minutes\ndo (genre, priceCnt, trackLength) \n-\n            filter_ (\\(genre, distinctPriceCount, totalTrackLength) -\n totalTrackLength \n=. 300000) $\n            aggregate_ (\\(genre, track) -\n\n                          ( group_ genre\n                          , as_ @Int $ countOver_ distinctInGroup_ (trackUnitPrice track)\n                          , sumOver_ allInGroupExplicitly_ (trackMilliseconds track) `div_` 1000 )) $\n            ((,) \n$\n all_ (genre chinookDb) \n*\n all_ (track chinookDb))\n   track \n- join_ (track chinookDb) (\\track -\n trackGenreId track ==. just_ (pk genre))\n   pure (genre, track, priceCnt, trackLength)\n\n\n        \n\n    \n        \n\n            \nSELECT \nt0\n.\nres0\n AS \nres0\n,\n       \nt0\n.\nres1\n AS \nres1\n,\n       \nt1\n.\nTrackId\n AS \nres2\n,\n       \nt1\n.\nName\n AS \nres3\n,\n       \nt1\n.\nAlbumId\n AS \nres4\n,\n       \nt1\n.\nMediaTypeId\n AS \nres5\n,\n       \nt1\n.\nGenreId\n AS \nres6\n,\n       \nt1\n.\nComposer\n AS \nres7\n,\n       \nt1\n.\nMilliseconds\n AS \nres8\n,\n       \nt1\n.\nBytes\n AS \nres9\n,\n       \nt1\n.\nUnitPrice\n AS \nres10\n,\n       \nt0\n.\nres2\n AS \nres11\n,\n       \nt0\n.\nres3\n AS \nres12\n\nFROM\n  (SELECT \nt0\n.\nGenreId\n AS \nres0\n,\n          \nt0\n.\nName\n AS \nres1\n,\n          COUNT(DISTINCT \nt1\n.\nUnitPrice\n) AS \nres2\n,\n          (SUM(ALL \nt1\n.\nMilliseconds\n)) / (?) AS \nres3\n\n   FROM \nGenre\n AS \nt0\n\n   INNER JOIN \nTrack\n AS \nt1\n\n   GROUP BY \nt0\n.\nGenreId\n,\n            \nt0\n.\nName\n\n   HAVING ((SUM(ALL \nt1\n.\nMilliseconds\n)) / (?))\n=(?)) AS \nt0\n\nINNER JOIN \nTrack\n AS \nt1\n ON (\nt1\n.\nGenreId\n)=(\nt0\n.\nres0\n)\n\n\n        \n\n    \n        \n\n            \nSELECT \nt0\n.\nres0\n AS \nres0\n,\n       \nt0\n.\nres1\n AS \nres1\n,\n       \nt1\n.\nTrackId\n AS \nres2\n,\n       \nt1\n.\nName\n AS \nres3\n,\n       \nt1\n.\nAlbumId\n AS \nres4\n,\n       \nt1\n.\nMediaTypeId\n AS \nres5\n,\n       \nt1\n.\nGenreId\n AS \nres6\n,\n       \nt1\n.\nComposer\n AS \nres7\n,\n       \nt1\n.\nMilliseconds\n AS \nres8\n,\n       \nt1\n.\nBytes\n AS \nres9\n,\n       \nt1\n.\nUnitPrice\n AS \nres10\n,\n       \nt0\n.\nres2\n AS \nres11\n,\n       \nt0\n.\nres3\n AS \nres12\n\nFROM\n  (SELECT \nt0\n.\nGenreId\n AS \nres0\n,\n          \nt0\n.\nName\n AS \nres1\n,\n          COUNT(DISTINCT \nt1\n.\nUnitPrice\n) AS \nres2\n,\n          (SUM(ALL \nt1\n.\nMilliseconds\n)) / (1000) AS \nres3\n\n   FROM \nGenre\n AS \nt0\n\n   INNER JOIN \nTrack\n AS \nt1\n\n   GROUP BY \nt0\n.\nGenreId\n,\n            \nt0\n.\nName\n\n   HAVING ((SUM(ALL \nt1\n.\nMilliseconds\n)) / (1000)) \n= (300000)) AS \nt0\n\nINNER JOIN \nTrack\n AS \nt1\n ON (\nt1\n.\nGenreId\n) = (\nt0\n.\nres0\n)\n\n\n        \n\n    \n        \n\n    \n                \n\n                    \n\n\nDue to the monadic structure, putting the filtered aggregate as the second\nclause in the JOIN causes the HAVING to be floated out, because the compiler\ncan't prove that the conditional expression only depends on the results of the\naggregate.\n\n\n\n                \n\n                    \n        \n\n            \n        \n\n            \nHaskell\n\n        \n\n    \n        \n\n            \nSqlite3\n\n        \n\n    \n        \n\n            \nPostgres\n\n        \n\n    \n        \n\n    \n        \n\n            \n        \n\n            \n-- Only return results for genres whose total track length is over 5 minutes\ndo track_ \n- all_ (track chinookDb)\n   (genre, priceCnt, trackLength) \n-\n            filter_ (\\(genre, distinctPriceCount, totalTrackLength) -\n totalTrackLength \n=. 300000) $\n            aggregate_ (\\(genre, track) -\n\n                          ( group_ genre\n                          , as_ @Int $ countOver_ distinctInGroup_ (trackUnitPrice track)\n                          , sumOver_ allInGroupExplicitly_ (trackMilliseconds track) `div_` 1000 )) $\n            ((,) \n$\n all_ (genre chinookDb) \n*\n all_ (track chinookDb)) \n   guard_ (trackGenreId track_ ==. just_ (pk genre))\n   pure (genre, track_, priceCnt, trackLength)\n\n\n        \n\n    \n        \n\n            \nSELECT \nt1\n.\nres0\n AS \nres0\n,\n       \nt1\n.\nres1\n AS \nres1\n,\n       \nt0\n.\nTrackId\n AS \nres2\n,\n       \nt0\n.\nName\n AS \nres3\n,\n       \nt0\n.\nAlbumId\n AS \nres4\n,\n       \nt0\n.\nMediaTypeId\n AS \nres5\n,\n       \nt0\n.\nGenreId\n AS \nres6\n,\n       \nt0\n.\nComposer\n AS \nres7\n,\n       \nt0\n.\nMilliseconds\n AS \nres8\n,\n       \nt0\n.\nBytes\n AS \nres9\n,\n       \nt0\n.\nUnitPrice\n AS \nres10\n,\n       \nt1\n.\nres2\n AS \nres11\n,\n       \nt1\n.\nres3\n AS \nres12\n\nFROM \nTrack\n AS \nt0\n\nINNER JOIN\n  (SELECT \nt0\n.\nGenreId\n AS \nres0\n,\n          \nt0\n.\nName\n AS \nres1\n,\n          COUNT(DISTINCT \nt1\n.\nUnitPrice\n) AS \nres2\n,\n          (SUM(ALL \nt1\n.\nMilliseconds\n)) / (?) AS \nres3\n\n   FROM \nGenre\n AS \nt0\n\n   INNER JOIN \nTrack\n AS \nt1\n\n   GROUP BY \nt0\n.\nGenreId\n,\n            \nt0\n.\nName\n) AS \nt1\n\nWHERE ((\nt1\n.\nres3\n)\n=(?))\n  AND ((\nt0\n.\nGenreId\n)=(\nt1\n.\nres0\n))\n\n\n        \n\n    \n        \n\n            \nSELECT \nt1\n.\nres0\n AS \nres0\n,\n       \nt1\n.\nres1\n AS \nres1\n,\n       \nt0\n.\nTrackId\n AS \nres2\n,\n       \nt0\n.\nName\n AS \nres3\n,\n       \nt0\n.\nAlbumId\n AS \nres4\n,\n       \nt0\n.\nMediaTypeId\n AS \nres5\n,\n       \nt0\n.\nGenreId\n AS \nres6\n,\n       \nt0\n.\nComposer\n AS \nres7\n,\n       \nt0\n.\nMilliseconds\n AS \nres8\n,\n       \nt0\n.\nBytes\n AS \nres9\n,\n       \nt0\n.\nUnitPrice\n AS \nres10\n,\n       \nt1\n.\nres2\n AS \nres11\n,\n       \nt1\n.\nres3\n AS \nres12\n\nFROM \nTrack\n AS \nt0\n\nINNER JOIN\n  (SELECT \nt0\n.\nGenreId\n AS \nres0\n,\n          \nt0\n.\nName\n AS \nres1\n,\n          COUNT(DISTINCT \nt1\n.\nUnitPrice\n) AS \nres2\n,\n          (SUM(ALL \nt1\n.\nMilliseconds\n)) / (1000) AS \nres3\n\n   FROM \nGenre\n AS \nt0\n\n   INNER JOIN \nTrack\n AS \nt1\n\n   GROUP BY \nt0\n.\nGenreId\n,\n            \nt0\n.\nName\n) AS \nt1\n\nWHERE ((\nt1\n.\nres3\n) \n= (300000))\n  AND ((\nt0\n.\nGenreId\n) = (\nt1\n.\nres0\n))\n\n\n        \n\n    \n        \n\n    \n                \n\n                    \n\n\nYou can prove to the compiler that the \nfilter_\n should generate a having by\nusing the \nsubselect_\n combinator.\n\n\n\n                \n\n                    \n        \n\n            \n        \n\n            \nHaskell\n\n        \n\n    \n        \n\n            \nSqlite3\n\n        \n\n    \n        \n\n            \nPostgres\n\n        \n\n    \n        \n\n    \n        \n\n            \n        \n\n            \n-- Only return results for genres whose total track length is over 5 minutes\ndo track_ \n- all_ (track chinookDb)\n   (genre, priceCnt, trackLength) \n-\n            subselect_ $\n            filter_ (\\(genre, distinctPriceCount, totalTrackLength) -\n totalTrackLength \n=. 300000) $\n            aggregate_ (\\(genre, track) -\n\n                          ( group_ genre\n                          , as_ @Int $ countOver_ distinctInGroup_ (trackUnitPrice track)\n                          , sumOver_ allInGroupExplicitly_ (trackMilliseconds track) `div_` 1000 )) $\n            ((,) \n$\n all_ (genre chinookDb) \n*\n all_ (track chinookDb)) \n   guard_ (trackGenreId track_ ==. just_ (pk genre))\n   pure (genre, track_, priceCnt, trackLength)\n\n\n        \n\n    \n        \n\n            \nSELECT \nt1\n.\nres0\n AS \nres0\n,\n       \nt1\n.\nres1\n AS \nres1\n,\n       \nt0\n.\nTrackId\n AS \nres2\n,\n       \nt0\n.\nName\n AS \nres3\n,\n       \nt0\n.\nAlbumId\n AS \nres4\n,\n       \nt0\n.\nMediaTypeId\n AS \nres5\n,\n       \nt0\n.\nGenreId\n AS \nres6\n,\n       \nt0\n.\nComposer\n AS \nres7\n,\n       \nt0\n.\nMilliseconds\n AS \nres8\n,\n       \nt0\n.\nBytes\n AS \nres9\n,\n       \nt0\n.\nUnitPrice\n AS \nres10\n,\n       \nt1\n.\nres2\n AS \nres11\n,\n       \nt1\n.\nres3\n AS \nres12\n\nFROM \nTrack\n AS \nt0\n\nINNER JOIN\n  (SELECT \nt0\n.\nres0\n AS \nres0\n,\n          \nt0\n.\nres1\n AS \nres1\n,\n          \nt0\n.\nres2\n AS \nres2\n,\n          \nt0\n.\nres3\n AS \nres3\n\n   FROM\n     (SELECT \nt0\n.\nGenreId\n AS \nres0\n,\n             \nt0\n.\nName\n AS \nres1\n,\n             COUNT(DISTINCT \nt1\n.\nUnitPrice\n) AS \nres2\n,\n             (SUM(ALL \nt1\n.\nMilliseconds\n)) / (?) AS \nres3\n\n      FROM \nGenre\n AS \nt0\n\n      INNER JOIN \nTrack\n AS \nt1\n\n      GROUP BY \nt0\n.\nGenreId\n,\n               \nt0\n.\nName\n\n      HAVING ((SUM(ALL \nt1\n.\nMilliseconds\n)) / (?))\n=(?)) AS \nt0\n) AS \nt1\n\nWHERE (\nt0\n.\nGenreId\n)=(\nt1\n.\nres0\n)\n\n\n        \n\n    \n        \n\n            \nSELECT \nt1\n.\nres0\n AS \nres0\n,\n       \nt1\n.\nres1\n AS \nres1\n,\n       \nt0\n.\nTrackId\n AS \nres2\n,\n       \nt0\n.\nName\n AS \nres3\n,\n       \nt0\n.\nAlbumId\n AS \nres4\n,\n       \nt0\n.\nMediaTypeId\n AS \nres5\n,\n       \nt0\n.\nGenreId\n AS \nres6\n,\n       \nt0\n.\nComposer\n AS \nres7\n,\n       \nt0\n.\nMilliseconds\n AS \nres8\n,\n       \nt0\n.\nBytes\n AS \nres9\n,\n       \nt0\n.\nUnitPrice\n AS \nres10\n,\n       \nt1\n.\nres2\n AS \nres11\n,\n       \nt1\n.\nres3\n AS \nres12\n\nFROM \nTrack\n AS \nt0\n\nINNER JOIN\n  (SELECT \nt0\n.\nres0\n AS \nres0\n,\n          \nt0\n.\nres1\n AS \nres1\n,\n          \nt0\n.\nres2\n AS \nres2\n,\n          \nt0\n.\nres3\n AS \nres3\n\n   FROM\n     (SELECT \nt0\n.\nGenreId\n AS \nres0\n,\n             \nt0\n.\nName\n AS \nres1\n,\n             COUNT(DISTINCT \nt1\n.\nUnitPrice\n) AS \nres2\n,\n             (SUM(ALL \nt1\n.\nMilliseconds\n)) / (1000) AS \nres3\n\n      FROM \nGenre\n AS \nt0\n\n      INNER JOIN \nTrack\n AS \nt1\n\n      GROUP BY \nt0\n.\nGenreId\n,\n               \nt0\n.\nName\n\n      HAVING ((SUM(ALL \nt1\n.\nMilliseconds\n)) / (1000)) \n= (300000)) AS \nt0\n) AS \nt1\n\nWHERE (\nt0\n.\nGenreId\n) = (\nt1\n.\nres0\n)", 
            "title": "Aggregates"
        }, 
        {
            "location": "/user-guide/queries/aggregates/#simple-aggregate-usage", 
            "text": "Suppose we wanted to count the number of genres in our database.  \n                 \n                    \n         \n            \n         \n             Haskell \n         \n    \n         \n             Sqlite3 \n         \n    \n         \n             Postgres \n         \n    \n         \n    \n         \n            \n         \n             aggregate_ (\\_ -  countAll_) (all_ (genre chinookDb)) \n         \n    \n         \n             SELECT COUNT(*) AS  res0 \nFROM  Genre  AS  t0  \n         \n    \n         \n             SELECT COUNT(*) AS  res0 \nFROM  Genre  AS  t0", 
            "title": "Simple aggregate usage"
        }, 
        {
            "location": "/user-guide/queries/aggregates/#adding-a-group-by-clause", 
            "text": "Above, SQL used the default grouping, which puts all rows in one group. We can\nalso specify columns and expressions to group by. For example, if we wanted to\ncount the number of tracks for each genre, we can use the  group_  function to\ngroup by the genre.  \n                 \n                    \n         \n            \n         \n             Haskell \n         \n    \n         \n             Sqlite3 \n         \n    \n         \n             Postgres \n         \n    \n         \n    \n         \n            \n         \n             aggregate_ (\\(genre, track) -  (group_ genre, as_ @Int $ count_ (trackId track)))\n           ((,)  $  all_ (genre chinookDb)  *  all_ (track chinookDb)) \n         \n    \n         \n             SELECT  t0 . GenreId  AS  res0 ,\n        t0 . Name  AS  res1 ,\n       COUNT( t1 . TrackId ) AS  res2 \nFROM  Genre  AS  t0 \nINNER JOIN  Track  AS  t1 \nGROUP BY  t0 . GenreId ,\n          t0 . Name  \n         \n    \n         \n             SELECT  t0 . GenreId  AS  res0 ,\n        t0 . Name  AS  res1 ,\n       COUNT( t1 . TrackId ) AS  res2 \nFROM  Genre  AS  t0 \nINNER JOIN  Track  AS  t1 \nGROUP BY  t0 . GenreId ,\n          t0 . Name  \n         \n    \n         \n    \n                 \n                       Tip  count_  can return any  Integral  type. Adding the explicit  as_ @Int  above \nprevents an ambiguous type error.", 
            "title": "Adding a GROUP BY clause"
        }, 
        {
            "location": "/user-guide/queries/aggregates/#sql-compatibility", 
            "text": "Above, we demonstrated the use of  count_  and  countAll_  which map to the\nappropriate SQL aggregates. Beam supports all of the other standard SQL92\naggregates.  In general, SQL aggregates are named similarly in beam and SQL. As usual, the\naggregate function in beam is suffixed by an underscore. For example,  sum_ \ncorresponds to the SQL aggregate  SUM .  SQL also allows you to specify set quantifiers for each aggregate. Beam supports\nthese as well. By convention, versions of aggregates that take in an optional\nset quantifier are suffixed by  Over . For example  SUM(DISTINCT x)  can be\nwritten  sumOver_ distinctInGroup_ x . The universally quantified version of\neach aggregate is obtained by using the  allInGroup_  quantifier. Thus,  sum_ ==\nsumOver_ allInGroup_ . Because  ALL  is the default set quantifier, beam does\nnot typically generate it in queries. If, for some reason, you would like beam\nto be explicit about it, you can use the  allInGroupExplicitly_  quantifier.  \n                 \n                    \n         \n            \n         \n             Haskell \n         \n    \n         \n             Sqlite3 \n         \n    \n         \n             Postgres \n         \n    \n         \n    \n         \n            \n         \n             aggregate_ (\\(genre, track) - \n              ( group_ genre\n              , as_ @Int $ countOver_ distinctInGroup_ (trackUnitPrice track)\n              , sumOver_ allInGroupExplicitly_ (trackMilliseconds track) `div_` 1000 )) $\n           ((,)  $  all_ (genre chinookDb)  *  all_ (track chinookDb)) \n         \n    \n         \n             SELECT  t0 . GenreId  AS  res0 ,\n        t0 . Name  AS  res1 ,\n       COUNT(DISTINCT  t1 . UnitPrice ) AS  res2 ,\n       (SUM(ALL  t1 . Milliseconds )) / (?) AS  res3 \nFROM  Genre  AS  t0 \nINNER JOIN  Track  AS  t1 \nGROUP BY  t0 . GenreId ,\n          t0 . Name  \n         \n    \n         \n             SELECT  t0 . GenreId  AS  res0 ,\n        t0 . Name  AS  res1 ,\n       COUNT(DISTINCT  t1 . UnitPrice ) AS  res2 ,\n       (SUM(ALL  t1 . Milliseconds )) / (1000) AS  res3 \nFROM  Genre  AS  t0 \nINNER JOIN  Track  AS  t1 \nGROUP BY  t0 . GenreId ,\n          t0 . Name  \n         \n    \n         \n    \n                 \n                      The  beam-core  library supports the standard SQL aggregation functions.\nIndividual backends are likely to support the full range of aggregates available\non that backend (if not, please send a bug report).     SQL Aggregate  Relevant standard  Unquantified beam function  Quantified beam function      SUM  SQL92  sum_  sumOver_    MIN  SQL92  min_  minOver_    MAX  SQL92  max_  maxOver_    AVG  SQL92  avg_  avgOver_    COUNT(x)  SQL92  count_  countOver_    COUNT(*)  SQL92  countAll_  N/A    EVERY(x)  SQL99  every_  everyOver_    ANY(x)/SOME(x)  SQL99  any_ ,  some_  anyOver_ ,  someOver_", 
            "title": "SQL compatibility"
        }, 
        {
            "location": "/user-guide/queries/aggregates/#the-having-clause", 
            "text": "SQL allows users to specify a  HAVING  condition to filter results based on the\ncomputed result of an aggregate. Beam fully supports  HAVIVG  clauses, but does\nnot use any special syntax. Simply use  filter_  or  guard_  as usual, and beam\nwill add a  HAVING  clause if it forms legal SQL. Otherwise, beam will create a\nsubselect and add a  WHERE  clause. Either way, this is transparent to the user.  \n                 \n                    \n         \n            \n         \n             Haskell \n         \n    \n         \n             Sqlite3 \n         \n    \n         \n             Postgres \n         \n    \n         \n    \n         \n            \n         \n             -- Only return results for genres whose total track length is over 5 minutes\nfilter_ (\\(genre, distinctPriceCount, totalTrackLength) -  totalTrackLength  =. 300000) $\naggregate_ (\\(genre, track) - \n              ( group_ genre\n              , as_ @Int $ countOver_ distinctInGroup_ (trackUnitPrice track)\n              , sumOver_ allInGroupExplicitly_ (trackMilliseconds track) `div_` 1000 )) $\n           ((,)  $  all_ (genre chinookDb)  *  all_ (track chinookDb)) \n         \n    \n         \n             SELECT  t0 . GenreId  AS  res0 ,\n        t0 . Name  AS  res1 ,\n       COUNT(DISTINCT  t1 . UnitPrice ) AS  res2 ,\n       (SUM(ALL  t1 . Milliseconds )) / (?) AS  res3 \nFROM  Genre  AS  t0 \nINNER JOIN  Track  AS  t1 \nGROUP BY  t0 . GenreId ,\n          t0 . Name \nHAVING ((SUM(ALL  t1 . Milliseconds )) / (?)) =(?) \n         \n    \n         \n             SELECT  t0 . GenreId  AS  res0 ,\n        t0 . Name  AS  res1 ,\n       COUNT(DISTINCT  t1 . UnitPrice ) AS  res2 ,\n       (SUM(ALL  t1 . Milliseconds )) / (1000) AS  res3 \nFROM  Genre  AS  t0 \nINNER JOIN  Track  AS  t1 \nGROUP BY  t0 . GenreId ,\n          t0 . Name \nHAVING ((SUM(ALL  t1 . Milliseconds )) / (1000))  = (300000) \n         \n    \n         \n    \n                 \n                      Beam will also handle the  filter_  correctly in the presence of more\ncomplicated queries. For example, we can now join our aggregate on genres back\nover tracks.  \n                 \n                    \n         \n            \n         \n             Haskell \n         \n    \n         \n             Sqlite3 \n         \n    \n         \n             Postgres \n         \n    \n         \n    \n         \n            \n         \n             -- Only return results for genres whose total track length is over 5 minutes\nfilter_ (\\(genre, track, distinctPriceCount, totalTrackLength) -  totalTrackLength  =. 300000) $\ndo (genre, priceCnt, trackLength)  -\n            aggregate_ (\\(genre, track) - \n                          ( group_ genre\n                          , as_ @Int $ countOver_ distinctInGroup_ (trackUnitPrice track)\n                          , sumOver_ allInGroupExplicitly_ (trackMilliseconds track) `div_` 1000 )) $\n            ((,)  $  all_ (genre chinookDb)  *  all_ (track chinookDb))\n   track  - join_ (track chinookDb) (\\track -  trackGenreId track ==. just_ (pk genre))\n   pure (genre, track, priceCnt, trackLength) \n         \n    \n         \n             SELECT  t0 . res0  AS  res0 ,\n        t0 . res1  AS  res1 ,\n        t1 . TrackId  AS  res2 ,\n        t1 . Name  AS  res3 ,\n        t1 . AlbumId  AS  res4 ,\n        t1 . MediaTypeId  AS  res5 ,\n        t1 . GenreId  AS  res6 ,\n        t1 . Composer  AS  res7 ,\n        t1 . Milliseconds  AS  res8 ,\n        t1 . Bytes  AS  res9 ,\n        t1 . UnitPrice  AS  res10 ,\n        t0 . res2  AS  res11 ,\n        t0 . res3  AS  res12 \nFROM\n  (SELECT  t0 . GenreId  AS  res0 ,\n           t0 . Name  AS  res1 ,\n          COUNT(DISTINCT  t1 . UnitPrice ) AS  res2 ,\n          (SUM(ALL  t1 . Milliseconds )) / (?) AS  res3 \n   FROM  Genre  AS  t0 \n   INNER JOIN  Track  AS  t1 \n   GROUP BY  t0 . GenreId ,\n             t0 . Name ) AS  t0 \nINNER JOIN  Track  AS  t1  ON ( t1 . GenreId )=( t0 . res0 )\nWHERE ( t0 . res3 ) =(?) \n         \n    \n         \n             SELECT  t0 . res0  AS  res0 ,\n        t0 . res1  AS  res1 ,\n        t1 . TrackId  AS  res2 ,\n        t1 . Name  AS  res3 ,\n        t1 . AlbumId  AS  res4 ,\n        t1 . MediaTypeId  AS  res5 ,\n        t1 . GenreId  AS  res6 ,\n        t1 . Composer  AS  res7 ,\n        t1 . Milliseconds  AS  res8 ,\n        t1 . Bytes  AS  res9 ,\n        t1 . UnitPrice  AS  res10 ,\n        t0 . res2  AS  res11 ,\n        t0 . res3  AS  res12 \nFROM\n  (SELECT  t0 . GenreId  AS  res0 ,\n           t0 . Name  AS  res1 ,\n          COUNT(DISTINCT  t1 . UnitPrice ) AS  res2 ,\n          (SUM(ALL  t1 . Milliseconds )) / (1000) AS  res3 \n   FROM  Genre  AS  t0 \n   INNER JOIN  Track  AS  t1 \n   GROUP BY  t0 . GenreId ,\n             t0 . Name ) AS  t0 \nINNER JOIN  Track  AS  t1  ON ( t1 . GenreId ) = ( t0 . res0 )\nWHERE ( t0 . res3 )  = (300000) \n         \n    \n         \n    \n                 \n                      The position of  filter_  changes the code generated. Above, the  filter_ \nproduced a  WHERE  clause on the outermost  SELECT . If instead, we put the filter_  clause right outside the  aggregate_ , beam will produce a  HAVING  clause instead.  \n                 \n                    \n         \n            \n         \n             Haskell \n         \n    \n         \n             Sqlite3 \n         \n    \n         \n             Postgres \n         \n    \n         \n    \n         \n            \n         \n             -- Only return results for genres whose total track length is over 5 minutes\ndo (genre, priceCnt, trackLength)  -\n            filter_ (\\(genre, distinctPriceCount, totalTrackLength) -  totalTrackLength  =. 300000) $\n            aggregate_ (\\(genre, track) - \n                          ( group_ genre\n                          , as_ @Int $ countOver_ distinctInGroup_ (trackUnitPrice track)\n                          , sumOver_ allInGroupExplicitly_ (trackMilliseconds track) `div_` 1000 )) $\n            ((,)  $  all_ (genre chinookDb)  *  all_ (track chinookDb))\n   track  - join_ (track chinookDb) (\\track -  trackGenreId track ==. just_ (pk genre))\n   pure (genre, track, priceCnt, trackLength) \n         \n    \n         \n             SELECT  t0 . res0  AS  res0 ,\n        t0 . res1  AS  res1 ,\n        t1 . TrackId  AS  res2 ,\n        t1 . Name  AS  res3 ,\n        t1 . AlbumId  AS  res4 ,\n        t1 . MediaTypeId  AS  res5 ,\n        t1 . GenreId  AS  res6 ,\n        t1 . Composer  AS  res7 ,\n        t1 . Milliseconds  AS  res8 ,\n        t1 . Bytes  AS  res9 ,\n        t1 . UnitPrice  AS  res10 ,\n        t0 . res2  AS  res11 ,\n        t0 . res3  AS  res12 \nFROM\n  (SELECT  t0 . GenreId  AS  res0 ,\n           t0 . Name  AS  res1 ,\n          COUNT(DISTINCT  t1 . UnitPrice ) AS  res2 ,\n          (SUM(ALL  t1 . Milliseconds )) / (?) AS  res3 \n   FROM  Genre  AS  t0 \n   INNER JOIN  Track  AS  t1 \n   GROUP BY  t0 . GenreId ,\n             t0 . Name \n   HAVING ((SUM(ALL  t1 . Milliseconds )) / (?)) =(?)) AS  t0 \nINNER JOIN  Track  AS  t1  ON ( t1 . GenreId )=( t0 . res0 ) \n         \n    \n         \n             SELECT  t0 . res0  AS  res0 ,\n        t0 . res1  AS  res1 ,\n        t1 . TrackId  AS  res2 ,\n        t1 . Name  AS  res3 ,\n        t1 . AlbumId  AS  res4 ,\n        t1 . MediaTypeId  AS  res5 ,\n        t1 . GenreId  AS  res6 ,\n        t1 . Composer  AS  res7 ,\n        t1 . Milliseconds  AS  res8 ,\n        t1 . Bytes  AS  res9 ,\n        t1 . UnitPrice  AS  res10 ,\n        t0 . res2  AS  res11 ,\n        t0 . res3  AS  res12 \nFROM\n  (SELECT  t0 . GenreId  AS  res0 ,\n           t0 . Name  AS  res1 ,\n          COUNT(DISTINCT  t1 . UnitPrice ) AS  res2 ,\n          (SUM(ALL  t1 . Milliseconds )) / (1000) AS  res3 \n   FROM  Genre  AS  t0 \n   INNER JOIN  Track  AS  t1 \n   GROUP BY  t0 . GenreId ,\n             t0 . Name \n   HAVING ((SUM(ALL  t1 . Milliseconds )) / (1000))  = (300000)) AS  t0 \nINNER JOIN  Track  AS  t1  ON ( t1 . GenreId ) = ( t0 . res0 ) \n         \n    \n         \n    \n                 \n                      Due to the monadic structure, putting the filtered aggregate as the second\nclause in the JOIN causes the HAVING to be floated out, because the compiler\ncan't prove that the conditional expression only depends on the results of the\naggregate.  \n                 \n                    \n         \n            \n         \n             Haskell \n         \n    \n         \n             Sqlite3 \n         \n    \n         \n             Postgres \n         \n    \n         \n    \n         \n            \n         \n             -- Only return results for genres whose total track length is over 5 minutes\ndo track_  - all_ (track chinookDb)\n   (genre, priceCnt, trackLength)  -\n            filter_ (\\(genre, distinctPriceCount, totalTrackLength) -  totalTrackLength  =. 300000) $\n            aggregate_ (\\(genre, track) - \n                          ( group_ genre\n                          , as_ @Int $ countOver_ distinctInGroup_ (trackUnitPrice track)\n                          , sumOver_ allInGroupExplicitly_ (trackMilliseconds track) `div_` 1000 )) $\n            ((,)  $  all_ (genre chinookDb)  *  all_ (track chinookDb)) \n   guard_ (trackGenreId track_ ==. just_ (pk genre))\n   pure (genre, track_, priceCnt, trackLength) \n         \n    \n         \n             SELECT  t1 . res0  AS  res0 ,\n        t1 . res1  AS  res1 ,\n        t0 . TrackId  AS  res2 ,\n        t0 . Name  AS  res3 ,\n        t0 . AlbumId  AS  res4 ,\n        t0 . MediaTypeId  AS  res5 ,\n        t0 . GenreId  AS  res6 ,\n        t0 . Composer  AS  res7 ,\n        t0 . Milliseconds  AS  res8 ,\n        t0 . Bytes  AS  res9 ,\n        t0 . UnitPrice  AS  res10 ,\n        t1 . res2  AS  res11 ,\n        t1 . res3  AS  res12 \nFROM  Track  AS  t0 \nINNER JOIN\n  (SELECT  t0 . GenreId  AS  res0 ,\n           t0 . Name  AS  res1 ,\n          COUNT(DISTINCT  t1 . UnitPrice ) AS  res2 ,\n          (SUM(ALL  t1 . Milliseconds )) / (?) AS  res3 \n   FROM  Genre  AS  t0 \n   INNER JOIN  Track  AS  t1 \n   GROUP BY  t0 . GenreId ,\n             t0 . Name ) AS  t1 \nWHERE (( t1 . res3 ) =(?))\n  AND (( t0 . GenreId )=( t1 . res0 )) \n         \n    \n         \n             SELECT  t1 . res0  AS  res0 ,\n        t1 . res1  AS  res1 ,\n        t0 . TrackId  AS  res2 ,\n        t0 . Name  AS  res3 ,\n        t0 . AlbumId  AS  res4 ,\n        t0 . MediaTypeId  AS  res5 ,\n        t0 . GenreId  AS  res6 ,\n        t0 . Composer  AS  res7 ,\n        t0 . Milliseconds  AS  res8 ,\n        t0 . Bytes  AS  res9 ,\n        t0 . UnitPrice  AS  res10 ,\n        t1 . res2  AS  res11 ,\n        t1 . res3  AS  res12 \nFROM  Track  AS  t0 \nINNER JOIN\n  (SELECT  t0 . GenreId  AS  res0 ,\n           t0 . Name  AS  res1 ,\n          COUNT(DISTINCT  t1 . UnitPrice ) AS  res2 ,\n          (SUM(ALL  t1 . Milliseconds )) / (1000) AS  res3 \n   FROM  Genre  AS  t0 \n   INNER JOIN  Track  AS  t1 \n   GROUP BY  t0 . GenreId ,\n             t0 . Name ) AS  t1 \nWHERE (( t1 . res3 )  = (300000))\n  AND (( t0 . GenreId ) = ( t1 . res0 )) \n         \n    \n         \n    \n                 \n                      You can prove to the compiler that the  filter_  should generate a having by\nusing the  subselect_  combinator.  \n                 \n                    \n         \n            \n         \n             Haskell \n         \n    \n         \n             Sqlite3 \n         \n    \n         \n             Postgres \n         \n    \n         \n    \n         \n            \n         \n             -- Only return results for genres whose total track length is over 5 minutes\ndo track_  - all_ (track chinookDb)\n   (genre, priceCnt, trackLength)  -\n            subselect_ $\n            filter_ (\\(genre, distinctPriceCount, totalTrackLength) -  totalTrackLength  =. 300000) $\n            aggregate_ (\\(genre, track) - \n                          ( group_ genre\n                          , as_ @Int $ countOver_ distinctInGroup_ (trackUnitPrice track)\n                          , sumOver_ allInGroupExplicitly_ (trackMilliseconds track) `div_` 1000 )) $\n            ((,)  $  all_ (genre chinookDb)  *  all_ (track chinookDb)) \n   guard_ (trackGenreId track_ ==. just_ (pk genre))\n   pure (genre, track_, priceCnt, trackLength) \n         \n    \n         \n             SELECT  t1 . res0  AS  res0 ,\n        t1 . res1  AS  res1 ,\n        t0 . TrackId  AS  res2 ,\n        t0 . Name  AS  res3 ,\n        t0 . AlbumId  AS  res4 ,\n        t0 . MediaTypeId  AS  res5 ,\n        t0 . GenreId  AS  res6 ,\n        t0 . Composer  AS  res7 ,\n        t0 . Milliseconds  AS  res8 ,\n        t0 . Bytes  AS  res9 ,\n        t0 . UnitPrice  AS  res10 ,\n        t1 . res2  AS  res11 ,\n        t1 . res3  AS  res12 \nFROM  Track  AS  t0 \nINNER JOIN\n  (SELECT  t0 . res0  AS  res0 ,\n           t0 . res1  AS  res1 ,\n           t0 . res2  AS  res2 ,\n           t0 . res3  AS  res3 \n   FROM\n     (SELECT  t0 . GenreId  AS  res0 ,\n              t0 . Name  AS  res1 ,\n             COUNT(DISTINCT  t1 . UnitPrice ) AS  res2 ,\n             (SUM(ALL  t1 . Milliseconds )) / (?) AS  res3 \n      FROM  Genre  AS  t0 \n      INNER JOIN  Track  AS  t1 \n      GROUP BY  t0 . GenreId ,\n                t0 . Name \n      HAVING ((SUM(ALL  t1 . Milliseconds )) / (?)) =(?)) AS  t0 ) AS  t1 \nWHERE ( t0 . GenreId )=( t1 . res0 ) \n         \n    \n         \n             SELECT  t1 . res0  AS  res0 ,\n        t1 . res1  AS  res1 ,\n        t0 . TrackId  AS  res2 ,\n        t0 . Name  AS  res3 ,\n        t0 . AlbumId  AS  res4 ,\n        t0 . MediaTypeId  AS  res5 ,\n        t0 . GenreId  AS  res6 ,\n        t0 . Composer  AS  res7 ,\n        t0 . Milliseconds  AS  res8 ,\n        t0 . Bytes  AS  res9 ,\n        t0 . UnitPrice  AS  res10 ,\n        t1 . res2  AS  res11 ,\n        t1 . res3  AS  res12 \nFROM  Track  AS  t0 \nINNER JOIN\n  (SELECT  t0 . res0  AS  res0 ,\n           t0 . res1  AS  res1 ,\n           t0 . res2  AS  res2 ,\n           t0 . res3  AS  res3 \n   FROM\n     (SELECT  t0 . GenreId  AS  res0 ,\n              t0 . Name  AS  res1 ,\n             COUNT(DISTINCT  t1 . UnitPrice ) AS  res2 ,\n             (SUM(ALL  t1 . Milliseconds )) / (1000) AS  res3 \n      FROM  Genre  AS  t0 \n      INNER JOIN  Track  AS  t1 \n      GROUP BY  t0 . GenreId ,\n                t0 . Name \n      HAVING ((SUM(ALL  t1 . Milliseconds )) / (1000))  = (300000)) AS  t0 ) AS  t1 \nWHERE ( t0 . GenreId ) = ( t1 . res0 )", 
            "title": "The HAVING clause"
        }, 
        {
            "location": "/user-guide/queries/combining-queries/", 
            "text": "SQL lets you combine the results of multiple \nSELECT\n statements using the\n\nUNION\n, \nINTERSECT\n, and \nEXCEPT\n clauses.\n\n\nSQL Set operations\n\n\nThe SQL Set operations are provided as the \nunion_\n, \nintersect_\n, and \nexcept_\n\nfunctions. SQL also allows an optional \nALL\n clause to be specified with each of\nthese. Beam implements these as \nunionAll_\n, \nintersectAll_\n, and \nexceptAll_\n\nrespectively. Each combinator takes two queries as arguments. The results of\nboth queries will be combined accordingly. The returned type is the same as the\ntype of both query arguments, which must be the same.\n\n\nFor example, suppose we wanted the first and last names of both customers and\nemployees.\n\n\n\n                \n\n                    \n        \n\n            \n        \n\n            \nHaskell\n\n        \n\n    \n        \n\n            \nSqlite3\n\n        \n\n    \n        \n\n            \nPostgres\n\n        \n\n    \n        \n\n    \n        \n\n            \n        \n\n            \nlet customerNames =\n      fmap (\\c -\n (customerFirstName c, customerLastName c))\n           (all_ (customer chinookDb))\n    employeeNames =\n      fmap (\\e -\n (employeeFirstName e, employeeLastName e))\n           (all_ (employee chinookDb))\nin union_ customerNames employeeNames\n\n\n        \n\n    \n        \n\n            \nSELECT \nt0\n.\nFirstName\n AS \nres0\n,\n       \nt0\n.\nLastName\n AS \nres1\n\nFROM \nCustomer\n AS \nt0\n\nUNION\nSELECT \nt0\n.\nFirstName\n AS \nres0\n,\n       \nt0\n.\nLastName\n AS \nres1\n\nFROM \nEmployee\n AS \nt0\n\n\n\n        \n\n    \n        \n\n            \n\n  (SELECT \nt0\n.\nFirstName\n AS \nres0\n,\n          \nt0\n.\nLastName\n AS \nres1\n\n   FROM \nCustomer\n AS \nt0\n)\nUNION\n  (SELECT \nt0\n.\nFirstName\n AS \nres0\n,\n          \nt0\n.\nLastName\n AS \nres1\n\n   FROM \nEmployee\n AS \nt0\n)\n\n\n        \n\n    \n        \n\n    \n                \n\n                    \n\n\nCombining arbitrary set expressions\n\n\nSuppose we wanted all employee and customer first names that were also customer\nlast names but not also employee last names. We could use \nUNION\n to combine the\nresults of a query over the first names of employees and customers, and an\n\nEXCEPT\n to get all customer last names that were not employee ones. Finally, an\n\nINTERSECT\n would give us the result we want. The beam query language allows\nthis and many popular backends do as well, but standard SQL makes it difficult\nto express. Beam has decided to go with the most common implement solution,\nwhich is to allow such nesting. This simplifies the API design.\n\n\nOn backends which allow such nesting (like Postgres), the query is translated\ndirectly. On backends that do not (like SQLite), an appropriate subselect is\ngenerated.\n\n\n\n                \n\n                    \n        \n\n            \n        \n\n            \nHaskell\n\n        \n\n    \n        \n\n            \nSqlite3\n\n        \n\n    \n        \n\n            \nPostgres\n\n        \n\n    \n        \n\n    \n        \n\n            \n        \n\n            \nlet customerFirstNames =\n      fmap customerFirstName\n           (all_ (customer chinookDb))\n    employeeFirstNames =\n      fmap employeeFirstName\n           (all_ (employee chinookDb))\n    customerLastNames =\n      fmap customerLastName\n           (all_ (customer chinookDb))\n    employeeLastNames =\n      fmap employeeLastName\n           (all_ (employee chinookDb))\nin (customerFirstNames `union_`employeeFirstNames) `intersect_`\n   (customerLastNames `except_` employeeLastNames)\n\n\n        \n\n    \n        \n\n            \nSELECT \nt0\n.\nres0\n AS \nres0\n\nFROM\n  (SELECT \nt0\n.\nFirstName\n AS \nres0\n\n   FROM \nCustomer\n AS \nt0\n\n   UNION SELECT \nt0\n.\nFirstName\n AS \nres0\n\n   FROM \nEmployee\n AS \nt0\n) AS \nt0\n INTERSECT\nSELECT \nt0\n.\nres0\n AS \nres0\n\nFROM\n  (SELECT \nt0\n.\nLastName\n AS \nres0\n\n   FROM \nCustomer\n AS \nt0\n\n   EXCEPT SELECT \nt0\n.\nLastName\n AS \nres0\n\n   FROM \nEmployee\n AS \nt0\n) AS \nt0\n\n\n\n        \n\n    \n        \n\n            \n(\n   (SELECT \nt0\n.\nFirstName\n AS \nres0\n\n    FROM \nCustomer\n AS \nt0\n)\n UNION\n   (SELECT \nt0\n.\nFirstName\n AS \nres0\n\n    FROM \nEmployee\n AS \nt0\n)) INTERSECT (\n                                           (SELECT \nt0\n.\nLastName\n AS \nres0\n\n                                            FROM \nCustomer\n AS \nt0\n)\n                                         EXCEPT\n                                           (SELECT \nt0\n.\nLastName\n AS \nres0\n\n                                            FROM \nEmployee\n AS \nt0\n))\n\n\n        \n\n    \n        \n\n    \n                \n\n                    \n\n\nLIMIT\n/\nOFFSET\n and set operations\n\n\nThe \nLIMIT\n and \nOFFSET\n clauses generated by \nlimit_\n and \noffset_\n apply to\nthe entire result of the set operation. Beam will correctly generate the query\nyou specify, placing the \nLIMIT\n and \nOFFSET\n at the appropriate point. If\nnecessary, it will also generate a sub select to preserve the meaning of the\nquery.\n\n\nFor example, to get the second ten full names in common.\n\n\n\n                \n\n                    \n        \n\n            \n        \n\n            \nHaskell\n\n        \n\n    \n        \n\n            \nSqlite3\n\n        \n\n    \n        \n\n            \nPostgres\n\n        \n\n    \n        \n\n    \n        \n\n            \n        \n\n            \nlet customerNames =\n      fmap (\\c -\n (customerFirstName c, customerLastName c))\n           (all_ (customer chinookDb))\n    employeeNames =\n      fmap (\\e -\n (employeeFirstName e, employeeLastName e))\n           (all_ (employee chinookDb))\nin limit_ 10 (union_ customerNames employeeNames)\n\n\n        \n\n    \n        \n\n            \nSELECT \nt0\n.\nFirstName\n AS \nres0\n,\n       \nt0\n.\nLastName\n AS \nres1\n\nFROM \nCustomer\n AS \nt0\n\nUNION\nSELECT \nt0\n.\nFirstName\n AS \nres0\n,\n       \nt0\n.\nLastName\n AS \nres1\n\nFROM \nEmployee\n AS \nt0\n\nLIMIT 10\n\n\n        \n\n    \n        \n\n            \n\n  (SELECT \nt0\n.\nFirstName\n AS \nres0\n,\n          \nt0\n.\nLastName\n AS \nres1\n\n   FROM \nCustomer\n AS \nt0\n)\nUNION\n  (SELECT \nt0\n.\nFirstName\n AS \nres0\n,\n          \nt0\n.\nLastName\n AS \nres1\n\n   FROM \nEmployee\n AS \nt0\n)\nLIMIT 10\n\n\n        \n\n    \n        \n\n    \n                \n\n                    \n\n\nIf we only wanted the union of the first 10 names of each.\n\n\n\n                \n\n                    \n        \n\n            \n        \n\n            \nHaskell\n\n        \n\n    \n        \n\n            \nSqlite3\n\n        \n\n    \n        \n\n            \nPostgres\n\n        \n\n    \n        \n\n    \n        \n\n            \n        \n\n            \nlet customerNames =\n      fmap (\\c -\n (customerFirstName c, customerLastName c))\n           (all_ (customer chinookDb))\n    employeeNames =\n      fmap (\\e -\n (employeeFirstName e, employeeLastName e))\n           (all_ (employee chinookDb))\nin union_ (limit_ 10 customerNames) (limit_ 10 employeeNames)\n\n\n        \n\n    \n        \n\n            \nSELECT \nt0\n.\nres0\n AS \nres0\n,\n       \nt0\n.\nres1\n AS \nres1\n\nFROM\n  (SELECT \nt0\n.\nFirstName\n AS \nres0\n,\n          \nt0\n.\nLastName\n AS \nres1\n\n   FROM \nCustomer\n AS \nt0\n\n   LIMIT 10) AS \nt0\n\nUNION\nSELECT \nt0\n.\nres0\n AS \nres0\n,\n       \nt0\n.\nres1\n AS \nres1\n\nFROM\n  (SELECT \nt0\n.\nFirstName\n AS \nres0\n,\n          \nt0\n.\nLastName\n AS \nres1\n\n   FROM \nEmployee\n AS \nt0\n\n   LIMIT 10) AS \nt0\n\n\n\n        \n\n    \n        \n\n            \n\n  (SELECT \nt0\n.\nres0\n AS \nres0\n,\n          \nt0\n.\nres1\n AS \nres1\n\n   FROM\n     (SELECT \nt0\n.\nFirstName\n AS \nres0\n,\n             \nt0\n.\nLastName\n AS \nres1\n\n      FROM \nCustomer\n AS \nt0\n\n      LIMIT 10) AS \nt0\n)\nUNION\n  (SELECT \nt0\n.\nres0\n AS \nres0\n,\n          \nt0\n.\nres1\n AS \nres1\n\n   FROM\n     (SELECT \nt0\n.\nFirstName\n AS \nres0\n,\n             \nt0\n.\nLastName\n AS \nres1\n\n      FROM \nEmployee\n AS \nt0\n\n      LIMIT 10) AS \nt0\n)", 
            "title": "Combining queries"
        }, 
        {
            "location": "/user-guide/queries/combining-queries/#sql-set-operations", 
            "text": "The SQL Set operations are provided as the  union_ ,  intersect_ , and  except_ \nfunctions. SQL also allows an optional  ALL  clause to be specified with each of\nthese. Beam implements these as  unionAll_ ,  intersectAll_ , and  exceptAll_ \nrespectively. Each combinator takes two queries as arguments. The results of\nboth queries will be combined accordingly. The returned type is the same as the\ntype of both query arguments, which must be the same.  For example, suppose we wanted the first and last names of both customers and\nemployees.  \n                 \n                    \n         \n            \n         \n             Haskell \n         \n    \n         \n             Sqlite3 \n         \n    \n         \n             Postgres \n         \n    \n         \n    \n         \n            \n         \n             let customerNames =\n      fmap (\\c -  (customerFirstName c, customerLastName c))\n           (all_ (customer chinookDb))\n    employeeNames =\n      fmap (\\e -  (employeeFirstName e, employeeLastName e))\n           (all_ (employee chinookDb))\nin union_ customerNames employeeNames \n         \n    \n         \n             SELECT  t0 . FirstName  AS  res0 ,\n        t0 . LastName  AS  res1 \nFROM  Customer  AS  t0 \nUNION\nSELECT  t0 . FirstName  AS  res0 ,\n        t0 . LastName  AS  res1 \nFROM  Employee  AS  t0  \n         \n    \n         \n             \n  (SELECT  t0 . FirstName  AS  res0 ,\n           t0 . LastName  AS  res1 \n   FROM  Customer  AS  t0 )\nUNION\n  (SELECT  t0 . FirstName  AS  res0 ,\n           t0 . LastName  AS  res1 \n   FROM  Employee  AS  t0 )", 
            "title": "SQL Set operations"
        }, 
        {
            "location": "/user-guide/queries/combining-queries/#combining-arbitrary-set-expressions", 
            "text": "Suppose we wanted all employee and customer first names that were also customer\nlast names but not also employee last names. We could use  UNION  to combine the\nresults of a query over the first names of employees and customers, and an EXCEPT  to get all customer last names that were not employee ones. Finally, an INTERSECT  would give us the result we want. The beam query language allows\nthis and many popular backends do as well, but standard SQL makes it difficult\nto express. Beam has decided to go with the most common implement solution,\nwhich is to allow such nesting. This simplifies the API design.  On backends which allow such nesting (like Postgres), the query is translated\ndirectly. On backends that do not (like SQLite), an appropriate subselect is\ngenerated.  \n                 \n                    \n         \n            \n         \n             Haskell \n         \n    \n         \n             Sqlite3 \n         \n    \n         \n             Postgres \n         \n    \n         \n    \n         \n            \n         \n             let customerFirstNames =\n      fmap customerFirstName\n           (all_ (customer chinookDb))\n    employeeFirstNames =\n      fmap employeeFirstName\n           (all_ (employee chinookDb))\n    customerLastNames =\n      fmap customerLastName\n           (all_ (customer chinookDb))\n    employeeLastNames =\n      fmap employeeLastName\n           (all_ (employee chinookDb))\nin (customerFirstNames `union_`employeeFirstNames) `intersect_`\n   (customerLastNames `except_` employeeLastNames) \n         \n    \n         \n             SELECT  t0 . res0  AS  res0 \nFROM\n  (SELECT  t0 . FirstName  AS  res0 \n   FROM  Customer  AS  t0 \n   UNION SELECT  t0 . FirstName  AS  res0 \n   FROM  Employee  AS  t0 ) AS  t0  INTERSECT\nSELECT  t0 . res0  AS  res0 \nFROM\n  (SELECT  t0 . LastName  AS  res0 \n   FROM  Customer  AS  t0 \n   EXCEPT SELECT  t0 . LastName  AS  res0 \n   FROM  Employee  AS  t0 ) AS  t0  \n         \n    \n         \n             (\n   (SELECT  t0 . FirstName  AS  res0 \n    FROM  Customer  AS  t0 )\n UNION\n   (SELECT  t0 . FirstName  AS  res0 \n    FROM  Employee  AS  t0 )) INTERSECT (\n                                           (SELECT  t0 . LastName  AS  res0 \n                                            FROM  Customer  AS  t0 )\n                                         EXCEPT\n                                           (SELECT  t0 . LastName  AS  res0 \n                                            FROM  Employee  AS  t0 ))", 
            "title": "Combining arbitrary set expressions"
        }, 
        {
            "location": "/user-guide/queries/combining-queries/#limitoffset-and-set-operations", 
            "text": "The  LIMIT  and  OFFSET  clauses generated by  limit_  and  offset_  apply to\nthe entire result of the set operation. Beam will correctly generate the query\nyou specify, placing the  LIMIT  and  OFFSET  at the appropriate point. If\nnecessary, it will also generate a sub select to preserve the meaning of the\nquery.  For example, to get the second ten full names in common.  \n                 \n                    \n         \n            \n         \n             Haskell \n         \n    \n         \n             Sqlite3 \n         \n    \n         \n             Postgres \n         \n    \n         \n    \n         \n            \n         \n             let customerNames =\n      fmap (\\c -  (customerFirstName c, customerLastName c))\n           (all_ (customer chinookDb))\n    employeeNames =\n      fmap (\\e -  (employeeFirstName e, employeeLastName e))\n           (all_ (employee chinookDb))\nin limit_ 10 (union_ customerNames employeeNames) \n         \n    \n         \n             SELECT  t0 . FirstName  AS  res0 ,\n        t0 . LastName  AS  res1 \nFROM  Customer  AS  t0 \nUNION\nSELECT  t0 . FirstName  AS  res0 ,\n        t0 . LastName  AS  res1 \nFROM  Employee  AS  t0 \nLIMIT 10 \n         \n    \n         \n             \n  (SELECT  t0 . FirstName  AS  res0 ,\n           t0 . LastName  AS  res1 \n   FROM  Customer  AS  t0 )\nUNION\n  (SELECT  t0 . FirstName  AS  res0 ,\n           t0 . LastName  AS  res1 \n   FROM  Employee  AS  t0 )\nLIMIT 10 \n         \n    \n         \n    \n                 \n                      If we only wanted the union of the first 10 names of each.  \n                 \n                    \n         \n            \n         \n             Haskell \n         \n    \n         \n             Sqlite3 \n         \n    \n         \n             Postgres \n         \n    \n         \n    \n         \n            \n         \n             let customerNames =\n      fmap (\\c -  (customerFirstName c, customerLastName c))\n           (all_ (customer chinookDb))\n    employeeNames =\n      fmap (\\e -  (employeeFirstName e, employeeLastName e))\n           (all_ (employee chinookDb))\nin union_ (limit_ 10 customerNames) (limit_ 10 employeeNames) \n         \n    \n         \n             SELECT  t0 . res0  AS  res0 ,\n        t0 . res1  AS  res1 \nFROM\n  (SELECT  t0 . FirstName  AS  res0 ,\n           t0 . LastName  AS  res1 \n   FROM  Customer  AS  t0 \n   LIMIT 10) AS  t0 \nUNION\nSELECT  t0 . res0  AS  res0 ,\n        t0 . res1  AS  res1 \nFROM\n  (SELECT  t0 . FirstName  AS  res0 ,\n           t0 . LastName  AS  res1 \n   FROM  Employee  AS  t0 \n   LIMIT 10) AS  t0  \n         \n    \n         \n             \n  (SELECT  t0 . res0  AS  res0 ,\n           t0 . res1  AS  res1 \n   FROM\n     (SELECT  t0 . FirstName  AS  res0 ,\n              t0 . LastName  AS  res1 \n      FROM  Customer  AS  t0 \n      LIMIT 10) AS  t0 )\nUNION\n  (SELECT  t0 . res0  AS  res0 ,\n           t0 . res1  AS  res1 \n   FROM\n     (SELECT  t0 . FirstName  AS  res0 ,\n              t0 . LastName  AS  res1 \n      FROM  Employee  AS  t0 \n      LIMIT 10) AS  t0 )", 
            "title": "LIMIT/OFFSET and set operations"
        }, 
        {
            "location": "/user-guide/queries/window-functions/", 
            "text": "Window functions allow you to calculate aggregates over portions of your result\nset. They are defined in SQL2003. Some databases use the alternative\nnomenclature \nanalytic functions\n. They are expressed in SQL with the \nOVER\n\nclause. The\n\nPostgres documentation\n\noffers a good overview of window functions.\n\n\nThe \nwithWindow_\n function\n\n\nWhen you want to add windows to a query, use the \nwithWindow_\n function to\nintroduce your frames, and compute the projection. You may notice that this is a\ndeparture from SQL syntax, where you can define window expressions inline. Beam\nseeks to be type-safe. Queries with window functions follow slightly different\nrules. Wrapping such a query with a special function allows beam to enforce\nthese rules.\n\n\nFor example, to get each invoice along with the average invoice total by each\ncustomer, use \nwithWindow_\n as follows.\n\n\n\n                \n\n                    \n        \n\n            \n        \n\n            \nHaskell\n\n        \n\n    \n        \n\n            \nPostgres\n\n        \n\n    \n        \n\n    \n        \n\n            \n        \n\n            \nwithWindow_ (\\i -\n frame_ (partitionBy_ (invoiceCustomer i)) noOrder_ noBounds_)\n            (\\i w -\n (i, avg_ (invoiceTotal i) `over_` w))\n            (all_ (invoice chinookDb))\n\n\n        \n\n    \n        \n\n            \nSELECT \nt0\n.\nInvoiceId\n AS \nres0\n,\n       \nt0\n.\nCustomerId\n AS \nres1\n,\n       \nt0\n.\nInvoiceDate\n AS \nres2\n,\n       \nt0\n.\nBillingAddress\n AS \nres3\n,\n       \nt0\n.\nBillingCity\n AS \nres4\n,\n       \nt0\n.\nBillingState\n AS \nres5\n,\n       \nt0\n.\nBillingCountry\n AS \nres6\n,\n       \nt0\n.\nBillingPostalCode\n AS \nres7\n,\n       \nt0\n.\nTotal\n AS \nres8\n,\n       AVG(\nt0\n.\nTotal\n) OVER (PARTITION BY \nt0\n.\nCustomerId\n) AS \nres9\n\nFROM \nInvoice\n AS \nt0\n\n\n\n        \n\n    \n        \n\n    \n                \n\n                    \n\n\nOr to get each invoice along with the ranking of each invoice by total per\ncustomer \nand\n the overall ranking,\n\n\n\n                \n\n                    \n        \n\n            \n        \n\n            \nHaskell\n\n        \n\n    \n        \n\n            \nPostgres\n\n        \n\n    \n        \n\n    \n        \n\n            \n        \n\n            \nwithWindow_ (\\i -\n ( frame_ noPartition_ (orderPartitionBy_ (asc_ (invoiceTotal i))) noBounds_\n                   , frame_ (partitionBy_ (invoiceCustomer i)) (orderPartitionBy_ (asc_ (invoiceTotal i))) noBounds_ ))\n            (\\i (allInvoices, customerInvoices) -\n (i, rank_ `over_` allInvoices, rank_ `over_` customerInvoices))\n            (all_ (invoice chinookDb))\n\n\n        \n\n    \n        \n\n            \nSELECT \nt0\n.\nInvoiceId\n AS \nres0\n,\n       \nt0\n.\nCustomerId\n AS \nres1\n,\n       \nt0\n.\nInvoiceDate\n AS \nres2\n,\n       \nt0\n.\nBillingAddress\n AS \nres3\n,\n       \nt0\n.\nBillingCity\n AS \nres4\n,\n       \nt0\n.\nBillingState\n AS \nres5\n,\n       \nt0\n.\nBillingCountry\n AS \nres6\n,\n       \nt0\n.\nBillingPostalCode\n AS \nres7\n,\n       \nt0\n.\nTotal\n AS \nres8\n,\n       RANK() OVER (\n                    ORDER BY \nt0\n.\nTotal\n ASC) AS \nres9\n,\n       RANK() OVER (PARTITION BY \nt0\n.\nCustomerId\n\n                    ORDER BY \nt0\n.\nTotal\n ASC) AS \nres10\n\nFROM \nInvoice\n AS \nt0\n\n\n\n        \n\n    \n        \n\n    \n                \n\n                    \n\n\n\n\nNote\n\n\nrank_\n is only available in backends that implement the optional SQL2003\nT621 feature \"Enhanced Numeric Functions\". Beam syntaxes that implement this\nfunctionality implement the\n\nIsSql2003ExpressionEnhancedNumericFunctionsSyntax\n type class.\n\n\n\n\nNotice that aggregates over the result of the window expression work as you'd\nexpect. Beam automatically generates a subquery once a query has been windowed.\nFor example, to get the sum of the totals of the invoices, by rank.\n\n\n\n                \n\n                    \n        \n\n            \n        \n\n            \nHaskell\n\n        \n\n    \n        \n\n            \nPostgres\n\n        \n\n    \n        \n\n    \n        \n\n            \n        \n\n            \norderBy_ (\\(rank, _) -\n asc_ rank) $\naggregate_ (\\(i, rank) -\n (group_ rank, sum_ $ invoiceTotal i)) $\nwithWindow_ (\\i -\n frame_ (partitionBy_ (invoiceCustomer i)) (orderPartitionBy_ (asc_ (invoiceTotal i))) noBounds_)\n            (\\i w -\n (i, rank_ `over_` w))\n            (all_ (invoice chinookDb))\n\n\n        \n\n    \n        \n\n            \nSELECT \nt0\n.\nres9\n AS \nres0\n,\n       SUM(\nt0\n.\nres8\n) AS \nres1\n\nFROM\n  (SELECT \nt0\n.\nInvoiceId\n AS \nres0\n,\n          \nt0\n.\nCustomerId\n AS \nres1\n,\n          \nt0\n.\nInvoiceDate\n AS \nres2\n,\n          \nt0\n.\nBillingAddress\n AS \nres3\n,\n          \nt0\n.\nBillingCity\n AS \nres4\n,\n          \nt0\n.\nBillingState\n AS \nres5\n,\n          \nt0\n.\nBillingCountry\n AS \nres6\n,\n          \nt0\n.\nBillingPostalCode\n AS \nres7\n,\n          \nt0\n.\nTotal\n AS \nres8\n,\n          RANK() OVER (PARTITION BY \nt0\n.\nCustomerId\n\n                       ORDER BY \nt0\n.\nTotal\n ASC) AS \nres9\n\n   FROM \nInvoice\n AS \nt0\n) AS \nt0\n\nGROUP BY \nt0\n.\nres9\n\nORDER BY \nt0\n.\nres9\n ASC\n\n\n        \n\n    \n        \n\n    \n                \n\n                    \n\n\nFrame syntax\n\n\nThe \nframe_\n function takes a partition, ordering, and bounds parameter, all of\nwhich are optional. To specify no partition, use \nnoPartition_\n. For no\nordering, use \nnoOrder_\n. For no bounds, use \nnoBounds_\n.\n\n\nTo specify a partition, use \npartitionBy_\n with an expression or a tuple of\nexpressions. To specify an ordering use \norderPartitionBy_\n with an ordering\nexpression or a tuple of ordering expressions. Ordering expressions are scalar\nexpressions passed to either \nasc_\n or \ndesc_\n. Finally, to specify bounds, use\n\nbounds_\n or \nfromBound_\n. \nfromBound_\n starts the window at the specified\nposition, which can be \nunbounded_\n (the default) to include all rows seen thus\nfar. \nbounds_\n lets you specify an optional ending bound, which can be \nNothing\n\n(the default), \nJust unbounded_\n (the semantic default, but producing an\nexplicit bound syntactically), or \nJust (nrows_ x)\n, where \nx\n is an integer\nexpression, specifying the number of rows before or after to include in the\ncalculation.\n\n\nThe following query illustrates some of these features. Along with each invoice, it returns\n\n\n\n\nThe average total of all invoices, given by the frame with no partition, ordering, and bounds.\n\n\nThe average total of all invoices, by customer.\n\n\nThe rank of each invoice over all the rows, when ordered by total.\n\n\nThe average of the totals of the invoices starting at the two immediately\n  preceding and ending with the two immediately succeeding invoices, when\n  ordered by date.\n\n\n\n\n\n                \n\n                    \n        \n\n            \n        \n\n            \nHaskell\n\n        \n\n    \n        \n\n            \nPostgres\n\n        \n\n    \n        \n\n    \n        \n\n            \n        \n\n            \nwithWindow_ (\\i -\n ( frame_ noPartition_ noOrder_ noBounds_\n                   , frame_ (partitionBy_ (invoiceCustomer i)) noOrder_ noBounds_\n                   , frame_ noPartition_ (orderPartitionBy_ (asc_ (invoiceTotal i))) noBounds_\n                   , frame_ noPartition_ (orderPartitionBy_ (asc_ (invoiceDate i))) (bounds_ (nrows_ 2) (Just (nrows_ 2)))))\n            (\\i (allRows_, sameCustomer_, totals_, fourInvoicesAround_) -\n\n                 ( i\n                 , avg_ (invoiceTotal i) `over_` allRows_\n                 , avg_ (invoiceTotal i) `over_` sameCustomer_\n                 , rank_ `over_` totals_\n                 , avg_ (invoiceTotal i) `over_` fourInvoicesAround_ ))\n            (all_ (invoice chinookDb))\n\n\n        \n\n    \n        \n\n            \nSELECT \nt0\n.\nInvoiceId\n AS \nres0\n,\n       \nt0\n.\nCustomerId\n AS \nres1\n,\n       \nt0\n.\nInvoiceDate\n AS \nres2\n,\n       \nt0\n.\nBillingAddress\n AS \nres3\n,\n       \nt0\n.\nBillingCity\n AS \nres4\n,\n       \nt0\n.\nBillingState\n AS \nres5\n,\n       \nt0\n.\nBillingCountry\n AS \nres6\n,\n       \nt0\n.\nBillingPostalCode\n AS \nres7\n,\n       \nt0\n.\nTotal\n AS \nres8\n,\n       AVG(\nt0\n.\nTotal\n) OVER () AS \nres9\n,\n       AVG(\nt0\n.\nTotal\n) OVER (PARTITION BY \nt0\n.\nCustomerId\n) AS \nres10\n,\n       RANK() OVER (\n                    ORDER BY \nt0\n.\nTotal\n ASC) AS \nres11\n,\n       AVG(\nt0\n.\nTotal\n) OVER (\n                               ORDER BY \nt0\n.\nInvoiceDate\n ASC ROWS BETWEEN 2 PRECEDING AND 2 FOLLOWING) AS \nres12\n\nFROM \nInvoice\n AS \nt0", 
            "title": "Window functions"
        }, 
        {
            "location": "/user-guide/queries/window-functions/#the-withwindow_-function", 
            "text": "When you want to add windows to a query, use the  withWindow_  function to\nintroduce your frames, and compute the projection. You may notice that this is a\ndeparture from SQL syntax, where you can define window expressions inline. Beam\nseeks to be type-safe. Queries with window functions follow slightly different\nrules. Wrapping such a query with a special function allows beam to enforce\nthese rules.  For example, to get each invoice along with the average invoice total by each\ncustomer, use  withWindow_  as follows.  \n                 \n                    \n         \n            \n         \n             Haskell \n         \n    \n         \n             Postgres \n         \n    \n         \n    \n         \n            \n         \n             withWindow_ (\\i -  frame_ (partitionBy_ (invoiceCustomer i)) noOrder_ noBounds_)\n            (\\i w -  (i, avg_ (invoiceTotal i) `over_` w))\n            (all_ (invoice chinookDb)) \n         \n    \n         \n             SELECT  t0 . InvoiceId  AS  res0 ,\n        t0 . CustomerId  AS  res1 ,\n        t0 . InvoiceDate  AS  res2 ,\n        t0 . BillingAddress  AS  res3 ,\n        t0 . BillingCity  AS  res4 ,\n        t0 . BillingState  AS  res5 ,\n        t0 . BillingCountry  AS  res6 ,\n        t0 . BillingPostalCode  AS  res7 ,\n        t0 . Total  AS  res8 ,\n       AVG( t0 . Total ) OVER (PARTITION BY  t0 . CustomerId ) AS  res9 \nFROM  Invoice  AS  t0  \n         \n    \n         \n    \n                 \n                      Or to get each invoice along with the ranking of each invoice by total per\ncustomer  and  the overall ranking,  \n                 \n                    \n         \n            \n         \n             Haskell \n         \n    \n         \n             Postgres \n         \n    \n         \n    \n         \n            \n         \n             withWindow_ (\\i -  ( frame_ noPartition_ (orderPartitionBy_ (asc_ (invoiceTotal i))) noBounds_\n                   , frame_ (partitionBy_ (invoiceCustomer i)) (orderPartitionBy_ (asc_ (invoiceTotal i))) noBounds_ ))\n            (\\i (allInvoices, customerInvoices) -  (i, rank_ `over_` allInvoices, rank_ `over_` customerInvoices))\n            (all_ (invoice chinookDb)) \n         \n    \n         \n             SELECT  t0 . InvoiceId  AS  res0 ,\n        t0 . CustomerId  AS  res1 ,\n        t0 . InvoiceDate  AS  res2 ,\n        t0 . BillingAddress  AS  res3 ,\n        t0 . BillingCity  AS  res4 ,\n        t0 . BillingState  AS  res5 ,\n        t0 . BillingCountry  AS  res6 ,\n        t0 . BillingPostalCode  AS  res7 ,\n        t0 . Total  AS  res8 ,\n       RANK() OVER (\n                    ORDER BY  t0 . Total  ASC) AS  res9 ,\n       RANK() OVER (PARTITION BY  t0 . CustomerId \n                    ORDER BY  t0 . Total  ASC) AS  res10 \nFROM  Invoice  AS  t0  \n         \n    \n         \n    \n                 \n                       Note  rank_  is only available in backends that implement the optional SQL2003\nT621 feature \"Enhanced Numeric Functions\". Beam syntaxes that implement this\nfunctionality implement the IsSql2003ExpressionEnhancedNumericFunctionsSyntax  type class.   Notice that aggregates over the result of the window expression work as you'd\nexpect. Beam automatically generates a subquery once a query has been windowed.\nFor example, to get the sum of the totals of the invoices, by rank.  \n                 \n                    \n         \n            \n         \n             Haskell \n         \n    \n         \n             Postgres \n         \n    \n         \n    \n         \n            \n         \n             orderBy_ (\\(rank, _) -  asc_ rank) $\naggregate_ (\\(i, rank) -  (group_ rank, sum_ $ invoiceTotal i)) $\nwithWindow_ (\\i -  frame_ (partitionBy_ (invoiceCustomer i)) (orderPartitionBy_ (asc_ (invoiceTotal i))) noBounds_)\n            (\\i w -  (i, rank_ `over_` w))\n            (all_ (invoice chinookDb)) \n         \n    \n         \n             SELECT  t0 . res9  AS  res0 ,\n       SUM( t0 . res8 ) AS  res1 \nFROM\n  (SELECT  t0 . InvoiceId  AS  res0 ,\n           t0 . CustomerId  AS  res1 ,\n           t0 . InvoiceDate  AS  res2 ,\n           t0 . BillingAddress  AS  res3 ,\n           t0 . BillingCity  AS  res4 ,\n           t0 . BillingState  AS  res5 ,\n           t0 . BillingCountry  AS  res6 ,\n           t0 . BillingPostalCode  AS  res7 ,\n           t0 . Total  AS  res8 ,\n          RANK() OVER (PARTITION BY  t0 . CustomerId \n                       ORDER BY  t0 . Total  ASC) AS  res9 \n   FROM  Invoice  AS  t0 ) AS  t0 \nGROUP BY  t0 . res9 \nORDER BY  t0 . res9  ASC", 
            "title": "The withWindow_ function"
        }, 
        {
            "location": "/user-guide/queries/window-functions/#frame-syntax", 
            "text": "The  frame_  function takes a partition, ordering, and bounds parameter, all of\nwhich are optional. To specify no partition, use  noPartition_ . For no\nordering, use  noOrder_ . For no bounds, use  noBounds_ .  To specify a partition, use  partitionBy_  with an expression or a tuple of\nexpressions. To specify an ordering use  orderPartitionBy_  with an ordering\nexpression or a tuple of ordering expressions. Ordering expressions are scalar\nexpressions passed to either  asc_  or  desc_ . Finally, to specify bounds, use bounds_  or  fromBound_ .  fromBound_  starts the window at the specified\nposition, which can be  unbounded_  (the default) to include all rows seen thus\nfar.  bounds_  lets you specify an optional ending bound, which can be  Nothing \n(the default),  Just unbounded_  (the semantic default, but producing an\nexplicit bound syntactically), or  Just (nrows_ x) , where  x  is an integer\nexpression, specifying the number of rows before or after to include in the\ncalculation.  The following query illustrates some of these features. Along with each invoice, it returns   The average total of all invoices, given by the frame with no partition, ordering, and bounds.  The average total of all invoices, by customer.  The rank of each invoice over all the rows, when ordered by total.  The average of the totals of the invoices starting at the two immediately\n  preceding and ending with the two immediately succeeding invoices, when\n  ordered by date.   \n                 \n                    \n         \n            \n         \n             Haskell \n         \n    \n         \n             Postgres \n         \n    \n         \n    \n         \n            \n         \n             withWindow_ (\\i -  ( frame_ noPartition_ noOrder_ noBounds_\n                   , frame_ (partitionBy_ (invoiceCustomer i)) noOrder_ noBounds_\n                   , frame_ noPartition_ (orderPartitionBy_ (asc_ (invoiceTotal i))) noBounds_\n                   , frame_ noPartition_ (orderPartitionBy_ (asc_ (invoiceDate i))) (bounds_ (nrows_ 2) (Just (nrows_ 2)))))\n            (\\i (allRows_, sameCustomer_, totals_, fourInvoicesAround_) - \n                 ( i\n                 , avg_ (invoiceTotal i) `over_` allRows_\n                 , avg_ (invoiceTotal i) `over_` sameCustomer_\n                 , rank_ `over_` totals_\n                 , avg_ (invoiceTotal i) `over_` fourInvoicesAround_ ))\n            (all_ (invoice chinookDb)) \n         \n    \n         \n             SELECT  t0 . InvoiceId  AS  res0 ,\n        t0 . CustomerId  AS  res1 ,\n        t0 . InvoiceDate  AS  res2 ,\n        t0 . BillingAddress  AS  res3 ,\n        t0 . BillingCity  AS  res4 ,\n        t0 . BillingState  AS  res5 ,\n        t0 . BillingCountry  AS  res6 ,\n        t0 . BillingPostalCode  AS  res7 ,\n        t0 . Total  AS  res8 ,\n       AVG( t0 . Total ) OVER () AS  res9 ,\n       AVG( t0 . Total ) OVER (PARTITION BY  t0 . CustomerId ) AS  res10 ,\n       RANK() OVER (\n                    ORDER BY  t0 . Total  ASC) AS  res11 ,\n       AVG( t0 . Total ) OVER (\n                               ORDER BY  t0 . InvoiceDate  ASC ROWS BETWEEN 2 PRECEDING AND 2 FOLLOWING) AS  res12 \nFROM  Invoice  AS  t0", 
            "title": "Frame syntax"
        }, 
        {
            "location": "/user-guide/queries/advanced-features/", 
            "text": "This page documents other advanced features that beam supports across backends\nthat support them.\n\n\nSQL2003 T612: Advanced OLAP operations\n\n\nThis optional SQL2003 feature allows attaching arbitrary \nFILTER (WHERE ..)\n\nclauses to aggregates. During querying only rows matching the given expression\nare included in computing the aggregate. This can often be simulated in other\ndatabases by an appropriate \nCASE\n expression, but beam will not do this\ntranslation.\n\n\n\n                \n\n                    \n        \n\n            \n        \n\n            \nHaskell\n\n        \n\n    \n        \n\n            \nPostgres\n\n        \n\n    \n        \n\n    \n        \n\n            \n        \n\n            \naggregate_ (\\i -\n (group_ (invoiceCustomer i), as_ @Int $ countAll_ `filterWhere_` (invoiceTotal i \n. 500), as_ @Int $ countAll_ `filterWhere_` (invoiceTotal i \n. 100))) $\nall_ (invoice chinookDb)\n\n\n        \n\n    \n        \n\n            \nSELECT \nt0\n.\nCustomerId\n AS \nres0\n,\n       COUNT(*) FILTER (\n                        WHERE (\nt0\n.\nTotal\n) \n ('500.0')) AS \nres1\n,\n       COUNT(*) FILTER (\n                        WHERE (\nt0\n.\nTotal\n) \n ('100.0')) AS \nres2\n\nFROM \nInvoice\n AS \nt0\n\nGROUP BY \nt0\n.\nCustomerId\n\n\n\n        \n\n    \n        \n\n    \n                \n\n                    \n\n\nThese combine as you'd expect with window functions. For example, to return each\ninvoice along with the average total of all invoices by the same customer where\nthe invoice was billed to an address in Los Angeles,\n\n\n\n                \n\n                    \n        \n\n            \n        \n\n            \nHaskell\n\n        \n\n    \n        \n\n            \nPostgres\n\n        \n\n    \n        \n\n    \n        \n\n            \n        \n\n            \nwithWindow_ (\\i -\n frame_ (partitionBy_ (invoiceCustomer i)) noOrder_ noBounds_)\n            (\\i w -\n (i, avg_ (invoiceTotal i) `filterWhere_` (addressCity (invoiceBillingAddress i) ==. just_ \nLos Angeles\n) `over_` w))\n            (all_ (invoice chinookDb))\n\n\n        \n\n    \n        \n\n            \nSELECT \nt0\n.\nInvoiceId\n AS \nres0\n,\n       \nt0\n.\nCustomerId\n AS \nres1\n,\n       \nt0\n.\nInvoiceDate\n AS \nres2\n,\n       \nt0\n.\nBillingAddress\n AS \nres3\n,\n       \nt0\n.\nBillingCity\n AS \nres4\n,\n       \nt0\n.\nBillingState\n AS \nres5\n,\n       \nt0\n.\nBillingCountry\n AS \nres6\n,\n       \nt0\n.\nBillingPostalCode\n AS \nres7\n,\n       \nt0\n.\nTotal\n AS \nres8\n,\n       AVG(\nt0\n.\nTotal\n) FILTER (\n                                 WHERE (\nt0\n.\nBillingCity\n) = ('Los Angeles')) OVER (PARTITION BY \nt0\n.\nCustomerId\n) AS \nres9\n\nFROM \nInvoice\n AS \nt0\n\n\n\n        \n\n    \n        \n\n    \n                \n\n                    \n\n\n\n\nDanger\n\n\nFILTER (WHERE ..)\n must be applied directly to a SQL aggregate function,\nbut this isn't enforced at compile time. This may be fixed in a later\nversion of beam.\n\n\n\n\nSQL2003 T621: Enhanced Numeric functions\n\n\nThis extension provides various numeric statistic functions for SQL. The only\none beam currently implements is \nRANK()\n via the \nrank_\n function.\nContributions are appreciated!", 
            "title": "Advanced features"
        }, 
        {
            "location": "/user-guide/queries/advanced-features/#sql2003-t612-advanced-olap-operations", 
            "text": "This optional SQL2003 feature allows attaching arbitrary  FILTER (WHERE ..) \nclauses to aggregates. During querying only rows matching the given expression\nare included in computing the aggregate. This can often be simulated in other\ndatabases by an appropriate  CASE  expression, but beam will not do this\ntranslation.  \n                 \n                    \n         \n            \n         \n             Haskell \n         \n    \n         \n             Postgres \n         \n    \n         \n    \n         \n            \n         \n             aggregate_ (\\i -  (group_ (invoiceCustomer i), as_ @Int $ countAll_ `filterWhere_` (invoiceTotal i  . 500), as_ @Int $ countAll_ `filterWhere_` (invoiceTotal i  . 100))) $\nall_ (invoice chinookDb) \n         \n    \n         \n             SELECT  t0 . CustomerId  AS  res0 ,\n       COUNT(*) FILTER (\n                        WHERE ( t0 . Total )   ('500.0')) AS  res1 ,\n       COUNT(*) FILTER (\n                        WHERE ( t0 . Total )   ('100.0')) AS  res2 \nFROM  Invoice  AS  t0 \nGROUP BY  t0 . CustomerId  \n         \n    \n         \n    \n                 \n                      These combine as you'd expect with window functions. For example, to return each\ninvoice along with the average total of all invoices by the same customer where\nthe invoice was billed to an address in Los Angeles,  \n                 \n                    \n         \n            \n         \n             Haskell \n         \n    \n         \n             Postgres \n         \n    \n         \n    \n         \n            \n         \n             withWindow_ (\\i -  frame_ (partitionBy_ (invoiceCustomer i)) noOrder_ noBounds_)\n            (\\i w -  (i, avg_ (invoiceTotal i) `filterWhere_` (addressCity (invoiceBillingAddress i) ==. just_  Los Angeles ) `over_` w))\n            (all_ (invoice chinookDb)) \n         \n    \n         \n             SELECT  t0 . InvoiceId  AS  res0 ,\n        t0 . CustomerId  AS  res1 ,\n        t0 . InvoiceDate  AS  res2 ,\n        t0 . BillingAddress  AS  res3 ,\n        t0 . BillingCity  AS  res4 ,\n        t0 . BillingState  AS  res5 ,\n        t0 . BillingCountry  AS  res6 ,\n        t0 . BillingPostalCode  AS  res7 ,\n        t0 . Total  AS  res8 ,\n       AVG( t0 . Total ) FILTER (\n                                 WHERE ( t0 . BillingCity ) = ('Los Angeles')) OVER (PARTITION BY  t0 . CustomerId ) AS  res9 \nFROM  Invoice  AS  t0  \n         \n    \n         \n    \n                 \n                       Danger  FILTER (WHERE ..)  must be applied directly to a SQL aggregate function,\nbut this isn't enforced at compile time. This may be fixed in a later\nversion of beam.", 
            "title": "SQL2003 T612: Advanced OLAP operations"
        }, 
        {
            "location": "/user-guide/queries/advanced-features/#sql2003-t621-enhanced-numeric-functions", 
            "text": "This extension provides various numeric statistic functions for SQL. The only\none beam currently implements is  RANK()  via the  rank_  function.\nContributions are appreciated!", 
            "title": "SQL2003 T621: Enhanced Numeric functions"
        }, 
        {
            "location": "/schema-guide/migrations/", 
            "text": "Warning\n\n\nThe \nbeam-migrations\n package is still a WIP. The following manual represents\nboth planned and implemented features.\n\n\n\n\nIn the User Guide we saw how to declare a schema for an already created database\nand use it to perform queries. Beam can also manage a database schema based on\nHaskell datatypes you feed it.\n\n\nThe Beam Migrations Framework is meant to be a robust, modular, and opinionated\nway of managing schema changes. It is an optional part of beam provided in the\n\nbeam-migrate\n package.\n\n\nInstall the migrations framework and tool by running\n\n\n$ cabal install beam-migrate\n# or\n$ stack install beam-migrate\n\n\n\n\nIf you use \nstack\n make sure you always use \nstack exec -- beam-migrate\n instead\nof the typical \nbeam-migrate\n command in order to have the package path\nautomatically and correctly set for you.\n\n\nThe Opinions\n\n\nAs stated above, \nbeam-migrate\n is an \nopinionated\n migrations framework. It\nspecifies a precise way to lay out your schema definitions.\n\n\nYou can use the \nbeam-migrate new \nmodule-name\n \ndb-type\n command to create a new\nmigration schema module. This will create the following directory structure\n\n\nmodule-name\n.hs\n\nmodule-name\n/V0001.hs\n\n\n\n\nIf you open up \nmodule-name\n.hs\n, it will contain the following\n\n\nmodule \nmodule-name\n ( module \nmodule-name\n.V0001, db, migration ) where\n\nimport Database.Beam\n\nimport qualified \nmodule-name\n.V0001 as V0001\n\ndb :: DatabaseSettings be \ndb-type\n\ndb = V0001.db\n\nmigration :: MigrationSteps be \ndb-type\n\nmigration = migrationStep \nInitial schema\n V0001.migration\n\n\n\n\nIf you specify the \n--backend\n option while running \nbeam-migrate new\n, your\nschema will be specialized to the specific backend.\n\n\nWhen you create a new version of your schema, you will \nnot\n delete the old one.\nInstead, you will copy it, increase the version number, make your changes to the\ndatabase schema, write an appropriate migration, and then update the top-level\nmodule to use the newest schema and invoke the latest migration.", 
            "title": "The Migrations Framework"
        }, 
        {
            "location": "/schema-guide/migrations/#the-opinions", 
            "text": "As stated above,  beam-migrate  is an  opinionated  migrations framework. It\nspecifies a precise way to lay out your schema definitions.  You can use the  beam-migrate new  module-name   db-type  command to create a new\nmigration schema module. This will create the following directory structure  module-name .hs module-name /V0001.hs  If you open up  module-name .hs , it will contain the following  module  module-name  ( module  module-name .V0001, db, migration ) where\n\nimport Database.Beam\n\nimport qualified  module-name .V0001 as V0001\n\ndb :: DatabaseSettings be  db-type \ndb = V0001.db\n\nmigration :: MigrationSteps be  db-type \nmigration = migrationStep  Initial schema  V0001.migration  If you specify the  --backend  option while running  beam-migrate new , your\nschema will be specialized to the specific backend.  When you create a new version of your schema, you will  not  delete the old one.\nInstead, you will copy it, increase the version number, make your changes to the\ndatabase schema, write an appropriate migration, and then update the top-level\nmodule to use the newest schema and invoke the latest migration.", 
            "title": "The Opinions"
        }, 
        {
            "location": "/schema-guide/tool/", 
            "text": "Tool", 
            "title": "The beam-migrate tool"
        }, 
        {
            "location": "/schema-guide/supported/", 
            "text": "supported", 
            "title": "Supported migrations"
        }, 
        {
            "location": "/user-guide/backends/beam-postgres/", 
            "text": "The \nbeam-postgres\n backend is the most feature complete SQL backend for beam.\nThe Postgres RDBMS supports most of the standards beam follows, so you can\nusually expect most queries to simply work. Additionally, \nbeam-postgres\n is\npart of the standard Beam distribution, and so upgrades are applied\nperiodically, and new functions are added to achieve feature-parity with the\nlatest Postgres stable\n\n\nPostgres-specific data types\n\n\nPostgres has several data types not available from \nbeam-core\n. The\n\nbeam-postgres\n library provides several types and functions to make working\nwith these easier.\n\n\nThe \ntsvector\n and \ntsquery\n types\n\n\nThe \ntsvector\n and \ntsquery\n types form the basis of full-text search in\nPostgres.", 
            "title": "beam-postgres"
        }, 
        {
            "location": "/user-guide/backends/beam-postgres/#postgres-specific-data-types", 
            "text": "Postgres has several data types not available from  beam-core . The beam-postgres  library provides several types and functions to make working\nwith these easier.", 
            "title": "Postgres-specific data types"
        }, 
        {
            "location": "/user-guide/backends/beam-postgres/#the-tsvector-and-tsquery-types", 
            "text": "The  tsvector  and  tsquery  types form the basis of full-text search in\nPostgres.", 
            "title": "The tsvector and tsquery types"
        }, 
        {
            "location": "/user-guide/backends/beam-sqlite/", 
            "text": "SQLite is a lightweight RDBMS meant for embedding in larger applications.\nBecause it is not designed to be full-featured, not all Beam queries will work\nwith SQLite. The module \nDatabase.Beam.SQLite.Checked\n provides many symbols\nusually imported from the \nDatabase.Beam\n module that enforce extra checks on\nqueries to assure compliance with SQLite. Use this module in code that is SQLite\nspecific for maximal compile-time safety. Note that this module should be\nimported instead of \nDatabase.Beam\n to avoid name clashes.\n\n\nCompatibility\n\n\nSQLite is compatible enough with Beam's query syntax, that adapting to its\nquirks is pretty straightforwards. The main special case for SQLite is its\nhandling of nested set operations. On most backends, beam can output these\ndirectly, but SQLite requires us to generate subqueries.", 
            "title": "beam-sqlite"
        }, 
        {
            "location": "/user-guide/backends/beam-sqlite/#compatibility", 
            "text": "SQLite is compatible enough with Beam's query syntax, that adapting to its\nquirks is pretty straightforwards. The main special case for SQLite is its\nhandling of nested set operations. On most backends, beam can output these\ndirectly, but SQLite requires us to generate subqueries.", 
            "title": "Compatibility"
        }, 
        {
            "location": "/user-guide/custom-backends/", 
            "text": "Writing a custom backend", 
            "title": "Writing a Custom Backend"
        }, 
        {
            "location": "/reference/models/", 
            "text": "A beam model is any single-constructer Haskell record type parameterized by a\ntype of kind \n* -\n *\n. The model must have an instance of \nGeneric\n, \nBeamable\n,\nand \nTable\n. \nGeneric\n can be derived using the \nDeriveGeneric\n extension of\nGHC. \nBeamable\n must be given an empty instance declaration (\ninstance Beamable\nTbl\n for a table of type \nTbl\n). \nTable\n is discussed next.\n\n\nEach field in the record type must either be a sub-table (another parameterized\ntype with a \nBeamable\n instance) or an explicit column. A column is specified\nusing the \nColumnar\n type family applied to the type's parameter and the\nunderlying Haskell type of the field.\n\n\nThe \nTable\n type class\n\n\nTable\n is a type class that must be instantiated for all types that you would\nlike to use as a table. It has one associated \ndata\n instance and one function.\n\n\nYou must create a type to represent the primary key of the table. The primary\nkey of a table \nTbl\n is the associated data type \nPrimaryKey Tbl\n. Like \nTbl\n,\nit takes one type parameter of kind \n* -\n *\n. It must have only one constructor\nwhich can hold all fields in the primary key. The constructor need not be a\nrecord constructor (although it can be).\n\n\nYou must also write a function \nprimaryKey\n that takes an instance of \nTbl\n\n(parameterized over any functor \nf\n) and returns the associated \nPrimaryKey\n\ntype. It is sometimes easiest to use the \nApplicative\n instance for \nr -\n to\nwrite this function. For example, if \ntblField1\n and \ntblField2\n are part of the\nprimary key, you can write\n\n\ninstance Table Tbl where\n  data PrimaryKey Tbl f = TblKey (Columnar f ..) (Columnar f ..)\n  primaryKey t = TblKey (tblField1 t) (tblField2 t)\n\n\n\n\nmore simply as\n\n\ninstance Table Tbl where\n  data PrimaryKey Tbl f = TblKey (Columnar f ..) (Columnar f ..)\n  primaryKey = TblKey \n$\n tblField1 \n*\n tblField2\n\n\n\n\nThe \nIdentity\n trick\n\n\nBeam table types are commonly prefixed by a \nT\n to indicate the name of the\ngeneric table type. Usually, a type synonym named by leaving out the \nT\n is\ndefined by applying the table to \nIdentity\n. Recall each field in the table is\neither another table or an application of \nColumnar\n to the type parameter. When\nthe type is parameterized by \nIdentity\n, every column is also parameterized by\n\nIdentity\n.\n\n\nColumnar\n is a type family defined such that \nColumnar Identity x ~ x\n. Thus,\nwhen parameterized over \nIdentity\n, every field in the table type takes on the\nunderlying Haskell type.\n\n\nSuppose you have a table type \nModelT\n and a type synonym \ntype Model = ModelT\nIdentity\n. Notice that deriving \nShow\n, \nEq\n, and other standard Haskell type\nclasses won't generally work for \nModelT\n. However, you can use the standalone\nderiving mechanism to derive these instances for \nModel\n.\n\n\ndata ModelT f = Model { .. } deriving Generic\ninstance Beamable ModelT\n\n-- deriving instance Show (ModelT f) -- Won't work because GHC won't get the constraints right\n\ntype Model = ModelT Identity\nderiving instance Show Model\nderiving instance Eq Model\nderiving instance Ord Model\n\n\n\n\nAllowed data types\n\n\nAny data type can be used within a \nColumnar\n. Beam does no checking that a\nfield can be used against a particular database when the data type is defined.\nInstead, type errors will occur when the table is being used as a query. For\nexample, the following is allowed, even though many backends will not work with\narray data types.\n\n\nimport qualified Data.Vector as V\n\ndata ArrayTable f\n    = ArrayTable\n    { arrayTablePoints :: Columnar f (V.Vector Int32)\n    } deriving Generic\n\n\n\n\nYou can construct values of type \nArrayTable Identity\n and even write queries\nover it (relying on type inference to get the constraints right). However, if\nyou attempt to solve the constraints over a database that doesn't support\ncolumns of type \nV.Vector Int32\n, GHC will throw an error. Thus, it's important\nto understand the limits of your backend when deciding which types to use. In\ngeneral, numeric, floating-point, and text types are well supported.\n\n\nMaybe\n types\n\n\nOptional fields (those that allow a SQL \nNULL\n) can usually be given a \nMaybe\n\ntype. However, you cannot use \nMaybe\n around an embedded table (you will be\nunable to instantiate \nBeamable\n).\n\n\nBeam offers a way around this. Instead of embedding the table applied to the\ntype parameter \nf\n, apply it to \nNullable f\n. \nColumnar (Nullable f) a ~ Maybe\n(Columnar f a)\n for all \na\n. Thus, this will make every column in the embedded\ntable take on the corresponding \nMaybe\n type.\n\n\n\n\nWarning\n\n\nNullable\n will nest \nMaybe\ns. That is \nColumnar (Nullable f) (Maybe a) ~\nMaybe (Maybe a)\n. This is bad from a SQL perspective, since SQL has no\nconcept of a nested optional type. Beam treats a \nNothing\n at any 'layer' of\nthe \nMaybe\n stack as a corresponding SQL \nNULL\n. When marshalling data back,\na SQL \nNULL\n is read in as a top-level \nNothing\n.\n\n\nThe reasons for this misfeature is basically code simplicity. Fixing this is\na top priority of future versions of beam.\n\n\n\n\nColumn tags\n\n\nAbove, we saw that applying \nIdentity\n to a table type results in a type whose\ncolumns are the underlying Haskell type. Beam uses other column tags for\nquerying and describing databases. Below is a table of common column tags and\ntheir meaning.\n\n\nConverting between tags\n\n\nSuppose you have a \nBeamable\n type paramaterized over a tag \nf\n and needed one\nparameterized over a tag \ng\n. Given a function \nconv :: forall a. Columnar f a\n-\n Columnar g a\n, you can use \nchangeBeamRep\n to convert between the tables.\n\n\nThere is one caveat however -- since \nColumnar\n is a type family, the type of\n\nconv\n is actually ambiguous. We need a way to carry the type of \nf\n, \ng\n, and\n\na\n into the code. For this reason, \nconv\n must actually be written over the\n\nColumnar'\n(notice the tick) \nnewtype\n. \nColumnar'\n is a newtype defined as such\n\n\nnewtype Columnar' f a = Columnar' (Columnar f a)\n\n\n\n\nNotice that, unlinke \nColumnar\n (a non-injective type family), \nColumnar'\n is a\nfull type. The type of \nconv' :: forall a. Columnar' f a -\n Columnar' g a\n is\nnow unambiguous. You can easily use \nconv\n to implement \nconv'\n:\n\n\nconv' (Columnar' a) = Columnar' (conv a)\n\n\n\n\nYou will often need to write explicit type signatures in order to get the\ncompiler to accept your code.", 
            "title": "Models"
        }, 
        {
            "location": "/reference/models/#the-table-type-class", 
            "text": "Table  is a type class that must be instantiated for all types that you would\nlike to use as a table. It has one associated  data  instance and one function.  You must create a type to represent the primary key of the table. The primary\nkey of a table  Tbl  is the associated data type  PrimaryKey Tbl . Like  Tbl ,\nit takes one type parameter of kind  * -  * . It must have only one constructor\nwhich can hold all fields in the primary key. The constructor need not be a\nrecord constructor (although it can be).  You must also write a function  primaryKey  that takes an instance of  Tbl \n(parameterized over any functor  f ) and returns the associated  PrimaryKey \ntype. It is sometimes easiest to use the  Applicative  instance for  r -  to\nwrite this function. For example, if  tblField1  and  tblField2  are part of the\nprimary key, you can write  instance Table Tbl where\n  data PrimaryKey Tbl f = TblKey (Columnar f ..) (Columnar f ..)\n  primaryKey t = TblKey (tblField1 t) (tblField2 t)  more simply as  instance Table Tbl where\n  data PrimaryKey Tbl f = TblKey (Columnar f ..) (Columnar f ..)\n  primaryKey = TblKey  $  tblField1  *  tblField2", 
            "title": "The Table type class"
        }, 
        {
            "location": "/reference/models/#the-identity-trick", 
            "text": "Beam table types are commonly prefixed by a  T  to indicate the name of the\ngeneric table type. Usually, a type synonym named by leaving out the  T  is\ndefined by applying the table to  Identity . Recall each field in the table is\neither another table or an application of  Columnar  to the type parameter. When\nthe type is parameterized by  Identity , every column is also parameterized by Identity .  Columnar  is a type family defined such that  Columnar Identity x ~ x . Thus,\nwhen parameterized over  Identity , every field in the table type takes on the\nunderlying Haskell type.  Suppose you have a table type  ModelT  and a type synonym  type Model = ModelT\nIdentity . Notice that deriving  Show ,  Eq , and other standard Haskell type\nclasses won't generally work for  ModelT . However, you can use the standalone\nderiving mechanism to derive these instances for  Model .  data ModelT f = Model { .. } deriving Generic\ninstance Beamable ModelT\n\n-- deriving instance Show (ModelT f) -- Won't work because GHC won't get the constraints right\n\ntype Model = ModelT Identity\nderiving instance Show Model\nderiving instance Eq Model\nderiving instance Ord Model", 
            "title": "The Identity trick"
        }, 
        {
            "location": "/reference/models/#allowed-data-types", 
            "text": "Any data type can be used within a  Columnar . Beam does no checking that a\nfield can be used against a particular database when the data type is defined.\nInstead, type errors will occur when the table is being used as a query. For\nexample, the following is allowed, even though many backends will not work with\narray data types.  import qualified Data.Vector as V\n\ndata ArrayTable f\n    = ArrayTable\n    { arrayTablePoints :: Columnar f (V.Vector Int32)\n    } deriving Generic  You can construct values of type  ArrayTable Identity  and even write queries\nover it (relying on type inference to get the constraints right). However, if\nyou attempt to solve the constraints over a database that doesn't support\ncolumns of type  V.Vector Int32 , GHC will throw an error. Thus, it's important\nto understand the limits of your backend when deciding which types to use. In\ngeneral, numeric, floating-point, and text types are well supported.", 
            "title": "Allowed data types"
        }, 
        {
            "location": "/reference/models/#maybe-types", 
            "text": "Optional fields (those that allow a SQL  NULL ) can usually be given a  Maybe \ntype. However, you cannot use  Maybe  around an embedded table (you will be\nunable to instantiate  Beamable ).  Beam offers a way around this. Instead of embedding the table applied to the\ntype parameter  f , apply it to  Nullable f .  Columnar (Nullable f) a ~ Maybe\n(Columnar f a)  for all  a . Thus, this will make every column in the embedded\ntable take on the corresponding  Maybe  type.   Warning  Nullable  will nest  Maybe s. That is  Columnar (Nullable f) (Maybe a) ~\nMaybe (Maybe a) . This is bad from a SQL perspective, since SQL has no\nconcept of a nested optional type. Beam treats a  Nothing  at any 'layer' of\nthe  Maybe  stack as a corresponding SQL  NULL . When marshalling data back,\na SQL  NULL  is read in as a top-level  Nothing .  The reasons for this misfeature is basically code simplicity. Fixing this is\na top priority of future versions of beam.", 
            "title": "Maybe types"
        }, 
        {
            "location": "/reference/models/#column-tags", 
            "text": "Above, we saw that applying  Identity  to a table type results in a type whose\ncolumns are the underlying Haskell type. Beam uses other column tags for\nquerying and describing databases. Below is a table of common column tags and\ntheir meaning.", 
            "title": "Column tags"
        }, 
        {
            "location": "/reference/models/#converting-between-tags", 
            "text": "Suppose you have a  Beamable  type paramaterized over a tag  f  and needed one\nparameterized over a tag  g . Given a function  conv :: forall a. Columnar f a\n-  Columnar g a , you can use  changeBeamRep  to convert between the tables.  There is one caveat however -- since  Columnar  is a type family, the type of conv  is actually ambiguous. We need a way to carry the type of  f ,  g , and a  into the code. For this reason,  conv  must actually be written over the Columnar' (notice the tick)  newtype .  Columnar'  is a newtype defined as such  newtype Columnar' f a = Columnar' (Columnar f a)  Notice that, unlinke  Columnar  (a non-injective type family),  Columnar'  is a\nfull type. The type of  conv' :: forall a. Columnar' f a -  Columnar' g a  is\nnow unambiguous. You can easily use  conv  to implement  conv' :  conv' (Columnar' a) = Columnar' (conv a)  You will often need to write explicit type signatures in order to get the\ncompiler to accept your code.", 
            "title": "Converting between tags"
        }, 
        {
            "location": "/reference/expression/", 
            "text": "Typing\n\n\nThe type of all SQL-level expressions is \nQGenExpr\n. See the \nquery tutorial\n for more information.\n\n\nIn many cases, you'd like to type the SQL-level result of an expression without\nhaving to give explicit types for the other \nQGenExpr\n parameters. You can do\nthis with the \nas_\n combinator and \n-XTypeApplications\n.\n\n\nThe following code types the literal 1 as a \nDouble\n.\n\n\nas_ @Double 1\n\n\n\n\nThis is rarely needed, but there are a few cases where the beam types are too\ngeneral for the compiler to meaningfully infer types.\n\n\nLiterals\n\n\n\n\nInteger literals\n can be constructed using \nfromIntegral\n in the \nNum\n\n  typeclass. This means you can also just use a Haskell integer literal as a\n  \nQGenExpr\n in any context.\n\n\nRational literals\n can be constructed via \nfromRational\n in \nRational\n.\n  Regular Haskell rational literals will be automatically converted to\n  \nQGenExprs\n.\n\n\nText literals\n can be constructed via \nfromString\n in \nIsString\n. Again,\n  Haskell string constants will automaticall be converted to \nQGenExprs\n,\n  although you may have to provide an explicit type, as different backends\n  support different text types natively.\n\n\nAll other literals\n can be constructed using the \nval_\n function in\n  \nSqlValable\n. This requires that there is an implementation of\n  \nHasSqlValueSyntax (Sql92ExpressionValueSyntax syntax) x\n for the type \nx\n in\n  the appropriate \nsyntax\n for the \nQGenExpr\n. For example, to construct a value\n  of type \nVector Int\n in the \nbeam-postgres\n backend.\n\n\n\n\nval_ (V.fromList [1, 2, 3 :: Int])\n\n\n\n\n\n\nExplicit tables\n can be brought to the SQL value level by using \nval_\n as\n  well. For example, if you have an \nAddressT Identity\n named \na\n, \nval_ a ::\n  AddressT (QGenExpr context expr s)\n.\n\n\n\n\nArithmetic\n\n\nArithmetic operations that are part of the \nFractional\n and \nNum\n classes can be\nused directly. For example, if \na\n and \nb\n are \nQGenExpr\ns of the same type,\nthen \na + b\n is a \nQGenExpr\n of the same type.\n\n\nBecause of the \ntoInteger\n class method in \nIntegral\n, \nQGenExpr\ns cannot\nimplement \nIntegral\n. Nevertheless, versions of \ndiv\n and \nmod\n are available as\n\ndiv_\n and \nmod_\n, respectively, having the corresponding type.\n\n\nCASE .. WHEN .. ELSE ..\n statements\n\n\nThe SQL \nCASE .. WHEN .. ELSE\n construct can be used to implement a multi-way\nif. The corresponding beam syntax is\n\n\nif_ [ cond1 `then_` result1, cond2 `then_` result2, ... ] (else_ elseResult)\n\n\n\n\nwhere \ncond\nn\n are \nQGenExpr\n of type \nBool\n, and \nresult1\n, \nresult2\n, and\n\nelseResult\n are \nQGenExprs\n of the same type.\n\n\nSQL Functions and operators\n\n\n\n\n\n\n\n\nSQL construct\n\n\nSQL standard\n\n\nBeam equivalent\n\n\nNotes\n\n\n\n\n\n\n\n\n\n\nEXISTS (x)\n\n\nSQL92\n\n\nexists_ x\n\n\nHere, \nx\n is any query (of type \nQ\n)\n\n\n\n\n\n\nUNIQUE (x)\n\n\nSQL92\n\n\nunique_ x\n\n\nSee note for \nEXISTS (x)\n\n\n\n\n\n\nDISTINCT (x)\n\n\nSQL99\n\n\ndistinct_ x\n\n\nSee note for \nEXISTS (x)\n\n\n\n\n\n\nSELECT .. FROM ...\n \n as an expression (subqueries)\n\n\nSQL92\n\n\nsubquery_ x\n\n\nx\n is an query (of type \nQ\n)\n\n\n\n\n\n\nCOALESCE(a, b, c, ...)\n\n\nSQL92\n\n\ncoalesce_ [a, b, c, ...]\n\n\na\n, \nb\n, and \nc\n must be of \ntype \nMaybe a\n.\nThe result has type \na\n\n\n\n\n\n\na BETWEEN b AND c\n\n\nSQL92\n\n\nbetween_ a b c\n\n\n\n\n\n\n\n\na LIKE b\n\n\nSQL92\n\n\na `like_` b\n\n\na\n and \nb\n should be string types\n\n\n\n\n\n\na SIMILAR TO b\n\n\nSQL99\n\n\na `similarTo_` b\n\n\nSee note for \nLIKE\n\n\n\n\n\n\nPOSITION(x IN y)\n\n\nSQL92\n\n\nposition_ x y\n\n\nx\n and \ny\n should be string types\n\n\n\n\n\n\nCHAR_LENGTH(x)\n\n\nSQL92\n\n\ncharLength_ x\n\n\n\n\n\n\n\n\nOCTET_LENGTH(x)\n\n\nSQL92\n\n\noctetLength_ x\n\n\n\n\n\n\n\n\nBIT_LENGTH(x)\n\n\nSQL92\n\n\nbitLength_ x\n\n\nx\n must be of the beam-specific \nSqlBitString\n type\n\n\n\n\n\n\nx IS TRUE\n / \nx IS NOT TRUE\n\n\nSQL92\n\n\nisTrue_ x\n / \nisNotTrue_ x\n\n\n\n\n\n\n\n\nx IS FALSE\n / \nx IS NOT FALSE\n\n\nSQL92\n\n\nisFalse_ x\n / \nisNotFalse_ x\n\n\n\n\n\n\n\n\nx IS UNKNOWN\n / \nx IS NOT UNKNOWN\n\n\nSQL92\n\n\nisUnknown_ x\n / \nisNotUnknown_ x\n\n\n\n\n\n\n\n\nNOT x\n\n\nSQL92\n\n\nnot_ x\n\n\n\n\n\n\n\n\n\n\nMy favorite operator / function isn't listed here!\n\n\nIf your favorite operator or function is not provided here, first ask yourself\nif it is part of any SQL standard. If it is not, then check the backend you are\nusing to see if it provides a corresponding construct. If the backend does not\nor if the function / operator you need is part of a SQL standard, please open an\nissue on GitHub. Alternatively, implement the construct yourself and send us a\npull request! See the section on \nadding your own functions", 
            "title": "Expressions"
        }, 
        {
            "location": "/reference/expression/#typing", 
            "text": "The type of all SQL-level expressions is  QGenExpr . See the  query tutorial  for more information.  In many cases, you'd like to type the SQL-level result of an expression without\nhaving to give explicit types for the other  QGenExpr  parameters. You can do\nthis with the  as_  combinator and  -XTypeApplications .  The following code types the literal 1 as a  Double .  as_ @Double 1  This is rarely needed, but there are a few cases where the beam types are too\ngeneral for the compiler to meaningfully infer types.", 
            "title": "Typing"
        }, 
        {
            "location": "/reference/expression/#literals", 
            "text": "Integer literals  can be constructed using  fromIntegral  in the  Num \n  typeclass. This means you can also just use a Haskell integer literal as a\n   QGenExpr  in any context.  Rational literals  can be constructed via  fromRational  in  Rational .\n  Regular Haskell rational literals will be automatically converted to\n   QGenExprs .  Text literals  can be constructed via  fromString  in  IsString . Again,\n  Haskell string constants will automaticall be converted to  QGenExprs ,\n  although you may have to provide an explicit type, as different backends\n  support different text types natively.  All other literals  can be constructed using the  val_  function in\n   SqlValable . This requires that there is an implementation of\n   HasSqlValueSyntax (Sql92ExpressionValueSyntax syntax) x  for the type  x  in\n  the appropriate  syntax  for the  QGenExpr . For example, to construct a value\n  of type  Vector Int  in the  beam-postgres  backend.   val_ (V.fromList [1, 2, 3 :: Int])   Explicit tables  can be brought to the SQL value level by using  val_  as\n  well. For example, if you have an  AddressT Identity  named  a ,  val_ a ::\n  AddressT (QGenExpr context expr s) .", 
            "title": "Literals"
        }, 
        {
            "location": "/reference/expression/#arithmetic", 
            "text": "Arithmetic operations that are part of the  Fractional  and  Num  classes can be\nused directly. For example, if  a  and  b  are  QGenExpr s of the same type,\nthen  a + b  is a  QGenExpr  of the same type.  Because of the  toInteger  class method in  Integral ,  QGenExpr s cannot\nimplement  Integral . Nevertheless, versions of  div  and  mod  are available as div_  and  mod_ , respectively, having the corresponding type.", 
            "title": "Arithmetic"
        }, 
        {
            "location": "/reference/expression/#case-when-else-statements", 
            "text": "The SQL  CASE .. WHEN .. ELSE  construct can be used to implement a multi-way\nif. The corresponding beam syntax is  if_ [ cond1 `then_` result1, cond2 `then_` result2, ... ] (else_ elseResult)  where  cond n  are  QGenExpr  of type  Bool , and  result1 ,  result2 , and elseResult  are  QGenExprs  of the same type.", 
            "title": "CASE .. WHEN .. ELSE .. statements"
        }, 
        {
            "location": "/reference/expression/#sql-functions-and-operators", 
            "text": "SQL construct  SQL standard  Beam equivalent  Notes      EXISTS (x)  SQL92  exists_ x  Here,  x  is any query (of type  Q )    UNIQUE (x)  SQL92  unique_ x  See note for  EXISTS (x)    DISTINCT (x)  SQL99  distinct_ x  See note for  EXISTS (x)    SELECT .. FROM ...    as an expression (subqueries)  SQL92  subquery_ x  x  is an query (of type  Q )    COALESCE(a, b, c, ...)  SQL92  coalesce_ [a, b, c, ...]  a ,  b , and  c  must be of  type  Maybe a . The result has type  a    a BETWEEN b AND c  SQL92  between_ a b c     a LIKE b  SQL92  a `like_` b  a  and  b  should be string types    a SIMILAR TO b  SQL99  a `similarTo_` b  See note for  LIKE    POSITION(x IN y)  SQL92  position_ x y  x  and  y  should be string types    CHAR_LENGTH(x)  SQL92  charLength_ x     OCTET_LENGTH(x)  SQL92  octetLength_ x     BIT_LENGTH(x)  SQL92  bitLength_ x  x  must be of the beam-specific  SqlBitString  type    x IS TRUE  /  x IS NOT TRUE  SQL92  isTrue_ x  /  isNotTrue_ x     x IS FALSE  /  x IS NOT FALSE  SQL92  isFalse_ x  /  isNotFalse_ x     x IS UNKNOWN  /  x IS NOT UNKNOWN  SQL92  isUnknown_ x  /  isNotUnknown_ x     NOT x  SQL92  not_ x", 
            "title": "SQL Functions and operators"
        }, 
        {
            "location": "/reference/expression/#my-favorite-operator-function-isnt-listed-here", 
            "text": "If your favorite operator or function is not provided here, first ask yourself\nif it is part of any SQL standard. If it is not, then check the backend you are\nusing to see if it provides a corresponding construct. If the backend does not\nor if the function / operator you need is part of a SQL standard, please open an\nissue on GitHub. Alternatively, implement the construct yourself and send us a\npull request! See the section on  adding your own functions", 
            "title": "My favorite operator / function isn't listed here!"
        }, 
        {
            "location": "/reference/queries/", 
            "text": "Literals", 
            "title": "Queries"
        }, 
        {
            "location": "/reference/queries/#literals", 
            "text": "", 
            "title": "Literals"
        }, 
        {
            "location": "/reference/extensibility/", 
            "text": "The \nbeam-core\n library and respective backends strive to expose the full power\nof each underlying database. If a particular feature is missing, please feel\nfree to file a bug report on the GitHub issue tracker.\n\n\nHowever, in the meantime, beam offers a few options to inject raw SQL into your\nqueries. Of course, beam cannot predict types of expressions and queries that\nwere not created with its combinators, so \ncaveat emptor\n.\n\n\nCustom expressions\n\n\nIf you'd like to write an expression that beam currently does not support, you\ncan use the \ncustomExpr_\n function. Your backend's syntax must implement the\n\nIsSqlCustomExpressionSyntax\n type class. \ncustomExpr_\n takes a function of\narity \nn\n and \nn\n arguments, which must all be \nQGenExpr\ns with the same thread\nparameter. The expressions may be from different contexts (i.e., you can pass an\naggregate and scalar into the same \ncustomExpr_\n).\n\n\nThe function supplied must return a \nByteString\n corresponding to the custom bit\nof SQL that implements your expression. The function's arguments are \nn\n\n\nByteString\ns corresponding to the expressions which will evaluate to the value\nof each of the arguments to \ncustomExpr_\n. The arguments are properly\nparenthesized and can be inserted whole into the final expression. You will\nlikely need to explicitly supply a result type using the \nas_\n function.\n\n\nFor example, below, we use \ncustomExpr_\n to access the \nregr_intercept\n and\n\nregr_slope\n functions in postgres.\n\n\n\n                \n\n                    \n        \n\n            \n        \n\n            \nHaskell\n\n        \n\n    \n        \n\n            \nPostgres\n\n        \n\n    \n        \n\n    \n        \n\n            \n        \n\n            \naggregate_ (\\t -\n ( as_ @Double @QAggregateContext $ customExpr_ (\\bytes ms -\n \nregr_intercept(\n \n bytes \n \n, \n \n ms \n \n)\n) (trackBytes t) (trackMilliseconds t)\n                  , as_ @Double @QAggregateContext $ customExpr_ (\\bytes ms -\n \nregr_slope(\n \n bytes \n \n, \n \n ms \n \n)\n) (trackBytes t) (trackMilliseconds t) )) $\nall_ (track chinookDb)\n\n\n        \n\n    \n        \n\n            \nSELECT regr_intercept((\nt0\n.\nBytes\n), (\nt0\n.\nMilliseconds\n)) AS \nres0\n,\n       regr_slope((\nt0\n.\nBytes\n), (\nt0\n.\nMilliseconds\n)) AS \nres1\n\nFROM \nTrack\n AS \nt0\n\n\n\n        \n\n    \n        \n\n    \n                \n\n                    \n\n\nOf course, this requires that the expression is easily expressible as a\n\nByteString\n.\n\n\nCustom queries\n\n\nSometimes you would like to drop down to raw SQL to write a query that will\nreturn an entire result. Beam supports this through the \ncustomQuery_\n function.\nLike \ncustomExpr_\n, this takes a function of \nn\n arity and \nn\n arguments, which\nmay be either \nQGenExpr\ns or \nQ\ns from the same thread, select syntax, etc. The\nfunction supplied to \ncustomQuery_\n must return a \nByteString\n and its arguments\nare \nByteString\ns corresponding to the given \nQ\n or \nQGenExpr\n parameter.", 
            "title": "Extending"
        }, 
        {
            "location": "/reference/extensibility/#custom-expressions", 
            "text": "If you'd like to write an expression that beam currently does not support, you\ncan use the  customExpr_  function. Your backend's syntax must implement the IsSqlCustomExpressionSyntax  type class.  customExpr_  takes a function of\narity  n  and  n  arguments, which must all be  QGenExpr s with the same thread\nparameter. The expressions may be from different contexts (i.e., you can pass an\naggregate and scalar into the same  customExpr_ ).  The function supplied must return a  ByteString  corresponding to the custom bit\nof SQL that implements your expression. The function's arguments are  n  ByteString s corresponding to the expressions which will evaluate to the value\nof each of the arguments to  customExpr_ . The arguments are properly\nparenthesized and can be inserted whole into the final expression. You will\nlikely need to explicitly supply a result type using the  as_  function.  For example, below, we use  customExpr_  to access the  regr_intercept  and regr_slope  functions in postgres.  \n                 \n                    \n         \n            \n         \n             Haskell \n         \n    \n         \n             Postgres \n         \n    \n         \n    \n         \n            \n         \n             aggregate_ (\\t -  ( as_ @Double @QAggregateContext $ customExpr_ (\\bytes ms -   regr_intercept(    bytes    ,     ms    ) ) (trackBytes t) (trackMilliseconds t)\n                  , as_ @Double @QAggregateContext $ customExpr_ (\\bytes ms -   regr_slope(    bytes    ,     ms    ) ) (trackBytes t) (trackMilliseconds t) )) $\nall_ (track chinookDb) \n         \n    \n         \n             SELECT regr_intercept(( t0 . Bytes ), ( t0 . Milliseconds )) AS  res0 ,\n       regr_slope(( t0 . Bytes ), ( t0 . Milliseconds )) AS  res1 \nFROM  Track  AS  t0  \n         \n    \n         \n    \n                 \n                      Of course, this requires that the expression is easily expressible as a ByteString .", 
            "title": "Custom expressions"
        }, 
        {
            "location": "/reference/extensibility/#custom-queries", 
            "text": "Sometimes you would like to drop down to raw SQL to write a query that will\nreturn an entire result. Beam supports this through the  customQuery_  function.\nLike  customExpr_ , this takes a function of  n  arity and  n  arguments, which\nmay be either  QGenExpr s or  Q s from the same thread, select syntax, etc. The\nfunction supplied to  customQuery_  must return a  ByteString  and its arguments\nare  ByteString s corresponding to the given  Q  or  QGenExpr  parameter.", 
            "title": "Custom queries"
        }, 
        {
            "location": "/about/compatibility/", 
            "text": "Beam strives to cover the full breadth of the relevant SQL\nstandards. In general, if there is something in a SQL standard that is\nnot implemented in a generic manner in \nbeam-core\n, feel free to file\nan issue requesting support. There are some features that beam\npurposefully omits because no major RDBMS implements them. For\nexample, database-level assertions are not supported in any of the\ndefault beam backends, and thus are not supported by \nbeam-core\n. If\nyou have a need for these features, feel free to file an issue. Be\nsure to motivate your use case with examples and a testing strategy.\n\n\nThe relevant SQL standards are SQL-92, SQL:1999, SQL:2003, SQL:2008,\nand SQL:2011. Because not all the standards are not publicly\naccessible, I've done my best to piece together features from various\ndocuments available online. I believe I've covered most of the common\ncases, but there may be pieces of functionality that are missing. File\nan issue if this is the case.\n\n\nThe table below summarizes the features defined in each SQL standard and beam's\nsupport for them. FULL means beam supports everything in that feature. NONE\nmeans that there is no support for that feature, and none planned. N/A means\nthat the feature only applies to RDBMSs, not the SQL language. WONTFIX means\nthat the feature has been considered and willfully ignored. UNKNOWN means not\nenough investigation has gone into the feature to make a determination. TODO\nmeans the feature has not been implemented yet, but an implementation is\nplanned.\n\n\n\n\nTip\n\n\nThe 'TODO' items are a great way to contribute to beam!\n\n\n\n\n\n\n\n\n\n\nFeature\n\n\nStatus\n\n\nNotes\n\n\n\n\n\n\n\n\n\n\nB011 Embedded Ada\n\n\nNONE\n\n\n\n\n\n\n\n\nB012 Embedded C\n\n\nNONE\n\n\n\n\n\n\n\n\nB013 Embedded COBOL\n\n\nNONE\n\n\n\n\n\n\n\n\nB014 Embedded FORTRAN\n\n\nNONE\n\n\n\n\n\n\n\n\nB015 Embedded MUMPS\n\n\nNONE\n\n\n\n\n\n\n\n\nB016 Embedded Pascal\n\n\nNONE\n\n\n\n\n\n\n\n\nB017 Embedded PL/I\n\n\nNONE\n\n\n\n\n\n\n\n\nB021 Direct SQL\n\n\nNONE\n\n\n\n\n\n\n\n\nB031 Basic dynamic SQL\n\n\nNONE\n\n\n\n\n\n\n\n\nB032 Extended dynamic SQL\n\n\nNONE\n\n\n\n\n\n\n\n\nB033 Untyped SQL-invoked function arguments\n\n\nNONE\n\n\n\n\n\n\n\n\nB034 Dynamic specification of cursor attributes\n\n\nNONE\n\n\n\n\n\n\n\n\nB035 Non-extended descriptor names\n\n\nNONE\n\n\n\n\n\n\n\n\nB051 Enhanced execution rights\n\n\nNONE\n\n\n\n\n\n\n\n\nB111 Module language Ada\n\n\nNONE\n\n\n\n\n\n\n\n\nB112 Module language C\n\n\nNONE\n\n\n\n\n\n\n\n\nB113 Module language COBOL\n\n\nNONE\n\n\n\n\n\n\n\n\nB114 Module language Fortran\n\n\nNONE\n\n\n\n\n\n\n\n\nB115 Module language MUMPS\n\n\nNONE\n\n\n\n\n\n\n\n\nB116 Module language Pascal\n\n\nNONE\n\n\n\n\n\n\n\n\nB117 Module language PL/I\n\n\nNONE\n\n\n\n\n\n\n\n\nB121 Routine language Ada\n\n\nNONE\n\n\n\n\n\n\n\n\nB122 Routine language C\n\n\nNONE\n\n\n\n\n\n\n\n\nB123 Routine language COBOL\n\n\nNONE\n\n\n\n\n\n\n\n\nB124 Routine language Fortran\n\n\nNONE\n\n\n\n\n\n\n\n\nB125 Routine language MUMPS\n\n\nNONE\n\n\n\n\n\n\n\n\nB126 Routine language Pascal\n\n\nNONE\n\n\n\n\n\n\n\n\nB127 Routine language PL/I\n\n\nNONE\n\n\n\n\n\n\n\n\nB128 Routine language SQL\n\n\nNONE\n\n\n\n\n\n\n\n\nB211 Module language Ada: VARCHAR and NUMERIC support\n\n\nNONE\n\n\n\n\n\n\n\n\nB221 Routine language Ada: VARCHAR and NUMERIC support\n\n\nNONE\n\n\n\n\n\n\n\n\nE011 - Numeric data types\n\n\n\n\n\n\n\n\n\n\nE011-01 INTEGER and SMALLINT data types\n\n\nFULL\n\n\nUse \nInt32\n for \nINTEGER\n, \nInt16\n for \nSMALLINT\n\n\n\n\n\n\nE011-02 REAL, DOUBLE PRECISION, FLOAT\n\n\nFULL\n\n\nUse \nDouble\n and \nFloat\n\n\n\n\n\n\nE011-03 DECIMAL and NUMERIC data types\n\n\nFULL\n\n\nUse \nScientific\n. You can provide the database precision using \nbeam-migrate\n\n\n\n\n\n\nE011-04 Arithmetic operators\n\n\nFULL\n\n\nUse the \nNum\n instance for \nQGenExpr\n\n\n\n\n\n\nE011-05 Numeric comparison\n\n\nFULL\n\n\nUse the \n.\n prefixed operators (i.e., \n==.\n, \n/=.\n, \n.\n, etc)\n\n\n\n\n\n\nE011-06 Implicit casting among numeric data types\n\n\nWONTFIX\n\n\nBeam never implicitly casts. Use \ncast_\n\n\n\n\n\n\nE021 Character string types\n\n\n\n\n\n\n\n\n\n\nE021-01 CHARACTER data type\n\n\nFULL\n\n\nUse \nText\n. Use \nbeam-migrate\n to specify width\n\n\n\n\n\n\nE021-02 CHARACTER VARYING data type\n\n\nFULL\n\n\nUse \nText\n. Use \nbeam-migrate\n to specify width.\n\n\n\n\n\n\nE021-03 Character literals\n\n\nFULL\n\n\nUse \nval_\n\n\n\n\n\n\nE021-04 CHARACTER_LENGTH function\n\n\nFULL\n\n\nUse \ncharLength_\n\n\n\n\n\n\nE021-05 OCTET_LENGTH function\n\n\nFULL\n\n\nUse \noctetLength_\n\n\n\n\n\n\nE021-06 SUBSTRING function\n\n\nTODO\n\n\n\n\n\n\n\n\nE021-07 Character concatenation\n\n\nTODO\n\n\nUse \n(\n#x007c;\n#0x007c).\n\n\n\n\n\n\nE021-08 UPPER and LOWER functions\n\n\nTODO\n\n\nUse \nupper_\n and \nlower_\n\n\n\n\n\n\nE021-09 TRIM function\n\n\nTODO\n\n\nUse \ntrim_\n\n\n\n\n\n\nE021-10 Implicit casting among string types\n\n\nWONTFIX\n\n\nBeam never implicitly casts. Use \ncast_\n\n\n\n\n\n\nE021-11 POSITION function\n\n\nFULL\n\n\nUse \nposition_\n\n\n\n\n\n\nE021-12 Character comparison\n\n\nFULL\n\n\nUse comparison operators (See E011-05)\n\n\n\n\n\n\nE031 Identifiers\n\n\n\n\n\n\n\n\n\n\nE031-01 Delimited identifiers\n\n\nTODO\n\n\nFind out more\n\n\n\n\n\n\nE021-02 Lower case identifiers\n\n\nTODO\n\n\n\n\n\n\n\n\nE021-03 Trailing underscore\n\n\nN/A\n\n\nBeam will use whatever column names you specify\n\n\n\n\n\n\nE051 Basic query specification\n\n\n\n\n\n\n\n\n\n\nE051-01 SELECT DISTINCT\n\n\nTODO\n\n\nUse \nselectDistinct_\n\n\n\n\n\n\nE051-02 GROUP BY clause\n\n\nFULL\n\n\nSee \naggregate_\n or read the \nsection on aggregates\n\n\n\n\n\n\nE051-04 GROUP BY can contain columns not in SELECT\n\n\nTODO\n\n\nUnsure how this applies to beam in particular\n\n\n\n\n\n\nE051-05 Select list items can be renamed\n\n\nN/A\n\n\nBeam uses this feature internally, the user never needs it\n\n\n\n\n\n\nE051-06 HAVING clause\n\n\nFULL\n\n\nguard_\n and \nfilter_\n are appropriately converted to \nHAVING\n when allowed\n\n\n\n\n\n\nE051-07 Qualified * in select list\n\n\nN/A\n\n\nBeam handles projections instead\n\n\n\n\n\n\nE051-08 Correlation names in FROM\n\n\nTODO\n\n\nUnsure how this applies to beam\n\n\n\n\n\n\nE051-09 Rename columns in the FROM clause\n\n\nNONE\n\n\nBeam doesn't need this\n\n\n\n\n\n\nE061 Basic predicates and search conditions\n\n\n\n\n\n\n\n\n\n\nE061-01 Comparison predicate\n\n\nFULL\n\n\nUse the comparison operators (see E011-05)\n\n\n\n\n\n\nE061-02 BETWEEN predicate\n\n\nFULL\n\n\nUse \nbetween_\n\n\n\n\n\n\nE061-03 IN predicate with list of values\n\n\nTODO\n\n\n\n\n\n\n\n\nE061-04 LIKE predicate\n\n\nFULL\n\n\nUse \nlike_\n\n\n\n\n\n\nE061-05 LIKE predicate ESCAPE clause\n\n\nTODO\n\n\nUnsure how this would apply\n\n\n\n\n\n\nE061-06 NULL predicate\n\n\nFULL\n\n\nUse \nisNull_\n and \nisNotNull_\n\n\n\n\n\n\nE061-07 Quantified comparison predicate\n\n\nPARTIAL\n\n\nSupported in syntaxes, not exposed in \nQGenExpr\n (TODO)\n\n\n\n\n\n\nE051-08 EXISTS predicate\n\n\nFULL\n\n\nUse \nexists_\n\n\n\n\n\n\nE061-09 Subqueries in comparison predicate\n\n\nFULL\n\n\nUse \nsubquery_\n as usual\n\n\n\n\n\n\nE061-11 Subqueries in IN predicate\n\n\nTODO\n\n\nWould be fixed by E061-03\n\n\n\n\n\n\nE061-12 Subqueries in quantified comparison predicate\n\n\nTODO\n\n\nWould be fixed by E061-07\n\n\n\n\n\n\nE061-13 Correlated subqueries\n\n\nFULL\n\n\nUse \nsubquery_\n\n\n\n\n\n\nE061-14 Search condition\n\n\nFULL\n\n\nConstruct \nQGenExprs\n with type \nBool\n\n\n\n\n\n\nE071 Basic query expressions\n\n\n\n\n\n\n\n\n\n\nE071-01 UNION DISTINCT table operator\n\n\nFULL\n\n\nUse \nunion_\n\n\n\n\n\n\nE071-02 UNION ALL table operator\n\n\nFULL\n\n\nUse \nunionAll_\n\n\n\n\n\n\nE071-03 EXCEPT DISTINCT table operator\n\n\nFULL\n\n\nUse \nexcept_\n\n\n\n\n\n\nE071-05 Columns combined via operators need not have same type\n\n\nWONTFIX\n\n\nBeam is strongly typed\n\n\n\n\n\n\nE071-06 Table operators in subqueries\n\n\nFULL\n\n\nSupported for backends that support it\n\n\n\n\n\n\nE081 Basic privileges\n\n\nNONE\n\n\nDatabase security is not beam's focus. \nbeam-migrate\n may expose this in the future\n\n\n\n\n\n\nE091 Set functions\n\n\n\n\n\n\n\n\n\n\nE091-01 AVG\n\n\nFULL\n\n\nUse \navg_\n or \navgOver_\n\n\n\n\n\n\nE091-02 COUNT\n\n\nFULL\n\n\nUse \ncountAll_\n, \ncountAllOver_\n, \ncount_\n, or \ncountOver_\n\n\n\n\n\n\nE091-03 MAX\n\n\nFULL\n\n\nUse \nmax_\n or \nmaxOver_\n\n\n\n\n\n\nE091-04 MIN\n\n\nFULL\n\n\nUse \nmin_\n or \nminOver_\n\n\n\n\n\n\nE091-05 SUM\n\n\nFULL\n\n\nUse \nsum_\n or \nsumOver_\n\n\n\n\n\n\nE091-06 ALL quantifier\n\n\nFULL\n\n\nUse the \n*Over_\n functions with the \nallInGroupExplicitly_\n quantifier\n\n\n\n\n\n\nE091-07 DISTINCT quantifier\n\n\nFULL\n\n\nUse the \n*Over_\n functions with the \ndistinctInGroup_\n quantifier\n\n\n\n\n\n\nE101 Basic data manipulation\n\n\n\n\n\n\n\n\n\n\nE101-01 INSERT statement\n\n\nFULL\n\n\nUse \ninsert\n and \nSqlInsert\n\n\n\n\n\n\nE101-03 Searched UPDATE\n\n\nFULL\n\n\nUse \nupdate\n and \nSqlUpdate\n\n\n\n\n\n\nE101-04 Searched DELETE\n\n\nFULL\n\n\nUse \ndelete\n and \nSqlDelete\n\n\n\n\n\n\nE111 Single row SELECT statement\n\n\nFULL\n\n\nUse \nselect\n as expected\n\n\n\n\n\n\nE121 Basic cursor support\n\n\nNONE\n\n\nUse the backends explicitly\n\n\n\n\n\n\nE131 Null value support\n\n\nPARTIAL\n\n\nUse \nMaybe\n column types, \nNullable\n, and the \njust_\n, \nnothing_\n, and \nmaybe_\n functions\n\n\n\n\n\n\nE141 Basic integrity constraints\n\n\n\n\nImplemented in \nbeam-migrate\n\n\n\n\n\n\nE141-01 NOT NULL constraints\n\n\nFULL\n\n\nUse \nnotNull_\n\n\n\n\n\n\nE141-02 UNIQUE constraints of NOT NULL columns\n\n\nTODO\n\n\n\n\n\n\n\n\nE141-03 PRIMARY KEY constraints\n\n\nFULL\n\n\nInstantiate \nTable\n with the correct \nPrimaryKey\n\n\n\n\n\n\nE141-04 Basic FOREIGN KEY constraints\n\n\nTODO\n\n\nYou can embed the \nPrimaryKey\n of the relation directly.\n\n\n\n\n\n\nE141-06 CHECK constraints\n\n\nTODO\n\n\n\n\n\n\n\n\nE141-07 Column defaults\n\n\nFULL\n\n\nUse \ndefault_\n from \nbeam-migrate\n\n\n\n\n\n\nE141-08 NOT NULL inferred on PRIMARY KEY\n\n\nN/A\n\n\n\n\n\n\n\n\nE141-10 Names in a foreign key can be specified in any order\n\n\nN/A\n\n\n\n\n\n\n\n\nE151 Transaction support\n\n\nNone\n\n\nUse the backend functions explicitly\n\n\n\n\n\n\nE152 SET TRANSACTION statement\n\n\nN/A\n\n\n\n\n\n\n\n\nE153 Updatable queries with subqueries\n\n\nTODO\n\n\n\n\n\n\n\n\nE161 SQL comments with double minus\n\n\nN/A\n\n\n\n\n\n\n\n\nE171 SQLSTATE support\n\n\nN/A\n\n\n\n\n\n\n\n\nE182 Host language binding\n\n\nN/A\n\n\n\n\n\n\n\n\nF031 Basic schema manipulation\n\n\n\n\n\n\n\n\n\n\nF031-01 CREATE TABLE for persistent base tables\n\n\nFULL\n\n\nUse \ncreateTable_\n in \nbeam-migrate\n\n\n\n\n\n\nF031-02 CREATE VIEW statement\n\n\nTODO\n\n\n\n\n\n\n\n\nF031-03 GRANT statement\n\n\nTODO\n\n\n\n\n\n\n\n\nF031-04 ALTER TABLE statement: ADD COLUMN clause\n\n\nTODO\n\n\n\n\n\n\n\n\nF031-13 DROP TABLE statement: RESTRICT clause\n\n\nTODO\n\n\n\n\n\n\n\n\nF031-16 DROP VIEW statement: RESTRICT clause\n\n\nTODO\n\n\n\n\n\n\n\n\nF031-19 REVOKE statement: RESTRICT clause\n\n\nNONE\n\n\nSee note for E081\n\n\n\n\n\n\nF032 CASCADE drop behavior\n\n\nTODO\n\n\nWould be in \nbeam-migrate\n\n\n\n\n\n\nF033 ALTER TABLE statement: DROP COLUMN clause\n\n\nTODO\n\n\n\n\n\n\n\n\nF034 Extended REVOKE statement\n\n\nNONE\n\n\n\n\n\n\n\n\nF041 Basic joined table\n\n\n\n\n\n\n\n\n\n\nF041-01 Inner join\n\n\nFULL\n\n\nUse the \nmonadic join interface\n\n\n\n\n\n\nF041-02 INNER keyword\n\n\nN/A\n\n\nNo semantic difference\n\n\n\n\n\n\nF041-03 LEFT OUTER JOIN\n\n\nFULL\n\n\nUse \nleftJoin_\n\n\n\n\n\n\nF041-04 RIGHT OUTER JOIN\n\n\nPARTIAL\n\n\nSupported in backend syntaxes, not exposed. Can always be written using LEFT OUTER JOIN\n\n\n\n\n\n\nF041-05 Outer joins can be nested\n\n\nTODO\n\n\nDepends on \nouterJoin_\n\n\n\n\n\n\nF041-07 The inner table in outer join can be used in inner join\n\n\nTODO\n\n\nHow does this apply to us?\n\n\n\n\n\n\nF041-08 All comparison operators in JOIN\n\n\nFULL\n\n\nArbitrary \nQGenExpr\ns are supported.\n\n\n\n\n\n\nF051 Basic date and time\n\n\n\n\n\n\n\n\n\n\nF051-01 DATE data type\n\n\nFULL\n\n\nUse \nDay\n from \nData.Time\n and \nval_\n\n\n\n\n\n\nF051-02 TIME data type\n\n\nFULL\n\n\nUse \nTimeOfDay\n from \nData.Time\n and \nval_\n\n\n\n\n\n\nF051-03 TIMESTAMP datatype\n\n\nFULL\n\n\nUse \nLocalTime\n from \nData.Time\n and \nval_\n. Precision can be specified in \nbeam-migrate\n\n\n\n\n\n\nF051-04 Comparison predicate on time types\n\n\nFULL\n\n\nUse comparison operatiors (See E011-05)\n\n\n\n\n\n\nF051-05 Explicit cast between date-time types and string\n\n\nTODO\n\n\n\n\n\n\n\n\nF051-06 CURRENT_DATE\n\n\nTODO\n\n\n\n\n\n\n\n\nF051-07 LOCALTIME\n\n\nTODO\n\n\n\n\n\n\n\n\nF051-08 LOCALTIMESTAMP\n\n\nTODO\n\n\n\n\n\n\n\n\nF081 UNION and EXCEPT in views\n\n\nTODO\n\n\nDepends on view support\n\n\n\n\n\n\nF111 Isolation levels other than SERIALIZABLE\n\n\nNONE\n\n\nUse backends\n\n\n\n\n\n\nF121 Basic diagnostics mangement\n\n\nNONE\n\n\nUse backends\n\n\n\n\n\n\nF122 Extended diagnostics management\n\n\nNONE\n\n\nUse backends\n\n\n\n\n\n\nF123 All diagnostics\n\n\nNONE\n\n\nUse backends\n\n\n\n\n\n\nF131 Grouped operations\n\n\nTODO\n\n\nDepends on grouped views\n\n\n\n\n\n\nF171 Multiple schemas per user\n\n\nN/A\n\n\nDepends on backend\n\n\n\n\n\n\nF191 Referential delete actions\n\n\nTODO\n\n\n\n\n\n\n\n\nF181 Multiple module support\n\n\nN/A\n\n\n\n\n\n\n\n\nF200 TRUNCATE TABLE statement\n\n\nTODO\n\n\nMay be added in the future\n\n\n\n\n\n\nF201 CAST function\n\n\nTODO\n\n\n\n\n\n\n\n\nF202 TRUNCATE TABLE: identity column restart option\n\n\nTODO\n\n\nDepends on F200\n\n\n\n\n\n\nF221 Explicit defaults\n\n\nFULL\n\n\nUse \nAuto Nothing\n when inserting\n\n\n\n\n\n\nF222 INSERT statement: DEFAULT VALUES clause\n\n\nTODO\n\n\n\n\n\n\n\n\nF251 Domain support\n\n\nTODO\n\n\nUse \nDomainType\n\n\n\n\n\n\nF261 CASE expression\n\n\n\n\n\n\n\n\n\n\nF261-01 Simple CASE\n\n\nTODO\n\n\nUse searched case (see F261-02)\n\n\n\n\n\n\nF261-02 Searched CASE\n\n\nFULL\n\n\nUse \nif_\n, \nthen_\n, and \nelse_\n\n\n\n\n\n\nF261-03 NULLIF\n\n\nFULL\n\n\nUse \nnullIf_\n\n\n\n\n\n\nF261-04 COALESCE\n\n\nFULL\n\n\nUse \ncoalesce_\n\n\n\n\n\n\nF262 Extended CASE expression\n\n\nPARTIAL\n\n\nBeam allows any value in a \nCASE\n expression\n\n\n\n\n\n\nF263 Comma-separater predicates in simple CASE expression\n\n\nTODO\n\n\n\n\n\n\n\n\nF271 Compound character literals\n\n\nPARTIAL\n\n\nBackends may do this automatically\n\n\n\n\n\n\nF281 LIKE enhancements\n\n\nPARTIAL\n\n\nSupported in backends that support this\n\n\n\n\n\n\nF291 UNIQUE predicate\n\n\nFULL\n\n\nUse \nunique_\n\n\n\n\n\n\nF301 CORRESPONDING in query expressions\n\n\nTODO\n\n\n\n\n\n\n\n\nF302 INTERSECT table operator\n\n\nFULL\n\n\nUse \nintersect_\n\n\n\n\n\n\nF302-01 INTERSECT DISTINCT table operator\n\n\nFULL\n\n\nUse \nintersect_\n\n\n\n\n\n\nF302-02 INTERSET ALL table operator\n\n\nFULL\n\n\nUse \nintersectAll_\n\n\n\n\n\n\nF304 EXCEPT ALL table operator\n\n\nFULL\n\n\nUse \nexceptAll_\n\n\n\n\n\n\nF311 Schema definition statement\n\n\nTODO\n\n\nWould be in \nbeam-migrate\n\n\n\n\n\n\nF312 MERGE statement\n\n\nTODO\n\n\n\n\n\n\n\n\nF313 Enhanced MERGE statement\n\n\nTODO\n\n\n\n\n\n\n\n\nF314 MERGE statement with DELETE branch\n\n\nTODO\n\n\n\n\n\n\n\n\nF321 User authorization\n\n\nN/A\n\n\n\n\n\n\n\n\nF361 Subprogram support\n\n\nN/A\n\n\n\n\n\n\n\n\nF381 Extended schema manipulation\n\n\nTODO\n\n\n\n\n\n\n\n\nF382 Alter column data type\n\n\nTODO\n\n\n\n\n\n\n\n\nF384 Drop identity property clause\n\n\nTODO\n\n\n\n\n\n\n\n\nF385 Drop column generation expression clause\n\n\nTODO\n\n\n\n\n\n\n\n\nF386 Set identity column generation clause\n\n\nTODO\n\n\n\n\n\n\n\n\nF391 Long identifiers\n\n\nPARTIAL\n\n\nSupported in backends that support it\n\n\n\n\n\n\nF392 Unicode escapes in identifiers\n\n\nTODO\n\n\nUnsure how this applies\n\n\n\n\n\n\nF393 Unicode escapes in literals\n\n\nTODO\n\n\nUnsure how this applies\n\n\n\n\n\n\nF394 Optional normal form specification\n\n\nN/A\n\n\n\n\n\n\n\n\nF401 Extended joined table\n\n\nPARTIAL\n\n\nFULL OUTER JOIN\n support planned with \nouterJoin_\n. \nCROSS JOIN\n is the same as \nINNER JOIN\n, and \nNATURAL JOIN\n has no meaning in beam.\n\n\n\n\n\n\nF402 Named column joins for LOBs, arrays, and multisets\n\n\nPARTIAL\n\n\nSupported in backends that support it\n\n\n\n\n\n\nF403 Partitioned join tables\n\n\nTODO\n\n\n\n\n\n\n\n\nF411 Time zone specification\n\n\nTODO\n\n\n\n\n\n\n\n\nF421 National character\n\n\nFULL\n\n\nSupported in \nbeam-migrate\n as a data type for \nText\n\n\n\n\n\n\nF431 Read-only scrollable cursors\n\n\nN/A\n\n\nUse the underlying backend\n\n\n\n\n\n\nF441 Extended set function support\n\n\nTODO\n\n\n\n\n\n\n\n\nF442 Mixed column references in set functions\n\n\nTODO\n\n\nUnsure how this would work with beam\n\n\n\n\n\n\nF451 Character set definition\n\n\nTODO\n\n\nLikely would go in \nbeam-migrate\n\n\n\n\n\n\nF461 Named character sets\n\n\nTODO\n\n\nSee F451\n\n\n\n\n\n\nF491 Constraint management\n\n\nTODO\n\n\n\n\n\n\n\n\nF492 Optional table constraint enforcement\n\n\nTODO\n\n\n\n\n\n\n\n\nF521 Assertions\n\n\nTODO\n\n\n\n\n\n\n\n\nF531 Temporary tables\n\n\nTODO\n\n\n\n\n\n\n\n\nF481 Expanded NULL predicate\n\n\nFULL\n\n\nSupported in backends that support it\n\n\n\n\n\n\nF555 Enhanced seconds precision\n\n\nTODO\n\n\n\n\n\n\n\n\nF561 Full value expressions\n\n\nTODO\n\n\n\n\n\n\n\n\nF571 Truth value tests\n\n\nTODO\n\n\n\n\n\n\n\n\nF591 Derived tables\n\n\nTODO\n\n\n\n\n\n\n\n\nF611 Indicator data types\n\n\nTODO\n\n\n\n\n\n\n\n\nF641 Row and table constructors\n\n\nPARTIAL\n\n\nUse \nrow_\n (TODO)\n\n\n\n\n\n\nF651 Catalog name qualifiers\n\n\nTODO\n\n\n\n\n\n\n\n\nF661 Simple tables\n\n\nTODO\n\n\n\n\n\n\n\n\nF671 Subqueries in CHECK constraints\n\n\nTODO\n\n\nPlanned with E141-06\n\n\n\n\n\n\nF672 Retrospective CHECK constraints\n\n\nTODO\n\n\nWould require temporal DB support\n\n\n\n\n\n\nF690 Collation support\n\n\nPARTIAL\n\n\nbeam-migrate\n supports some collation features\n\n\n\n\n\n\nF692 Enhanced collation support\n\n\nTODO\n\n\n\n\n\n\n\n\nF693 SQL-session and client module collations\n\n\nTODO\n\n\n\n\n\n\n\n\nF695 Translation support\n\n\nTODO\n\n\n\n\n\n\n\n\nF701 Referential update actions\n\n\nTODO\n\n\n\n\n\n\n\n\nF711 ALTER domain\n\n\nTODO\n\n\n\n\n\n\n\n\nF721 Deferrable constraints\n\n\nPARTIAL\n\n\nThe syntax exists in \nbeam-migrate\n\n\n\n\n\n\nF731 INSERT column privileges\n\n\nN/A\n\n\n\n\n\n\n\n\nF741 Referential MATCH type\n\n\nPARTIAL\n\n\nExists in the syntax in \nbeam-migrate\n, not exposed yet (TODO)\n\n\n\n\n\n\nF751 View CHECK enhancements\n\n\nTODO\n\n\n\n\n\n\n\n\nF761 Session management\n\n\nTODO\n\n\n\n\n\n\n\n\nF762 CURRENT_CATALOG\n\n\nTODO\n\n\n\n\n\n\n\n\nF763 CURRENT_SCHEMA\n\n\nTODO\n\n\n\n\n\n\n\n\nF812 Basic flagging\n\n\nN/A\n\n\n\n\n\n\n\n\nF841 LIKE_REGEX predicate\n\n\nTODO\n\n\nEasy\n\n\n\n\n\n\nF842 OCCURENCES_REGEX function\n\n\nTODO\n\n\nEasy\n\n\n\n\n\n\nF843 POSITION_REGEX function\n\n\nTODO\n\n\nEasy\n\n\n\n\n\n\nF844 SUBSTRING_REGEX function\n\n\nTODO\n\n\nEasy\n\n\n\n\n\n\nF845 TRANSLATE_REGEX function\n\n\nTODO\n\n\nEasy\n\n\n\n\n\n\nF846 Octet support in regular expression operators\n\n\nTODO\n\n\n\n\n\n\n\n\nF847 Nonconstant regular expression\n\n\nTODO\n\n\nEasy once regex support is added\n\n\n\n\n\n\nF850 Top-level \n in \n\n\nFULL\n\n\nUse \norderBy_\n as usual. Beam will do the right thing behind the scenes.\n\n\n\n\n\n\nF851 \n in subqueries\n\n\nFULL\n\n\nWorks in backends that support it\n\n\n\n\n\n\nF852 Top-level \n in views\n\n\nTODO\n\n\n\n\n\n\n\n\nF855 Nested \n in \n\n\nUNKNOWN\n\n\n\n\n\n\n\n\nF856 Nested \n in \n\n\nTODO\n\n\n\n\n\n\n\n\nF857 Top-level \n in \n\n\nTODO\n\n\n\n\n\n\n\n\nF858 \n in subqueries\n\n\nTODO\n\n\n\n\n\n\n\n\nF859 Top-level \n in subqueries\n\n\nTODO\n\n\n\n\n\n\n\n\n*\nF860 dynamic \n in \n\n\nTODO\n\n\n\n\n\n\n\n\n*\nF861 Top-level \n in \n\n\nTODO\n\n\n\n\n\n\n\n\nF862 \n in subqueries\n\n\nTODO\n\n\n\n\n\n\n\n\nF863 Nested \n in \n\n\nTODO\n\n\n\n\n\n\n\n\nF864 TOp-level \n in views\n\n\nTODO\n\n\n\n\n\n\n\n\nF865 dynamic \n in \n\n\nTODO\n\n\n\n\n\n\n\n\nF866 FETCH FIRST clause: PERCENT option\n\n\nTODO\n\n\n\n\n\n\n\n\nF867 FETCH FIRST clause: WITH TIES option\n\n\nTODO\n\n\n\n\n\n\n\n\nR010 Row pattern recognition: FROM clause\n\n\nTODO\n\n\n\n\n\n\n\n\nR020 Row pattern recognition: WINDOW clause\n\n\nTODO\n\n\n\n\n\n\n\n\nR030 Row pattern recognition: full aggregate support\n\n\nTODO\n\n\n\n\n\n\n\n\nS011 Distinct data types\n\n\nTODO\n\n\n\n\n\n\n\n\nS023 Basic structured types\n\n\nTODO\n\n\n\n\n\n\n\n\nS024 Enhanced structured types\n\n\nTODO\n\n\n\n\n\n\n\n\nS025 Final structured types\n\n\nTODO\n\n\n\n\n\n\n\n\nS026 Self-referencing structured types\n\n\nTODO\n\n\n\n\n\n\n\n\nS027 Create method by specific method name\n\n\nTODO\n\n\n\n\n\n\n\n\nS028 Permutable UDT options list\n\n\nTODO\n\n\n\n\n\n\n\n\nS041 Basic reference types\n\n\nTODO\n\n\n\n\n\n\n\n\nS043 Enhanced reference types\n\n\nTODO\n\n\n\n\n\n\n\n\nS051 Create table of type\n\n\nTODO\n\n\n\n\n\n\n\n\nS071 SQL paths in function and type name resolution\n\n\nN/A\n\n\nBeam qualifies everything anyway\n\n\n\n\n\n\nS081 Subtables\n\n\nPARTIAL\n\n\nYou can use them right now, but there's no support for their creation or management in \nbeam-migrate\n\n\n\n\n\n\nS091 Basic array support\n\n\nPARTIAL\n\n\nSupported via custom syntax in some backends (\nbeam-postgres\n for example)\n\n\n\n\n\n\nS092 Arrays of user-defined types\n\n\nTODO\n\n\nDepends on user-defined types\n\n\n\n\n\n\nS094 Arrays of reference types\n\n\nTODO\n\n\n\n\n\n\n\n\nS095 Array constructors by query\n\n\nTODO\n\n\n\n\n\n\n\n\nS096 Optional array bounds\n\n\nPARTIAL\n\n\nSupported in \nbeam-postgres\n\n\n\n\n\n\nS097 Array element assignment\n\n\nTODO\n\n\nNot yet, but should be easy enough in \nbeam-postgres\n\n\n\n\n\n\nS098 ARRAY_AGG\n\n\nTODO\n\n\nEasily added to \nbeam-postgres\n (Easy)\n\n\n\n\n\n\nS111 ONLY in query expressions\n\n\nTODO\n\n\n\n\n\n\n\n\nS151 Type predicate\n\n\nTODO\n\n\n\n\n\n\n\n\nS161 Subtype treatment\n\n\nTODO\n\n\n\n\n\n\n\n\nS162 Subtype treatment for references\n\n\nTODO\n\n\n\n\n\n\n\n\nS201 SQL-invoked routines on arrays\n\n\nTODO\n\n\nWould be subsumed by sql-routines (T-321)\n\n\n\n\n\n\nS202 SQL-invoked routines on multisets\n\n\nTODO\n\n\nWould be subsumed by sql-routines (T-321)\n\n\n\n\n\n\nS211 User-defined cast functions\n\n\nTODO\n\n\n\n\n\n\n\n\nS231 Structured type locators\n\n\nTODO\n\n\n\n\n\n\n\n\nS232 Array locators\n\n\nTODO\n\n\n\n\n\n\n\n\nS233 Multiset locators\n\n\nTODO\n\n\n\n\n\n\n\n\nS241 Transform functions\n\n\nTODO\n\n\n\n\n\n\n\n\nS242 Alter transform statement\n\n\nTODO\n\n\n\n\n\n\n\n\nS251 User-defined orderings\n\n\nTODO\n\n\n\n\n\n\n\n\nS261 Specific type method\n\n\nTODO\n\n\n\n\n\n\n\n\nS271 Basic multiset support\n\n\nTODO\n\n\n\n\n\n\n\n\nS272 Multisets of user-defined types\n\n\nTODO\n\n\n\n\n\n\n\n\nS274 Multisets reference types\n\n\nTODO\n\n\n\n\n\n\n\n\nS275 Advanced multiset support\n\n\nTODO\n\n\n\n\n\n\n\n\nS281 Nested collection types\n\n\nTODO\n\n\n\n\n\n\n\n\nS291 Unique constraint on entire row\n\n\nTODO\n\n\n\n\n\n\n\n\nS301 Enhanced UNNEST\n\n\nTODO\n\n\n\n\n\n\n\n\nS401 Distinct types based on array types\n\n\nTODO\n\n\n\n\n\n\n\n\nS402 Distinct types based on distinct types\n\n\nTODO\n\n\n\n\n\n\n\n\nS403 ARRAY_MAX_CARDINALITY\n\n\nTODO\n\n\n\n\n\n\n\n\nS404 TRIM_ARRAY\n\n\nTODO\n\n\n\n\n\n\n\n\nT021 BINARY and VARBINARY data types\n\n\nTODO\n\n\n\n\n\n\n\n\nT022 Advanced support for BINARY and VARBINARY data types\n\n\nTODO\n\n\n\n\n\n\n\n\nT023 Compound binary literals\n\n\nTODO\n\n\n\n\n\n\n\n\nT024 Spaces in binary literals\n\n\nTODO\n\n\n\n\n\n\n\n\nT031 Boolean data type\n\n\nTODO\n\n\n\n\n\n\n\n\nT041 Basic LOB data type support\n\n\nTODO\n\n\n\n\n\n\n\n\nT042 Extended LOB data type support\n\n\nTODO\n\n\n\n\n\n\n\n\nT043 Multiplier T\n\n\nTODO\n\n\n\n\n\n\n\n\nT044 Multiplier P\n\n\nTODO\n\n\n\n\n\n\n\n\nT051 Row types\n\n\nPARTIAL\n\n\n\n\n\n\n\n\nT061 UCS support\n\n\nTODO\n\n\n\n\n\n\n\n\nT071 BIGINT data type\n\n\nTODO\n\n\n\n\n\n\n\n\nT101 Enhanced nullability detection\n\n\nTODO\n\n\n\n\n\n\n\n\nT111 Updatable joins, unions,  and columns\n\n\nTODO\n\n\n\n\n\n\n\n\nT121 WITH (excluding recursive) in query expression\n\n\nTODO\n\n\n\n\n\n\n\n\nT122 WITH (excluding recursive) in subquery\n\n\nTODO\n\n\n\n\n\n\n\n\nT131 Recursive query\n\n\nTODO\n\n\n\n\n\n\n\n\nT132 Recursive query in subquery\n\n\nTODO\n\n\n\n\n\n\n\n\nT141 SIMILAR predicate\n\n\nFULL\n\n\n\n\n\n\n\n\nT151 DISTINCT predicate\n\n\nFULL\n\n\n\n\n\n\n\n\nT152 DISTINCT predicate with negation\n\n\nTODO\n\n\n\n\n\n\n\n\nT171 LIKE clause in table definition\n\n\nTODO\n\n\n\n\n\n\n\n\nT172 AS subquery clause in table definition\n\n\nTODO\n\n\n\n\n\n\n\n\nT173 Extended LIKE clause in table definition\n\n\nTODO\n\n\n\n\n\n\n\n\nT174 Identity columns\n\n\nTODO\n\n\n\n\n\n\n\n\nT175 Generated columns\n\n\nTODO\n\n\n\n\n\n\n\n\nT176 Sequence generator support\n\n\nTODO\n\n\n\n\n\n\n\n\nT177 Sequence generator support: simple restart option\n\n\nTODO\n\n\n\n\n\n\n\n\nT178 Identity columns: simple restart option\n\n\nTODO\n\n\n\n\n\n\n\n\nT180 System-versioned tables\n\n\nTODO\n\n\n\n\n\n\n\n\nT181 Application-time period tables\n\n\nTODO\n\n\n\n\n\n\n\n\nT191 Referential action RESTART\n\n\nTODO\n\n\n\n\n\n\n\n\nT201 Comparable data types for referential constraints\n\n\nTODO\n\n\n\n\n\n\n\n\nT211 Basic trigger capability\n\n\nTODO\n\n\n\n\n\n\n\n\nT212 Enhanced trigger capability\n\n\nTODO\n\n\n\n\n\n\n\n\nT213 INSTEAD OF triggers\n\n\nTODO\n\n\n\n\n\n\n\n\nT231 Sensitive cursors\n\n\nTODO\n\n\n\n\n\n\n\n\nT241 START TRANSACTION statement\n\n\nWONTFIX\n\n\nUse the backend library\n\n\n\n\n\n\nT251 SET TRANSACTION option: LOCAL option\n\n\nWONTFIX\n\n\nUse the backend library\n\n\n\n\n\n\nT261 Chained transactions\n\n\nN/A\n\n\n\n\n\n\n\n\nT271 Savepoints\n\n\nN/A\n\n\n\n\n\n\n\n\nT272 Enhanced savepoint management\n\n\nN/A\n\n\n\n\n\n\n\n\nT281 SELECT privilege with column granularity\n\n\nN/A\n\n\n\n\n\n\n\n\nT285 Enhanced derived column names\n\n\nN/A\n\n\n\n\n\n\n\n\nT301 Functional dependencies\n\n\nTODO\n\n\n\n\n\n\n\n\nT312 OVERLAY function\n\n\nTODO\n\n\n\n\n\n\n\n\nT321 Basic SQL-invoked routines\n\n\nTODO\n\n\n\n\n\n\n\n\nT323 Explicit security for external routines\n\n\nTODO\n\n\n\n\n\n\n\n\nT324 Explicit security for SQL routines\n\n\nTODO\n\n\n\n\n\n\n\n\nT325 Qualified SQL parameter references\n\n\nN/A\n\n\nBeam will likely use the qualified ones by default. Likely not exposed to user\n\n\n\n\n\n\nT326 Table functions\n\n\nTODO\n\n\n\n\n\n\n\n\nT331 Basic roles\n\n\nN/A\n\n\n\n\n\n\n\n\nT332 Extended roles\n\n\nN/A\n\n\n\n\n\n\n\n\nT341 Overleading of SQL-invoked functions and procodures\n\n\nWONTFIX\n\n\nHaskell doesn't allow overloading, and this seems complicated and unnecessary\n\n\n\n\n\n\nT351 Bracketed comments\n\n\nN/A\n\n\n\n\n\n\n\n\nT431 Extended grouping capabalities\n\n\nTODO\n\n\n\n\n\n\n\n\nT432 Nested and concatenated GROUPING SETs\n\n\nTODO\n\n\n\n\n\n\n\n\nT433 Multiargument GROUPING function\n\n\nTODO\n\n\n\n\n\n\n\n\nT434 GROUP BY DISTINCT\n\n\nTODO\n\n\n\n\n\n\n\n\nT441 ABS and MOD functions\n\n\nFULL\n\n\n\n\n\n\n\n\nT461 Symmetric BETWEEN predicate\n\n\nFULL\n\n\nBeam doesn't check this\n\n\n\n\n\n\nT471 Result sets return value\n\n\nTODO\n\n\n\n\n\n\n\n\nT472 DESCRIBE CURSOR\n\n\nN/A\n\n\nUse the backend library\n\n\n\n\n\n\nT491 LATERAL derived table\n\n\nTODO\n\n\n\n\n\n\n\n\nT495 Combined data change and retrieval\n\n\nTODO\n\n\n\n\n\n\n\n\nT501 Enhanced EXISTS predicate\n\n\nTODO\n\n\n\n\n\n\n\n\nT502 Period predicates\n\n\nTODO\n\n\n\n\n\n\n\n\nT511 Transaction counts\n\n\nTODO\n\n\n\n\n\n\n\n\nT521 Nested arguments in CALL statement\n\n\nTODO\n\n\n\n\n\n\n\n\nT522 Default values for IN parameters of SQL-invoked procs\n\n\nTODO\n\n\n\n\n\n\n\n\nT551 Optional key words for DEFAULT syntax\n\n\nTODO\n\n\n\n\n\n\n\n\nT561 Holdable locators\n\n\nTODO\n\n\n\n\n\n\n\n\nT571 Array-returning SQL-invoked functions\n\n\nTODO\n\n\nWill be supported once SQL-invoked functions are\n\n\n\n\n\n\nT572 Multiset-returning SQL-invoked functions\n\n\nTODO\n\n\n\n\n\n\n\n\nT581 Regular expression substring function\n\n\nTODO\n\n\n\n\n\n\n\n\nT591 UNIQUE constraints of possible NULL columns\n\n\nTODO\n\n\n\n\n\n\n\n\nT601 Local cursor references\n\n\nN/A\n\n\n\n\n\n\n\n\nT611 Elementary OLAP operations\n\n\nPARTIAL\n\n\nSee \nwithWindow_\n, \nwindow functions\n. Need \nnullsFirst_\n and \nnullsLast_\n\n\n\n\n\n\nT612 Advanced OLAP operations\n\n\nPARTIAL\n\n\nNeed \npercentRank_\n, \ncumeDist_\n, no exclusions yet\n\n\n\n\n\n\nT613 Sampling\n\n\nTODO\n\n\n\n\n\n\n\n\nT614 NTILE function\n\n\nTODO\n\n\n\n\n\n\n\n\nT615 LEAD and LAG function\n\n\nTODO\n\n\n\n\n\n\n\n\nT616 Null treatment for LEAD and LAG functions\n\n\nTODO\n\n\n\n\n\n\n\n\nT617 FIRST_VALUE and LAST_VALUE function\n\n\nTODO\n\n\n\n\n\n\n\n\nT618 NTH_VALUE function\n\n\nTODO\n\n\n\n\n\n\n\n\nT619 Nested window function\n\n\nTODO\n\n\n\n\n\n\n\n\nT620 WINDOW clause: GROUPS option\n\n\nTODO\n\n\n\n\n\n\n\n\nT621 Enhanced numeric functions\n\n\nTODO\n\n\n\n\n\n\n\n\nT641 Multiple column assignment\n\n\nTODO\n\n\n\n\n\n\n\n\nT651 SQL-schema statements in SQL routines\n\n\nTODO\n\n\n\n\n\n\n\n\nT652 SQL-dynamic statements in SQL routines\n\n\nTODO\n\n\n\n\n\n\n\n\nT653 SQL-schema statements in external routines\n\n\nTODO\n\n\n\n\n\n\n\n\nT654 SQL-dynamic statements in external routines\n\n\nTODO\n\n\n\n\n\n\n\n\nT655 Cyclically dependent routines\n\n\nTODO", 
            "title": "Compatibility Matrix"
        }, 
        {
            "location": "/about/license/", 
            "text": "The MIT License (MIT)\n\n\nCopyright \u00a9 2017 Travis Athougies\n\n\nPermission is hereby granted, free of charge, to any person\nobtaining a copy of this software and associated documentation\nfiles (the \u201cSoftware\u201d), to deal in the Software without\nrestriction, including without limitation the rights to use,\ncopy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the\nSoftware is furnished to do so, subject to the following\nconditions:\n\n\nThe above copyright notice and this permission notice shall be\nincluded in all copies or substantial portions of the Software.\n\n\nTHE SOFTWARE IS PROVIDED \u201cAS IS\u201d, WITHOUT WARRANTY OF ANY KIND,\nEXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES\nOF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND\nNONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT\nHOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY,\nWHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING\nFROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR\nOTHER DEALINGS IN THE SOFTWARE.", 
            "title": "License"
        }, 
        {
            "location": "/about/license/#the-mit-license-mit", 
            "text": "Copyright \u00a9 2017 Travis Athougies  Permission is hereby granted, free of charge, to any person\nobtaining a copy of this software and associated documentation\nfiles (the \u201cSoftware\u201d), to deal in the Software without\nrestriction, including without limitation the rights to use,\ncopy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the\nSoftware is furnished to do so, subject to the following\nconditions:  The above copyright notice and this permission notice shall be\nincluded in all copies or substantial portions of the Software.  THE SOFTWARE IS PROVIDED \u201cAS IS\u201d, WITHOUT WARRANTY OF ANY KIND,\nEXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES\nOF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND\nNONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT\nHOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY,\nWHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING\nFROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR\nOTHER DEALINGS IN THE SOFTWARE.", 
            "title": "The MIT License (MIT)"
        }, 
        {
            "location": "/about/release-notes/", 
            "text": "Beam Release Notes\n\n\n0.5.0.0\n\n\n\n\nMove to using finally tagless style for SQL generation\n\n\nSplit out backends from \nbeam-core\n\n\nAllow non-table entities to be stored in databases\n\n\nBasic migrations support", 
            "title": "Release Notes"
        }, 
        {
            "location": "/about/release-notes/#beam-release-notes", 
            "text": "", 
            "title": "Beam Release Notes"
        }, 
        {
            "location": "/about/release-notes/#0500", 
            "text": "Move to using finally tagless style for SQL generation  Split out backends from  beam-core  Allow non-table entities to be stored in databases  Basic migrations support", 
            "title": "0.5.0.0"
        }
    ]
}