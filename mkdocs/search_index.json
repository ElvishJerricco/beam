{
    "docs": [
        {
            "location": "/", 
            "text": "Beam is a highly-general library for accessing any kind of database with\nHaskell. Currently, it supports two SQL backends, Postgres and SQLite. Work is\nunderway for a MySQL backend. For information on creating additional SQL\nbackends, see the \nmanual section\n for more.\n\n\nBeam features\n\n\n\n\nEasy schema generation\n from existing databases\n\n\nA basic migration infrastructure\n for working with multiple versions of your\n  database schema.\n\n\nSupport for most SQL92, SQL99, and SQL2003 features\n across backends that\n  support them, including aggregations, subqueries, and window functions.\n\n\nA straightforward Haskell-friendly query syntax\n. You can use Beam's \nQ\n\n  monad much like you would interact with the \n[]\n monad.\n\n\nNo Template Haskell\n Beam uses the GHC Haskell type system and nothing else.\n  The types have been designed to be easily-inferrable by the compiler, and\n  appropriate functions to refine types have been provided for the where the\n  compiler may need more help.\n\n\n\n\nHow to install\n\n\nBeam is available via both \nHackage\n\nand \nStack\n. For most Haskell\ninstallations, one of the two commands should work.\n\n\ncabal install beam-core beam-migrate \nbackend\n\n\n\n\n\n\nor\n\n\nstack install beam-core beam-migrate \nbackend\n\n\n\n\n\n\nYou will also need to install an appropriate backend. Available backends are\n\n\n\n\n\n\nbeam-postgres\n -- A feature-complete backend for the Postgres RDBMS.\n  See \nthe beam-postgres documentation\n\n  for more information.\n\n\n\n\n\n\nbeam-sqlite\n -- An almost feature-complete backend for the Sqlite library.\n  Note that SQLite does not support all SQL92 features, so some of the examples\n  may not work. Refer\n  to \nthe beam-sqlite documentation\n for\n  more information on compatibility.\n\n\n\n\n\n\nQuick Start Guide\n\n\nIf you already have a database schema, you can use the \nbeam-migrate\n command to\nautomatically generate appropriate Beam data definitions.\n\n\nbeam-migrate --backend \nbackend-module\n \nbackend-options\n new \noutput-module-name\n\n\n\n\n\n\nFor example, to generate a schema for the Postgres database \nemployees\n at\n\nlocalhost:5000\n with the user \nbeam\n, run the following command.\n\n\nbeam-migrate --backend Database.Beam.Postgres.Migrate --pgconnect postgres://beam@localhost:5000/employees new BeamTutorial.Schema\n\n\n\n\n\nThis will generate a\n\n\nHow to Contribute\n\n\nWe always welcome contributions, especially to cover more database features or\nto add support for a new backend. Help is available on the \nbeam-users\n Google\nGroup. The following is a quick step-by-step guide of contributing a new feature:\n\n\n\n\nFork the github repository at \nhttps://github.com/tathougies/beam\n\n   and clone the fork to a local directory.\n\n\nWork on your feature on your own branch, or pick\n   an \nissue\n.\n\n\nWhen you feel ready to contribute the feature back to \nbeam-core\n, send a\n   Pull Request on Github, with an explanation of what your patch does and\n   whether it breaks the API.\n\n\nRespond to community comments and rework your patch.\n\n\nWhen the maintainer feels comfortable with your patch, he will commit it to\n   the \nmaster\n branch and it will be included in the next minor version.\n   API-breaking changes will not be included until the next major version.\n\n\n\n\nQuestions, Feedback, Discussion\n\n\n\n\nFor frequently asked questions, see the \nFAQ\n.\n\n\nFor general questions, feedback on patches, support, or other concerns, please\n  write to the mailing list\n\n\nFor bugs or feature requests,\n  please \nopen an issue\n\n\n\n\nWhy Beam?\n\n\nBeam is the most feature-complete, turnkey Haskell database solution out there.\nIt supports the entire breadth of the SQL92, SQL99, SQL2003, SQL2006, SQL2008,\nSQL2011, and SQL2016 specifications, as well as the entire breadth of features\nof each of its backends. See\nthe \ncompatibility matrix\n. You will rarely be forced to\nwrite a SQL query 'by hand' when using Beam.\n\n\nAdditionally, Beam plays nice with the rest of the Haskell ecosystem, the\nstandard Beam backends are all implemented in terms of the underlying\n\ndb\n-simple\n packages. Beam provides only minimum support for querying data\nacross multiple databases. It is assumed that you have chosen you RDBMS with\nmuch care, and we want to support you in that. Beam's main purpose is to marshal\ndata back and forth, to serve as the source of truth for the DB schema, and to\ngenerate properly formed SQL from Haskell expressions.", 
            "title": "Home"
        }, 
        {
            "location": "/#how-to-install", 
            "text": "Beam is available via both  Hackage \nand  Stack . For most Haskell\ninstallations, one of the two commands should work.  cabal install beam-core beam-migrate  backend   or  stack install beam-core beam-migrate  backend   You will also need to install an appropriate backend. Available backends are    beam-postgres  -- A feature-complete backend for the Postgres RDBMS.\n  See  the beam-postgres documentation \n  for more information.    beam-sqlite  -- An almost feature-complete backend for the Sqlite library.\n  Note that SQLite does not support all SQL92 features, so some of the examples\n  may not work. Refer\n  to  the beam-sqlite documentation  for\n  more information on compatibility.", 
            "title": "How to install"
        }, 
        {
            "location": "/#quick-start-guide", 
            "text": "If you already have a database schema, you can use the  beam-migrate  command to\nautomatically generate appropriate Beam data definitions.  beam-migrate --backend  backend-module   backend-options  new  output-module-name   For example, to generate a schema for the Postgres database  employees  at localhost:5000  with the user  beam , run the following command.  beam-migrate --backend Database.Beam.Postgres.Migrate --pgconnect postgres://beam@localhost:5000/employees new BeamTutorial.Schema  This will generate a", 
            "title": "Quick Start Guide"
        }, 
        {
            "location": "/#how-to-contribute", 
            "text": "We always welcome contributions, especially to cover more database features or\nto add support for a new backend. Help is available on the  beam-users  Google\nGroup. The following is a quick step-by-step guide of contributing a new feature:   Fork the github repository at  https://github.com/tathougies/beam \n   and clone the fork to a local directory.  Work on your feature on your own branch, or pick\n   an  issue .  When you feel ready to contribute the feature back to  beam-core , send a\n   Pull Request on Github, with an explanation of what your patch does and\n   whether it breaks the API.  Respond to community comments and rework your patch.  When the maintainer feels comfortable with your patch, he will commit it to\n   the  master  branch and it will be included in the next minor version.\n   API-breaking changes will not be included until the next major version.", 
            "title": "How to Contribute"
        }, 
        {
            "location": "/#questions-feedback-discussion", 
            "text": "For frequently asked questions, see the  FAQ .  For general questions, feedback on patches, support, or other concerns, please\n  write to the mailing list  For bugs or feature requests,\n  please  open an issue", 
            "title": "Questions, Feedback, Discussion"
        }, 
        {
            "location": "/#why-beam", 
            "text": "Beam is the most feature-complete, turnkey Haskell database solution out there.\nIt supports the entire breadth of the SQL92, SQL99, SQL2003, SQL2006, SQL2008,\nSQL2011, and SQL2016 specifications, as well as the entire breadth of features\nof each of its backends. See\nthe  compatibility matrix . You will rarely be forced to\nwrite a SQL query 'by hand' when using Beam.  Additionally, Beam plays nice with the rest of the Haskell ecosystem, the\nstandard Beam backends are all implemented in terms of the underlying db -simple  packages. Beam provides only minimum support for querying data\nacross multiple databases. It is assumed that you have chosen you RDBMS with\nmuch care, and we want to support you in that. Beam's main purpose is to marshal\ndata back and forth, to serve as the source of truth for the DB schema, and to\ngenerate properly formed SQL from Haskell expressions.", 
            "title": "Why Beam?"
        }, 
        {
            "location": "/about/faq/", 
            "text": "How does \nbeam\n compare with \nx\n?\n\n\nHelp! The type checker keeps complaining about \nSyntax\n types\n\n\nSuppose you had the following code to run a query over an arbitrary backend that\nsupported the SQL92 syntax.\n\n\nlistEmployees\n \n::\n \n(\nIsSql92Syntax\n \ncmd\n,\n \nMonadBeam\n \ncmd\n \nbe\n \nhdl\n \nm\n)\n \n=\n \nm\n \n[\nEmployee\n]\n\n\nlistEmployees\n \n=\n \nrunSelectReturningList\n \n$\n \nselect\n \n(\nall_\n \n(\nemployees\n \nemployeeDb\n))\n\n\n\n\n\n\nYou may get an error message like the following\n\n\nMyQueries.hs:1:1: error:\n    * Could not deduce: Sql92ProjectionExpressionSyntax\n                          (Sql92SelectTableProjectionSyntax\n                             (Sql92SelectSelectTableSyntax (Sql92SelectSyntax cmd)))\n                        ~\n                        Sql92SelectTableExpressionSyntax\n                          (Sql92SelectSelectTableSyntax (Sql92SelectSyntax cmd))\n        arising from a use of \nselect\n\n\n\n\n\n\nBeam uses a \nfinally-tagless\n\nencoding for syntaxes. This means we never deal with concrete syntax types\ninternally, just types that fulfill certain constraints (in this case, being a\nvalid description of a SQL92 syntax). This works really nicely for\nextensibility, but makes the types slightly confusing. Here, the type checker is\ncomplaining that it cannot prove that the type of expressions used in\nprojections is the same as the type of expressions used in \nWHERE\n and \nHAVING\n\nclauses. Of course, any sane SQL92 syntax would indeed meet this criteria, but\nthis criteria is difficult to enforce at the type class level (it leads to\ncycles in superclasses, which requires the scary-looking\n\nUndecidableSuperclasses\n extension in GHC).\n\n\nNevertheless, we can avoid all this hullabaloo by using the \nSql92SanityCheck\n\nconstraint synonym. This takes a command syntax and asserts all the type\nequalities that a sane SQL92 syntax would support. Thus the code above becomes.\n\n\nlistEmployees\n \n::\n \n(\n \nIsSql92Syntax\n \ncmd\n,\n \nSql92SanityCheck\n \ncmd\n\n                 \n,\n \nMonadBeam\n \ncmd\n \nbe\n \nhdl\n \nm\n)\n\n              \n=\n \nm\n \n[\nEmployee\n]\n\n\nlistEmployees\n \n=\n \nrunSelectReturningList\n \n$\n \nselect\n \n(\nall_\n \n(\nemployees\n \nemployeeDb\n))\n\n\n\n\n\n\nOther database mappers simulate features on databases that lack support, why not beam?\n\n\nBeam assumes that the developer has picked their RDBMS for a reason. Beam does\nnot try to take on features of the RDBMS, because often there is no reasonable\nand equally performant substitution that can be made. Beam tries to follow the\nprinciple of least surprise -- the SQL queries beam generates should be easily\nguessable from the Haskell query DSL (modulo aliasing). Generating complicated\nemulation code which can result in unpredictable performance would violate this\nprinciple.", 
            "title": "Frequently Asked Questions"
        }, 
        {
            "location": "/about/faq/#how-does-beam-compare-with-x", 
            "text": "", 
            "title": "How does beam compare with &lt;x&gt;?"
        }, 
        {
            "location": "/about/faq/#help-the-type-checker-keeps-complaining-about-syntax-types", 
            "text": "Suppose you had the following code to run a query over an arbitrary backend that\nsupported the SQL92 syntax.  listEmployees   ::   ( IsSql92Syntax   cmd ,   MonadBeam   cmd   be   hdl   m )   =   m   [ Employee ]  listEmployees   =   runSelectReturningList   $   select   ( all_   ( employees   employeeDb ))   You may get an error message like the following  MyQueries.hs:1:1: error:\n    * Could not deduce: Sql92ProjectionExpressionSyntax\n                          (Sql92SelectTableProjectionSyntax\n                             (Sql92SelectSelectTableSyntax (Sql92SelectSyntax cmd)))\n                        ~\n                        Sql92SelectTableExpressionSyntax\n                          (Sql92SelectSelectTableSyntax (Sql92SelectSyntax cmd))\n        arising from a use of  select   Beam uses a  finally-tagless \nencoding for syntaxes. This means we never deal with concrete syntax types\ninternally, just types that fulfill certain constraints (in this case, being a\nvalid description of a SQL92 syntax). This works really nicely for\nextensibility, but makes the types slightly confusing. Here, the type checker is\ncomplaining that it cannot prove that the type of expressions used in\nprojections is the same as the type of expressions used in  WHERE  and  HAVING \nclauses. Of course, any sane SQL92 syntax would indeed meet this criteria, but\nthis criteria is difficult to enforce at the type class level (it leads to\ncycles in superclasses, which requires the scary-looking UndecidableSuperclasses  extension in GHC).  Nevertheless, we can avoid all this hullabaloo by using the  Sql92SanityCheck \nconstraint synonym. This takes a command syntax and asserts all the type\nequalities that a sane SQL92 syntax would support. Thus the code above becomes.  listEmployees   ::   (   IsSql92Syntax   cmd ,   Sql92SanityCheck   cmd \n                  ,   MonadBeam   cmd   be   hdl   m ) \n               =   m   [ Employee ]  listEmployees   =   runSelectReturningList   $   select   ( all_   ( employees   employeeDb ))", 
            "title": "Help! The type checker keeps complaining about Syntax types"
        }, 
        {
            "location": "/about/faq/#other-database-mappers-simulate-features-on-databases-that-lack-support-why-not-beam", 
            "text": "Beam assumes that the developer has picked their RDBMS for a reason. Beam does\nnot try to take on features of the RDBMS, because often there is no reasonable\nand equally performant substitution that can be made. Beam tries to follow the\nprinciple of least surprise -- the SQL queries beam generates should be easily\nguessable from the Haskell query DSL (modulo aliasing). Generating complicated\nemulation code which can result in unpredictable performance would violate this\nprinciple.", 
            "title": "Other database mappers simulate features on databases that lack support, why not beam?"
        }, 
        {
            "location": "/tutorials/tutorial1/", 
            "text": "In this tutorial sequence, we'll walk through creating a schema for a simple\nshopping cart database. We'll start by defining a user table. Then, we'll show\nhow beam makes it easy to manipulate data in our database. Finally, we'll\ndemonstrate how beam lets us declare type-safe and composable queries.\n\n\nBeam Module Structure\n\n\nBeam makes extensive use of GHC's Generics mechanism. This extension means beam does not need to\nrely on template haskell.\n\n\n{-# LANGUAGE StandaloneDeriving, TypeSynonymInstances, FlexibleInstances, TypeFamilies, DeriveGeneric, OverloadedStrings #-}\n\n\nmodule\n \nMain\n \nwhere\n\n\n\n\n\n\nTo start defining beam schemas and queries, you only need to import the\n\nDatabase.Beam\n module. To interface with an actual database, you'll need to\nimport one of the database backends. We'll see how to use the Sqlite backend\nhere (found in the \nbeam-sqlite\n package).\n\n\nimport\n \nDatabase.Beam\n\n\nimport\n \nDatabase.Beam.Sqlite\n\n\n\nimport\n \nData.Text\n \n(\nText\n)\n\n\n\n\n\n\nDefining our first table\n\n\nBeam tables are regular Haskell data types with a bit of scaffolding. Thankfully, the magic of the\nmodern Haskell type system allows us to remove the overhead and the syntactic fuzz of the\nscaffolding in most situations.\n\n\nWe start by declaring a data structure named \nUserT\n. As a matter of convention, Beam table types\nare suffixed with 'T'. Table types have only one constructor. Again, as a matter of convention, the\nconstructor has the same name as the table, but without the 'T' suffix. We'll soon see the reason\nfor this convention.\n\n\nIn this tutorial, I'll prefix all record selectors with an underscore. This is a matter of personal\npreference. One reason for the prefix is that it plays nicely with the \nlens\n library. Beam does not\nnecessitate the use of \nlens\n (in fact Beam includes its own mechanism to generically derive van\nLaarhoven lenses), but I recognize that some programmers use \nlens\n quite a lot.\n\n\ndata\n \nUserT\n \nf\n \n=\n \nUser\n\n             \n{\n \n_userEmail\n     \n::\n \nColumnar\n \nf\n \nText\n\n             \n,\n \n_userFirstName\n \n::\n \nColumnar\n \nf\n \nText\n\n             \n,\n \n_userLastName\n  \n::\n \nColumnar\n \nf\n \nText\n\n             \n,\n \n_userPassword\n  \n::\n \nColumnar\n \nf\n \nText\n \n}\n\n              \nderiving\n \nGeneric\n\n\n\n\n\n\nThis data type might look very complicated, so I'd like to show you that it's not that scary. Let's\nsee if we can use GHCi to help us.\n\n\n*BasicTutorial\n :t User\nUser\n :: Columnar f Text\n    -\n Columnar f Text -\n Columnar f Text -\n Columnar f Text -\n UserT f\n\n\n\n\n\nHmm... That did not help much. However, consider the type of the following:\n\n\n*BasicTutorial\n :t (\\email firstName lastName password -\n User email firstName lastName password :: UserT Identity)\n (\\email firstName lastName password -\n User email firstName lastName password :: UserT Identity)\n :: Text -\n Text -\n Text -\n Text -\n UserT Identity\n\n\n\n\n\nWoah! That looks a lot like what we'd expect if we had declared the type in the \"regular\" Haskell way:\n\n\ndata\n \nUser\n \n=\n \nUser\n\n          \n{\n \n_userEmail\n     \n::\n \nText\n\n          \n,\n \n_userFirstName\n \n::\n \nText\n\n          \n,\n \n_userLastName\n  \n::\n \nText\n\n          \n,\n \n_userPassword\n  \n::\n \nText\n \n}\n\n\n\n\n\n\nThis functionality is due to the fact that \nColumnar\n is a type family defined such that for any\n\nx\n, \nColumnar Identity x = x\n. Knowing this, let's define a type synonym to make our life easier.\n\n\ntype\n \nUser\n \n=\n \nUserT\n \nIdentity\n\n\n\n\n\n\nNow you can see why we named the type of the table \nUserT\n and its constructor \nUser\n. This allows\nus to use the \"regular\" \nUser\n constructor to construct values of type \nUser\n. We can use the\n\nStandaloneDeriving\n extension to derive an instance of \nShow\n and \nEq\n for the 'regular' datatype.\n\n\nderiving\n \ninstance\n \nShow\n \nUser\n\n\nderiving\n \ninstance\n \nEq\n \nUser\n\n\n\n\n\n\nTeaching Beam about our table\n\n\nWe've defined a type that can represent our the data in our table. Now, let's inform beam that we'd like to\nuse `UserT' as a table.\n\n\ninstance\n \nTable\n \nUserT\n \nwhere\n\n\n\n\n\n\nAll beam tables need to implement the \nTable\n type class. The \nTable\n type class contains functions\nfor marshalling values between SQL and Haskell and for allowing the table to be queried. Due to\nGHC's DeriveGeneric and DefaultSignatures extensions, all these methods can be written for us by the\ncompiler at compile-time!\n\n\nThe only thing we need to provide is the type of the primary keys for users, and a function that can\nextract the primary key from any \nUserT f\n object. To do this, we just add the following lines to\nthe instance declaration.\n\n\n    \ndata\n \nPrimaryKey\n \nUserT\n \nf\n \n=\n \nUserId\n \n(\nColumnar\n \nf\n \nText\n)\n \nderiving\n \nGeneric\n\n    \nprimaryKey\n \n=\n \nUserId\n \n$\n \n_userEmail\n\n\n\n\n\n\nWe use the applicative \n$\n operator to simplify our definiton for\n\nprimaryKey\n, relying on the applicative instance for \n(a -\n)\n.\n\n\ntype\n \nUserId\n \n=\n \nPrimaryKey\n \nUserT\n \nIdentity\n\n\nderiving\n \ninstance\n \nShow\n \nUserId\n\n\nderiving\n \ninstance\n \nEq\n \nUserId\n\n\n\n\n\n\nDefining our database\n\n\nNow that we have our table, we're going to define a type to hold information about our\ndatabase. Defining our database is going to follow the same pattern as defining a table. We'll\ndefine a higher-kinded datatype and then declare an instance of \nDatabase\n, and let the compiler\nfigure most of it out.\n\n\nA database is made up of a collection of 'entities', of which tables are just one type.\n\n\ndata\n \nShoppingCartDb\n \nf\n \n=\n \nShoppingCartDb\n\n                      \n{\n \n_shoppingCartUsers\n \n::\n \nf\n \nUserT\n \n}\n\n                        \nderiving\n \nGeneric\n\n\n\ninstance\n \nDatabase\n \nShoppingCartDb\n\n\n\n\n\n\nThe next step is to create a description of the particular database we'd like to create. This\ninvolves giving each of the tables in our database a name. If you've named all your database\nselectors using camel case, beam can automatically figure out what all the table names should be. If\nyou haven't, or you have multiple tables holding the same type in your database, you might have to\nmanually name your tables. For now, we'll let beam do the hard work.\n\n\nshoppingCartDb\n \n::\n \nDatabaseSettings\n \nShoppingCartDb\n\n\nshoppingCartDb\n \n=\n \nautoDbSettings\n\n\n\n\n\n\nAdding users to our database\n\n\nLet's add some users to our database. First, we'll open a connection to a SQLite3 database using\nbeam.  The \nopenDatabase\n function will open the database and optionally attempt to make the\nschema of the opened database match the schema implied by the data types. Below we use the\n\nopenDatabaseDebug\n function to put beam into debug mode. This will cause beam to log every sql\nstatement executed. \nopenDatabaseDebug\n and \nopenDatabase\n have the same type signature, so they can\nbe used interchangeably. In the call below, we pass in \nAutoMigrate\n to have beam automatically\nattempt to match up the real database schema with what it would expect based on the Haskell types.\n\n\nmain\n \n::\n \nIO\n \n()\n\n\nmain\n \n=\n \ndo\n \nbeam\n \n-\n \nopenDatabaseDebug\n \nshoppingCartDb\n \nAutoMigrate\n \n(\nSqlite3Settings\n \nshoppingcart1.db\n)\n\n\n\n\n\n\nTo make sure that beam correctly inferred the database schema, let's dump it.\n\n\n          \ndumpSchema\n \nshoppingCartDb\n\n\n\n\n\n\nThis will dump the SQL CREATE TABLE statements to the console.\n\n\nDumping database schema ...\nCREATE TABLE cart_users (email VARCHAR NOT NULL, first_name VARCHAR NOT NULL, last_name VARCHAR NOT NULL, password VARCHAR NOT NULL, PRIMARY KEY( email ))\n\n\n\n\n\nBeam automatically converted our \nUserT\n table in the \n_shoppingCartUsers\n selector to a table named\n\ncart_users\n. As we stated above, this comes from un-CamelCase-ing the selector name and dropping\nthe first component. Beam followed the same rule to derive the column names.\n\n\nNow let's add a few users. We'll give each user an MD5 encoded password too.\n\n\n          \nbeamTxn\n \nbeam\n \n$\n \n\\\n(\nShoppingCartDb\n \nusersT\n)\n \n-\n\n           \ndo\n \ninsertInto\n \nusersT\n \n(\nUser\n \njames@example.com\n \nJames\n \nSmith\n \nb4cc344d25a2efe540adbf2678e2304c\n \n{- james -}\n)\n\n              \ninsertInto\n \nusersT\n \n(\nUser\n \nbetty@example.com\n \nBetty\n \nJones\n \n82b054bd83ffad9b6cf8bdb98ce3cc2f\n \n{- betty -}\n)\n\n              \ninsertInto\n \nusersT\n \n(\nUser\n \nsam@example.com\n \nSam\n \nTaylor\n \n332532dcfaa1cbf61e2a266bd723612c\n \n{- sam -}\n)\n\n\n\n\n\n\nThe \ninsertInto\n function inserts a value into a table that can hold that type.\n\n\nBecause we're in debug mode, we'll see the SQL that beam is running:\n\n\nWill execute INSERT INTO cart_users VALUES (?, ?, ?, ?) with [SqlString \njames@example.com\n,SqlString \nJames\n,SqlString \nSmith\n,SqlString \nb4cc344d25a2efe540adbf2678e2304c\n]\nWill execute INSERT INTO cart_users VALUES (?, ?, ?, ?) with [SqlString \nbetty@example.com\n,SqlString \nBetty\n,SqlString \nJones\n,SqlString \n82b054bd83ffad9b6cf8bdb98ce3cc2f\n]\nWill execute INSERT INTO cart_users VALUES (?, ?, ?, ?) with [SqlString \nsam@example.com\n,SqlString \nSam\n,SqlString \nTaylor\n,SqlString \n332532dcfaa1cbf61e2a266bd723612c\n]\n\n\n\n\n\nQuerying the database\n\n\nNow let's write some queries for the database. You can think of queries in Beam as similar to lists.\nLike lists you can filter them (SQL WHERE clauses), take only a certain number of items from them\n(SQL LIMIT clauses), and drop a certain number of items from them (SQL OFFSET clauses).\n\n\nLet's look at a simple example. Let's get all users.\n\n\n          \nputStrLn\n \nall users: \n\n          \nSuccess\n \nallUsers\n \n-\n \nbeamTxn\n \nbeam\n \n$\n \n\\\n(\nShoppingCartDb\n \nusersT\n)\n \n-\n\n                              \nqueryList\n \n(\nall_\n \nusersT\n)\n\n          \nmapM_\n \n(\nputStrLn\n \n.\n \nshow\n)\n \nallUsers\n\n          \nputStrLn\n \n----\n\\n\\n\n\n\n\n\n\n\nWhen run, you should get output like the following:\n\n\nWill execute SELECT `t0`.`email`, `t0`.`first_name`, `t0`.`last_name`, `t0`.`password` FROM  cart_users AS t0 with []\nUser {_userEmail = \njames@example.com\n, _userFirstName = \nJames\n, _userLastName = \nSmith\n, _userPassword = \nb4cc344d25a2efe540adbf2678e2304c\n}\nUser {_userEmail = \nbetty@example.com\n, _userFirstName = \nBetty\n, _userLastName = \nJones\n, _userPassword = \n82b054bd83ffad9b6cf8bdb98ce3cc2f\n}\nUser {_userEmail = \nsam@example.com\n, _userFirstName = \nSam\n, _userLastName = \nTaylor\n, _userPassword = \n332532dcfaa1cbf61e2a266bd723612c\n}\n\n\n\n\n\nNext let's suppose you wanted to sort the users into order by their first name. We can use the\n\norderBy\n function to order the query results. This is similar to the \nsortBy\n function for lists.\n\n\n          \nputStrLn\n \nusers sorted by first name:\n\n          \nSuccess\n \nsortedUsersByFirstName\n \n-\n \nbeamTxn\n \nbeam\n \n$\n \n\\\n(\nShoppingCartDb\n \nusersT\n)\n \n-\n\n                                            \nqueryList\n \n$\n \norderBy\n \n(\nasc_\n \n.\n \n_userFirstName\n)\n \n(\nall_\n \nusersT\n)\n\n          \nmapM_\n \n(\nputStrLn\n \n.\n \nshow\n)\n \nsortedUsersByFirstName\n\n          \nputStrLn\n \n----\n\\n\\n\n\n\n\n\n\n\nThe corresponding output:\n\n\nWill execute SELECT `t0`.`email`, `t0`.`first_name`, `t0`.`last_name`, `t0`.`password` FROM  cart_users AS t0 ORDER BY `t0`.`first_name` ASC with []\nUser {_userEmail = \nbetty@example.com\n, _userFirstName = \nBetty\n, _userLastName = \nJones\n, _userPassword = \n82b054bd83ffad9b6cf8bdb98ce3cc2f\n}\nUser {_userEmail = \njames@example.com\n, _userFirstName = \nJames\n, _userLastName = \nSmith\n, _userPassword = \nb4cc344d25a2efe540adbf2678e2304c\n}\nUser {_userEmail = \nsam@example.com\n, _userFirstName = \nSam\n, _userLastName = \nTaylor\n, _userPassword = \n332532dcfaa1cbf61e2a266bd723612c\n}\n\n\n\n\n\nWe can also sort by more than one column.\n\n\n          \nputStrLn\n \nusers sorted by first and last name:\n\n          \nSuccess\n \nsortedUsersByFirstAndLastName\n \n-\n \nbeamTxn\n \nbeam\n \n$\n \n\\\n(\nShoppingCartDb\n \nusersT\n)\n \n-\n\n                                                   \nqueryList\n \n$\n \norderBy\n \n(\n\\\nuser\n \n-\n \n(\nasc_\n \n(\n_userFirstName\n \nuser\n),\n \nasc_\n \n(\n_userLastName\n \nuser\n)))\n \n(\nall_\n \nusersT\n)\n\n          \nmapM_\n \n(\nputStrLn\n \n.\n \nshow\n)\n \nsortedUsersByFirstAndLastName\n\n          \nputStrLn\n \n----\n\\n\\n\n\n\n\n\n\n\nAgain, the output for convenience:\n\n\nWill execute SELECT `t0`.`email`, `t0`.`first_name`, `t0`.`last_name`, `t0`.`password` FROM  cart_users AS t0 ORDER BY `t0`.`first_name` ASC, `t0`.`last_name` ASC with []\nUser {_userEmail = \nbetty@example.com\n, _userFirstName = \nBetty\n, _userLastName = \nJones\n, _userPassword = \n82b054bd83ffad9b6cf8bdb98ce3cc2f\n}\nUser {_userEmail = \njames@example.com\n, _userFirstName = \nJames\n, _userLastName = \nSmith\n, _userPassword = \nb4cc344d25a2efe540adbf2678e2304c\n}\nUser {_userEmail = \nsam@example.com\n, _userFirstName = \nSam\n, _userLastName = \nTaylor\n, _userPassword = \n332532dcfaa1cbf61e2a266bd723612c\n}\n\n\n\n\n\nWe can use \nlimit_\n and \noffset_\n in a similar manner to \ntake\n and \ndrop\n respectively.\n\n\n          \nputStrLn\n \nthe second user based on their first names\n\n          \nSuccess\n \n[\nsecondUser\n]\n \n-\n \nbeamTxn\n \nbeam\n \n$\n \n\\\n(\nShoppingCartDb\n \nusersT\n)\n \n-\n\n                                  \nqueryList\n \n$\n \nlimit_\n \n1\n \n(\noffset_\n \n1\n \n(\norderBy\n \n(\nasc_\n \n.\n \n_userFirstName\n)\n \n(\nall_\n \nusersT\n)))\n\n          \nputStrLn\n \n(\nshow\n \nsecondUser\n)\n\n          \nputStrLn\n \n----\n\\n\\n\n\n\n\n\n\n\nAnd the output:\n\n\nWill execute SELECT `t0`.`email`, `t0`.`first_name`, `t0`.`last_name`, `t0`.`password` FROM  cart_users AS t0 ORDER BY `t0`.`first_name` ASC LIMIT 1 OFFSET 1 with []\nUser {_userEmail = \njames@example.com\n, _userFirstName = \nJames\n, _userLastName = \nSmith\n, _userPassword = \nb4cc344d25a2efe540adbf2678e2304c\n}\n\n\n\n\n\nAggregations\n\n\nSometimes we also want to group our data together and perform calculations over the groups of data. SQL calls these aggregations.\n\n\nThe simplest aggregation is counting. We use the \naggregate\n function to create aggregations.\n\n\n          \nputStrLn\n \nThe total number of users\n\n          \nSuccess\n \n[\nuserCount\n]\n \n-\n \nbeamTxn\n \nbeam\n \n$\n \n\\\n(\nShoppingCartDb\n \nusersT\n)\n \n-\n\n                                 \nqueryList\n \n$\n \naggregate\n \n(\n\\\nuser\n \n-\n \ncount_\n \n(\n_userEmail\n \nuser\n))\n \n(\nall_\n \nusersT\n)\n\n          \nputStrLn\n \n(\nshow\n \nuserCount\n)\n\n          \nputStrLn\n \n----\n\\n\\n\n\n\n\n\n\n\nMaybe we'd like something a little more interesting, such as the number of users for each unique\nfirst name. We can also express these aggregations using the \naggregate\n function. In order to get\ninteresting results, we'll need to add more users to our database.\n\n\n          \nbeamTxn\n \nbeam\n \n$\n \n\\\n(\nShoppingCartDb\n \nusersT\n)\n \n-\n\n           \ndo\n \ninsertInto\n \nusersT\n \n(\nUser\n \njames@pallo.com\n \nJames\n \nPallo\n \nb4cc344d25a2efe540adbf2678e2304c\n \n{- james -}\n)\n\n              \ninsertInto\n \nusersT\n \n(\nUser\n \nbetty@sims.com\n \nBetty\n \nSims\n \n82b054bd83ffad9b6cf8bdb98ce3cc2f\n \n{- betty -}\n)\n\n              \ninsertInto\n \nusersT\n \n(\nUser\n \njames@oreily.com\n \nJames\n \nO\nReily\n \nb4cc344d25a2efe540adbf2678e2304c\n \n{- james -}\n)\n\n              \ninsertInto\n \nusersT\n \n(\nUser\n \nsam@sophitz.com\n \nSam\n \nSophitz\n \n332532dcfaa1cbf61e2a266bd723612c\n \n{- sam -}\n)\n\n              \ninsertInto\n \nusersT\n \n(\nUser\n \nsam@jely.com\n \nSam\n \nJely\n \n332532dcfaa1cbf61e2a266bd723612c\n \n{- sam -}\n)\n\n\n\n\n\n\nNow we can use \naggregate\n to both group by a user's first name, and then count the number of users.\n\n\n          \nputStrLn\n \nThe number of users for each name\n\n          \nSuccess\n \ncountsByFirstName\n \n-\n \nbeamTxn\n \nbeam\n \n$\n \n\\\n(\nShoppingCartDb\n \nusersT\n)\n \n-\n\n                                       \nqueryList\n \n$\n\n                                       \naggregate\n \n(\n\\\nuser\n \n-\n \n(\ngroup_\n \n(\n_userFirstName\n \nuser\n),\n \ncount_\n \n(\n_userEmail\n \nuser\n)))\n \n$\n\n                                       \nall_\n \nusersT\n\n          \nmapM\n \n(\nputStrLn\n \n.\n \nshow\n)\n \ncountsByFirstName\n\n          \nputStrLn\n \n----\n\\n\\n\n\n\n\n\n\n\nBeam produces a very reasonable looking SQL statement, and returns the correct results.\n\n\nThe number of users for each name\nWill execute SELECT `t0`.`first_name`, COUNT(`t0`.`email`) FROM  cart_users AS t0 GROUP BY `t0`.`first_name` with []\n(\nBetty\n,2)\n(\nJames\n,3)\n(\nSam\n,3)\n----\n\n\n\n\n\nConclusion\n\n\nIn this tutorial, we've covered creating a database schema, opening up a beam database, inserting\nvalues into the database, and querying values from them. We used the knowledge we learned to create\na partial shopping cart database that contains information about users. In the next tutorial, we'll\ndelve deeper into the some of the query types and show how we can create relations between\ntables. We'll also use the monadic query interface to create SQL joins.\n\n\nUntil next time!\n\n\nIf you have any questions about beam, feel free to send them to travis@athougies.net . Pull requests and bug reports are welcome on \nGitHub\n.", 
            "title": "Part 1"
        }, 
        {
            "location": "/tutorials/tutorial1/#beam-module-structure", 
            "text": "Beam makes extensive use of GHC's Generics mechanism. This extension means beam does not need to\nrely on template haskell.  {-# LANGUAGE StandaloneDeriving, TypeSynonymInstances, FlexibleInstances, TypeFamilies, DeriveGeneric, OverloadedStrings #-}  module   Main   where   To start defining beam schemas and queries, you only need to import the Database.Beam  module. To interface with an actual database, you'll need to\nimport one of the database backends. We'll see how to use the Sqlite backend\nhere (found in the  beam-sqlite  package).  import   Database.Beam  import   Database.Beam.Sqlite  import   Data.Text   ( Text )", 
            "title": "Beam Module Structure"
        }, 
        {
            "location": "/tutorials/tutorial1/#defining-our-first-table", 
            "text": "Beam tables are regular Haskell data types with a bit of scaffolding. Thankfully, the magic of the\nmodern Haskell type system allows us to remove the overhead and the syntactic fuzz of the\nscaffolding in most situations.  We start by declaring a data structure named  UserT . As a matter of convention, Beam table types\nare suffixed with 'T'. Table types have only one constructor. Again, as a matter of convention, the\nconstructor has the same name as the table, but without the 'T' suffix. We'll soon see the reason\nfor this convention.  In this tutorial, I'll prefix all record selectors with an underscore. This is a matter of personal\npreference. One reason for the prefix is that it plays nicely with the  lens  library. Beam does not\nnecessitate the use of  lens  (in fact Beam includes its own mechanism to generically derive van\nLaarhoven lenses), but I recognize that some programmers use  lens  quite a lot.  data   UserT   f   =   User \n              {   _userEmail       ::   Columnar   f   Text \n              ,   _userFirstName   ::   Columnar   f   Text \n              ,   _userLastName    ::   Columnar   f   Text \n              ,   _userPassword    ::   Columnar   f   Text   } \n               deriving   Generic   This data type might look very complicated, so I'd like to show you that it's not that scary. Let's\nsee if we can use GHCi to help us.  *BasicTutorial  :t User\nUser\n :: Columnar f Text\n    -  Columnar f Text -  Columnar f Text -  Columnar f Text -  UserT f  Hmm... That did not help much. However, consider the type of the following:  *BasicTutorial  :t (\\email firstName lastName password -  User email firstName lastName password :: UserT Identity)\n (\\email firstName lastName password -  User email firstName lastName password :: UserT Identity)\n :: Text -  Text -  Text -  Text -  UserT Identity  Woah! That looks a lot like what we'd expect if we had declared the type in the \"regular\" Haskell way:  data   User   =   User \n           {   _userEmail       ::   Text \n           ,   _userFirstName   ::   Text \n           ,   _userLastName    ::   Text \n           ,   _userPassword    ::   Text   }   This functionality is due to the fact that  Columnar  is a type family defined such that for any x ,  Columnar Identity x = x . Knowing this, let's define a type synonym to make our life easier.  type   User   =   UserT   Identity   Now you can see why we named the type of the table  UserT  and its constructor  User . This allows\nus to use the \"regular\"  User  constructor to construct values of type  User . We can use the StandaloneDeriving  extension to derive an instance of  Show  and  Eq  for the 'regular' datatype.  deriving   instance   Show   User  deriving   instance   Eq   User", 
            "title": "Defining our first table"
        }, 
        {
            "location": "/tutorials/tutorial1/#teaching-beam-about-our-table", 
            "text": "We've defined a type that can represent our the data in our table. Now, let's inform beam that we'd like to\nuse `UserT' as a table.  instance   Table   UserT   where   All beam tables need to implement the  Table  type class. The  Table  type class contains functions\nfor marshalling values between SQL and Haskell and for allowing the table to be queried. Due to\nGHC's DeriveGeneric and DefaultSignatures extensions, all these methods can be written for us by the\ncompiler at compile-time!  The only thing we need to provide is the type of the primary keys for users, and a function that can\nextract the primary key from any  UserT f  object. To do this, we just add the following lines to\nthe instance declaration.       data   PrimaryKey   UserT   f   =   UserId   ( Columnar   f   Text )   deriving   Generic \n     primaryKey   =   UserId   $   _userEmail   We use the applicative  $  operator to simplify our definiton for primaryKey , relying on the applicative instance for  (a - ) .  type   UserId   =   PrimaryKey   UserT   Identity  deriving   instance   Show   UserId  deriving   instance   Eq   UserId", 
            "title": "Teaching Beam about our table"
        }, 
        {
            "location": "/tutorials/tutorial1/#defining-our-database", 
            "text": "Now that we have our table, we're going to define a type to hold information about our\ndatabase. Defining our database is going to follow the same pattern as defining a table. We'll\ndefine a higher-kinded datatype and then declare an instance of  Database , and let the compiler\nfigure most of it out.  A database is made up of a collection of 'entities', of which tables are just one type.  data   ShoppingCartDb   f   =   ShoppingCartDb \n                       {   _shoppingCartUsers   ::   f   UserT   } \n                         deriving   Generic  instance   Database   ShoppingCartDb   The next step is to create a description of the particular database we'd like to create. This\ninvolves giving each of the tables in our database a name. If you've named all your database\nselectors using camel case, beam can automatically figure out what all the table names should be. If\nyou haven't, or you have multiple tables holding the same type in your database, you might have to\nmanually name your tables. For now, we'll let beam do the hard work.  shoppingCartDb   ::   DatabaseSettings   ShoppingCartDb  shoppingCartDb   =   autoDbSettings", 
            "title": "Defining our database"
        }, 
        {
            "location": "/tutorials/tutorial1/#adding-users-to-our-database", 
            "text": "Let's add some users to our database. First, we'll open a connection to a SQLite3 database using\nbeam.  The  openDatabase  function will open the database and optionally attempt to make the\nschema of the opened database match the schema implied by the data types. Below we use the openDatabaseDebug  function to put beam into debug mode. This will cause beam to log every sql\nstatement executed.  openDatabaseDebug  and  openDatabase  have the same type signature, so they can\nbe used interchangeably. In the call below, we pass in  AutoMigrate  to have beam automatically\nattempt to match up the real database schema with what it would expect based on the Haskell types.  main   ::   IO   ()  main   =   do   beam   -   openDatabaseDebug   shoppingCartDb   AutoMigrate   ( Sqlite3Settings   shoppingcart1.db )   To make sure that beam correctly inferred the database schema, let's dump it.             dumpSchema   shoppingCartDb   This will dump the SQL CREATE TABLE statements to the console.  Dumping database schema ...\nCREATE TABLE cart_users (email VARCHAR NOT NULL, first_name VARCHAR NOT NULL, last_name VARCHAR NOT NULL, password VARCHAR NOT NULL, PRIMARY KEY( email ))  Beam automatically converted our  UserT  table in the  _shoppingCartUsers  selector to a table named cart_users . As we stated above, this comes from un-CamelCase-ing the selector name and dropping\nthe first component. Beam followed the same rule to derive the column names.  Now let's add a few users. We'll give each user an MD5 encoded password too.             beamTxn   beam   $   \\ ( ShoppingCartDb   usersT )   - \n            do   insertInto   usersT   ( User   james@example.com   James   Smith   b4cc344d25a2efe540adbf2678e2304c   {- james -} ) \n               insertInto   usersT   ( User   betty@example.com   Betty   Jones   82b054bd83ffad9b6cf8bdb98ce3cc2f   {- betty -} ) \n               insertInto   usersT   ( User   sam@example.com   Sam   Taylor   332532dcfaa1cbf61e2a266bd723612c   {- sam -} )   The  insertInto  function inserts a value into a table that can hold that type.  Because we're in debug mode, we'll see the SQL that beam is running:  Will execute INSERT INTO cart_users VALUES (?, ?, ?, ?) with [SqlString  james@example.com ,SqlString  James ,SqlString  Smith ,SqlString  b4cc344d25a2efe540adbf2678e2304c ]\nWill execute INSERT INTO cart_users VALUES (?, ?, ?, ?) with [SqlString  betty@example.com ,SqlString  Betty ,SqlString  Jones ,SqlString  82b054bd83ffad9b6cf8bdb98ce3cc2f ]\nWill execute INSERT INTO cart_users VALUES (?, ?, ?, ?) with [SqlString  sam@example.com ,SqlString  Sam ,SqlString  Taylor ,SqlString  332532dcfaa1cbf61e2a266bd723612c ]", 
            "title": "Adding users to our database"
        }, 
        {
            "location": "/tutorials/tutorial1/#querying-the-database", 
            "text": "Now let's write some queries for the database. You can think of queries in Beam as similar to lists.\nLike lists you can filter them (SQL WHERE clauses), take only a certain number of items from them\n(SQL LIMIT clauses), and drop a certain number of items from them (SQL OFFSET clauses).  Let's look at a simple example. Let's get all users.             putStrLn   all users:  \n           Success   allUsers   -   beamTxn   beam   $   \\ ( ShoppingCartDb   usersT )   - \n                               queryList   ( all_   usersT ) \n           mapM_   ( putStrLn   .   show )   allUsers \n           putStrLn   ---- \\n\\n   When run, you should get output like the following:  Will execute SELECT `t0`.`email`, `t0`.`first_name`, `t0`.`last_name`, `t0`.`password` FROM  cart_users AS t0 with []\nUser {_userEmail =  james@example.com , _userFirstName =  James , _userLastName =  Smith , _userPassword =  b4cc344d25a2efe540adbf2678e2304c }\nUser {_userEmail =  betty@example.com , _userFirstName =  Betty , _userLastName =  Jones , _userPassword =  82b054bd83ffad9b6cf8bdb98ce3cc2f }\nUser {_userEmail =  sam@example.com , _userFirstName =  Sam , _userLastName =  Taylor , _userPassword =  332532dcfaa1cbf61e2a266bd723612c }  Next let's suppose you wanted to sort the users into order by their first name. We can use the orderBy  function to order the query results. This is similar to the  sortBy  function for lists.             putStrLn   users sorted by first name: \n           Success   sortedUsersByFirstName   -   beamTxn   beam   $   \\ ( ShoppingCartDb   usersT )   - \n                                             queryList   $   orderBy   ( asc_   .   _userFirstName )   ( all_   usersT ) \n           mapM_   ( putStrLn   .   show )   sortedUsersByFirstName \n           putStrLn   ---- \\n\\n   The corresponding output:  Will execute SELECT `t0`.`email`, `t0`.`first_name`, `t0`.`last_name`, `t0`.`password` FROM  cart_users AS t0 ORDER BY `t0`.`first_name` ASC with []\nUser {_userEmail =  betty@example.com , _userFirstName =  Betty , _userLastName =  Jones , _userPassword =  82b054bd83ffad9b6cf8bdb98ce3cc2f }\nUser {_userEmail =  james@example.com , _userFirstName =  James , _userLastName =  Smith , _userPassword =  b4cc344d25a2efe540adbf2678e2304c }\nUser {_userEmail =  sam@example.com , _userFirstName =  Sam , _userLastName =  Taylor , _userPassword =  332532dcfaa1cbf61e2a266bd723612c }  We can also sort by more than one column.             putStrLn   users sorted by first and last name: \n           Success   sortedUsersByFirstAndLastName   -   beamTxn   beam   $   \\ ( ShoppingCartDb   usersT )   - \n                                                    queryList   $   orderBy   ( \\ user   -   ( asc_   ( _userFirstName   user ),   asc_   ( _userLastName   user )))   ( all_   usersT ) \n           mapM_   ( putStrLn   .   show )   sortedUsersByFirstAndLastName \n           putStrLn   ---- \\n\\n   Again, the output for convenience:  Will execute SELECT `t0`.`email`, `t0`.`first_name`, `t0`.`last_name`, `t0`.`password` FROM  cart_users AS t0 ORDER BY `t0`.`first_name` ASC, `t0`.`last_name` ASC with []\nUser {_userEmail =  betty@example.com , _userFirstName =  Betty , _userLastName =  Jones , _userPassword =  82b054bd83ffad9b6cf8bdb98ce3cc2f }\nUser {_userEmail =  james@example.com , _userFirstName =  James , _userLastName =  Smith , _userPassword =  b4cc344d25a2efe540adbf2678e2304c }\nUser {_userEmail =  sam@example.com , _userFirstName =  Sam , _userLastName =  Taylor , _userPassword =  332532dcfaa1cbf61e2a266bd723612c }  We can use  limit_  and  offset_  in a similar manner to  take  and  drop  respectively.             putStrLn   the second user based on their first names \n           Success   [ secondUser ]   -   beamTxn   beam   $   \\ ( ShoppingCartDb   usersT )   - \n                                   queryList   $   limit_   1   ( offset_   1   ( orderBy   ( asc_   .   _userFirstName )   ( all_   usersT ))) \n           putStrLn   ( show   secondUser ) \n           putStrLn   ---- \\n\\n   And the output:  Will execute SELECT `t0`.`email`, `t0`.`first_name`, `t0`.`last_name`, `t0`.`password` FROM  cart_users AS t0 ORDER BY `t0`.`first_name` ASC LIMIT 1 OFFSET 1 with []\nUser {_userEmail =  james@example.com , _userFirstName =  James , _userLastName =  Smith , _userPassword =  b4cc344d25a2efe540adbf2678e2304c }", 
            "title": "Querying the database"
        }, 
        {
            "location": "/tutorials/tutorial1/#aggregations", 
            "text": "Sometimes we also want to group our data together and perform calculations over the groups of data. SQL calls these aggregations.  The simplest aggregation is counting. We use the  aggregate  function to create aggregations.             putStrLn   The total number of users \n           Success   [ userCount ]   -   beamTxn   beam   $   \\ ( ShoppingCartDb   usersT )   - \n                                  queryList   $   aggregate   ( \\ user   -   count_   ( _userEmail   user ))   ( all_   usersT ) \n           putStrLn   ( show   userCount ) \n           putStrLn   ---- \\n\\n   Maybe we'd like something a little more interesting, such as the number of users for each unique\nfirst name. We can also express these aggregations using the  aggregate  function. In order to get\ninteresting results, we'll need to add more users to our database.             beamTxn   beam   $   \\ ( ShoppingCartDb   usersT )   - \n            do   insertInto   usersT   ( User   james@pallo.com   James   Pallo   b4cc344d25a2efe540adbf2678e2304c   {- james -} ) \n               insertInto   usersT   ( User   betty@sims.com   Betty   Sims   82b054bd83ffad9b6cf8bdb98ce3cc2f   {- betty -} ) \n               insertInto   usersT   ( User   james@oreily.com   James   O Reily   b4cc344d25a2efe540adbf2678e2304c   {- james -} ) \n               insertInto   usersT   ( User   sam@sophitz.com   Sam   Sophitz   332532dcfaa1cbf61e2a266bd723612c   {- sam -} ) \n               insertInto   usersT   ( User   sam@jely.com   Sam   Jely   332532dcfaa1cbf61e2a266bd723612c   {- sam -} )   Now we can use  aggregate  to both group by a user's first name, and then count the number of users.             putStrLn   The number of users for each name \n           Success   countsByFirstName   -   beamTxn   beam   $   \\ ( ShoppingCartDb   usersT )   - \n                                        queryList   $ \n                                        aggregate   ( \\ user   -   ( group_   ( _userFirstName   user ),   count_   ( _userEmail   user )))   $ \n                                        all_   usersT \n           mapM   ( putStrLn   .   show )   countsByFirstName \n           putStrLn   ---- \\n\\n   Beam produces a very reasonable looking SQL statement, and returns the correct results.  The number of users for each name\nWill execute SELECT `t0`.`first_name`, COUNT(`t0`.`email`) FROM  cart_users AS t0 GROUP BY `t0`.`first_name` with []\n( Betty ,2)\n( James ,3)\n( Sam ,3)\n----", 
            "title": "Aggregations"
        }, 
        {
            "location": "/tutorials/tutorial1/#conclusion", 
            "text": "In this tutorial, we've covered creating a database schema, opening up a beam database, inserting\nvalues into the database, and querying values from them. We used the knowledge we learned to create\na partial shopping cart database that contains information about users. In the next tutorial, we'll\ndelve deeper into the some of the query types and show how we can create relations between\ntables. We'll also use the monadic query interface to create SQL joins.  Until next time!  If you have any questions about beam, feel free to send them to travis@athougies.net . Pull requests and bug reports are welcome on  GitHub .", 
            "title": "Conclusion"
        }, 
        {
            "location": "/tutorials/tutorial2/", 
            "text": "", 
            "title": "Part 2"
        }, 
        {
            "location": "/tutorials/tutorial3/", 
            "text": "", 
            "title": "Part 3"
        }, 
        {
            "location": "/user-guide/getting-started/", 
            "text": "Beam is made up of several Haskell packages. The main package is \nbeam-core\n,\nwhich provides core type definitions and common combinators for SQL. It also\nprovides a basic interface for using backends in a generic way. Beam also comes\nwith some default backends. Backends for other RDBMSs are available on Hackage\nand GitHub.\n\n\nGetting Started", 
            "title": "Getting Started"
        }, 
        {
            "location": "/user-guide/getting-started/#getting-started", 
            "text": "", 
            "title": "Getting Started"
        }, 
        {
            "location": "/user-guide/models/", 
            "text": "Beam allows you to define models for your data that can be used across multiple\nbackends. Beam models are standard Haskell data structures, but with some extra\nfeatures that allow Beam to introspect them at run-time.\n\n\nA Simple Model\n\n\nLet's define a simple model to represent a person. Open up a file named\n\nSchema.hs\n and add the following.\n\n\n{-# LANGUAGE GADTs #-}\n\n\n{-# LANGUAGE DeriveGeneric #-}\n\n\nmodule\n \nSchema\n \nwhere\n\n\n\nimport\n \nDatabase.Beam\n\n\nimport\n \nDatabase.Beam.SQLite\n\n\nimport\n \nDatabase.SQLite.Simple\n\n\n\nimport\n \nData.Text\n \n(\nText\n)\n\n\n\ndata\n \nPersonT\n \nf\n\n    \n=\n \nPerson\n\n    \n{\n \npersonFirstName\n \n::\n \nColumnar\n \nf\n \nText\n\n    \n,\n \npersonLastName\n  \n::\n \nColumnar\n \nf\n \nText\n\n    \n,\n \npersonAge\n       \n::\n \nColumnar\n \nf\n \nInt\n\n    \n}\n \nderiving\n \nGeneric\n\n\ninstance\n \nBeamable\n \nPersonT\n\n\n\n\n\n\nBeam also requires that your tables have a primary key that can be used to\nuniquely identify each tuple in your relation. We tell beam about the primary\nkey by implementing the \nTable\n type class for your table.\n\n\ninstance\n \nTable\n \nPersonT\n \nwhere\n\n  \ndata\n \nPrimaryKey\n \nPersonT\n \nf\n\n      \n=\n \nPersonKey\n\n      \n{\n \npersonKeyFirstName\n \n::\n \nColumnar\n \nf\n \nText\n\n      \n,\n \npersonKeyLastName\n  \n::\n \nColumnar\n \nf\n \nText\n\n      \n}\n \nderiving\n \nGeneric\n\n  \nprimaryKey\n \nperson\n \n=\n \nPersonKey\n \n(\npersonFirstName\n \nperson\n)\n \n(\npersonLastName\n \nperson\n)\n\n\n\n\n\n\n\n\nNote\n\n\nUsing the first and last name as a primary key is a bad idea, we use it here\nto illustrate using multiple fields as the primary key.\n\n\n\n\n\n\nTip\n\n\nMany people find it useful to use the \nApplicative\n instance for \n(-\n) a\n to\nwrite \nprimaryKey\n. For example, we could have written the above \nprimaryKey\nperson = PersonKey (personFirstName person) (personLastName person)\n as\n\nprimaryKey = PersonKey \n$\n personFirstName \n*\n personLastName\n.\n\n\n\n\nFor ease-of-use purposes we define some type synonyms for \nPersonT\n and\n\nPrimaryKey PersonT\n and some convenient instances. These are not strictly\nrequired but make working with these tables much easier.\n\n\ntype\n \nPerson\n \n=\n \nPersonT\n \nIdentity\n\n\ntype\n \nPersonKey\n \n=\n \nPrimaryKey\n \nPersonT\n \nIdentity\n\n\nderiving\n \ninstance\n \nShow\n \nPerson\n;\n \nderiving\n \ninstance\n \nEq\n \nPerson\n\n\nderiving\n \ninstance\n \nShow\n \nPersonKey\n;\n \nderiving\n \ninstance\n \nEq\n \nPersonKey\n\n\n\n\n\n\nDue to the magic of the \nColumnar\n type family, the \nPerson\n type can\nbe thought of as having the following definition.\n\n\ndata\n \nPerson\n\n    \n=\n \nPerson\n\n    \n{\n \npersonFirstName\n \n::\n \nText\n\n    \n,\n \npersonLastName\n  \n::\n \nText\n\n    \n,\n \npersonAge\n       \n::\n \nInt\n\n    \n}\n \nderiving\n \n(\nShow\n,\n \nEq\n)\n\n\n\n\n\n\nThis allows us to use your type definitions for Beam as regular\nHaskell data structures without wrapping/unwrapping.\n\n\n\n\nTip\n\n\nTyping \nColumnar\n may become tiresome. \nDatabase.Beam\n also exports \nC\n as a\ntype alias for \nColumnar\n, which may make writing models easier. Since \nC\n\nmay cause name clashes, all examples are given using \nColumnar\n.\n\n\n\n\nForeign references\n\n\nForeign references are also easily supported in models by simply\nembedding the \nPrimaryKey\n of the referred to table directly in the\nparent. For example, suppose we want to create a new model\nrepresenting a post by a user.\n\n\ndata\n \nPostT\n \nf\n\n    \n=\n \nPost\n\n    \n{\n \npostId\n       \n::\n \nColumnar\n \nf\n \n(\nAuto\n \nInt\n)\n\n    \n,\n \npostPostedAt\n \n::\n \nColumnar\n \nf\n \nLocalTime\n\n    \n,\n \npostContent\n  \n::\n \nColumnar\n \nf\n \nText\n\n    \n,\n \npostPoster\n   \n::\n \nPrimaryKey\n \nPersonT\n \nf\n\n    \n}\n \nderiving\n \nGeneric\n\n\ninstance\n \nBeamable\n \nPostT\n\n\n\ninstance\n \nTable\n \nPostT\n \nwhere\n\n  \ndata\n \nPrimaryKey\n \nPostT\n \nf\n\n      \n=\n \nPostId\n \n(\nColumnar\n \nf\n \n(\nAuto\n \nInt\n))\n \nderiving\n \nGeneric\n\n  \nprimaryKey\n \n=\n \nPostId\n \n.\n \npostId\n\n\n\ntype\n \nPost\n \n=\n \nPostT\n \nIdentity\n\n\ntype\n \nPostId\n \n=\n \nPrimaryKey\n \nPostT\n \nIdentity\n\n\nderiving\n \ninstance\n \nShow\n \nPost\n;\n \nderiving\n \ninstance\n \nEq\n \nPost\n\n\nderiving\n \ninstance\n \nShow\n \nPostId\n;\n \nderiving\n \ninstance\n \nEq\n \nPostId\n\n\n\n\n\n\nThe \nAuto\n type constructor is provided by \nbeam-core\n for fields that\nare automatically assigned by the database. Internally, \nAuto x\n is\nsimply a newtype over \nMaybe x\n. The guarantee is that all values of\ntype \nAuto x\n returned by beam in the result set will have a value,\nalthough this guarantee is not enforced at the type level (yet).\n\n\nEmbedding\n\n\nSometimes, we want to declare multiple models with fields in common. Beam allows\nyou to simple embed such fields in common types and embed those directly into\nmodels. For example, in the Chinook example schema (see\n\nbeam-sqlite/examples/Chinook/Schema.hs\n), we define the following structure for\naddresses.\n\n\ndata\n \nAddressMixin\n \nf\n\n  \n=\n \nAddress\n\n  \n{\n \naddress\n           \n::\n \nColumnar\n \nf\n \n(\nMaybe\n \nText\n)\n\n  \n,\n \naddressCity\n       \n::\n \nColumnar\n \nf\n \n(\nMaybe\n \nText\n)\n\n  \n,\n \naddressState\n      \n::\n \nColumnar\n \nf\n \n(\nMaybe\n \nText\n)\n\n  \n,\n \naddressCountry\n    \n::\n \nColumnar\n \nf\n \n(\nMaybe\n \nText\n)\n\n  \n,\n \naddressPostalCode\n \n::\n \nColumnar\n \nf\n \n(\nMaybe\n \nText\n)\n\n  \n}\n \nderiving\n \nGeneric\n\n\ninstance\n \nBeamable\n \nAddressMixin\n\n\ntype\n \nAddress\n \n=\n \nAddressMixin\n \nIdentity\n\n\nderiving\n \ninstance\n \nShow\n \n(\nAddressMixin\n \nIdentity\n)\n\n\n\n\n\n\nWe can then use \nAddressMixin\n in our models.\n\n\ndata\n \nEmployeeT\n \nf\n\n  \n=\n \nEmployee\n\n  \n{\n \nemployeeId\n        \n::\n \nColumnar\n \nf\n \nInt32\n\n  \n,\n \nemployeeLastName\n  \n::\n \nColumnar\n \nf\n \nText\n\n  \n,\n \nemployeeFirstName\n \n::\n \nColumnar\n \nf\n \nText\n\n  \n,\n \nemployeeTitle\n     \n::\n \nColumnar\n \nf\n \n(\nMaybe\n \nText\n)\n\n  \n,\n \nemployeeReportsTo\n \n::\n \nPrimaryKey\n \nEmployeeT\n \n(\nNullable\n \nf\n)\n\n  \n,\n \nemployeeBirthDate\n \n::\n \nColumnar\n \nf\n \n(\nMaybe\n \nLocalTime\n)\n\n  \n,\n \nemployeeHireDate\n  \n::\n \nColumnar\n \nf\n \n(\nMaybe\n \nLocalTime\n)\n\n  \n,\n \nemployeeAddress\n   \n::\n \nAddressMixin\n \nf\n\n  \n,\n \nemployeePhone\n     \n::\n \nColumnar\n \nf\n \n(\nMaybe\n \nText\n)\n\n  \n,\n \nemployeeFax\n       \n::\n \nColumnar\n \nf\n \n(\nMaybe\n \nText\n)\n\n  \n,\n \nemployeeEmail\n     \n::\n \nColumnar\n \nf\n \n(\nMaybe\n \nText\n)\n\n  \n}\n \nderiving\n \nGeneric\n\n\n-- ...\n\n\ndata\n \nCustomerT\n \nf\n\n  \n=\n \nCustomer\n\n  \n{\n \ncustomerId\n        \n::\n \nColumnar\n \nf\n \nInt32\n\n  \n,\n \ncustomerFirstName\n \n::\n \nColumnar\n \nf\n \nText\n\n  \n,\n \ncustomerLastName\n  \n::\n \nColumnar\n \nf\n \nText\n\n  \n,\n \ncustomerCompany\n   \n::\n \nColumnar\n \nf\n \n(\nMaybe\n \nText\n)\n\n  \n,\n \ncustomerAddress\n   \n::\n \nAddressMixin\n \nf\n\n  \n,\n \ncustomerPhone\n     \n::\n \nColumnar\n \nf\n \n(\nMaybe\n \nText\n)\n\n  \n,\n \ncustomerFax\n       \n::\n \nColumnar\n \nf\n \n(\nMaybe\n \nText\n)\n\n  \n,\n \ncustomerEmail\n     \n::\n \nColumnar\n \nf\n \nText\n\n  \n,\n \ncustomerSupportRep\n \n::\n \nPrimaryKey\n \nEmployeeT\n \n(\nNullable\n \nf\n)\n\n  \n}\n \nderiving\n \nGeneric\n\n\n\n\n\n\nDefaults\n\n\nBased on your data type declarations, beam can already guess a lot\nabout your tables. For example, it already assumes that the\n\npersonFirstName\n field is accessible in SQL as \nfirst_name\n. This\ndefaulting behavior makes it very easy to interact with typical\ndatabases.\n\n\nFor the easiest user experience, it's best to follow beam's\nconventions for declaring models. In particular, the defaulting\nmechanisms rely on each table type declaring only one constructor\nwhich has fields named in the camelCase style.\n\n\nWhen defaulting the name of a table field or column, beam\nun-camelCases the field name (after dropping leading underscores) and\ndrops the first word. The remaining words are joined with\nunderscores. If there is only one component, it is not\ndropped. Trailing and internal underscores are preserved in the name\nand if the name consists solely of underscores, beam makes no\nchanges. A summary of these rules is given in the table below.\n\n\n\n\n\n\n\n\nHaskell field name\n\n\nBeam defaulted column name\n\n\n\n\n\n\n\n\n\n\npersonFirstName\n\n\nfirst_name\n\n\n\n\n\n\n_personLastName\n\n\nlast_name\n\n\n\n\n\n\nname\n\n\nname\n\n\n\n\n\n\nfirst_name\n\n\nfirst_name\n\n\n\n\n\n\n_first_name\n\n\nfirst_name\n\n\n\n\n\n\n___\n (three underscores)\n\n\n___\n (no changes)\n\n\n\n\n\n\n\n\nNote that beam only uses lower case in field names. While typically\ncase does not matter for SQL queries, beam always quotes\nidentifiers. Many DBMS's are case-sensitive for quoted\nidentifiers. Thus, queries can sometimes fail if your tables use\nmixtures of lower- and upper-case to distinguish between fields.\n\n\nAll of these defaults can be overriden using the modifications system,\ndescribed in the \nmodel reference\n.\n\n\nWhat about tables without primary keys?\n\n\nTables without primary keys are considered bad style. However,\nsometimes you need to use beam with a schema that you have no control\nover. To declare a table without a primary key, simply instantiate the\n\nTable\n class without overriding the defaults.\n\n\nMore complicated relationships\n\n\nThis is the extent of beam's support for defining models. Although\nsimilar packages in other languages provide support for declaring\none-to-many, many-to-one, and many-to-many relationships, beam's\nfocused is providing a direct mapping of relational database concepts\nto Haskell, not on abstracting away the complexities of database\nquerying. Thus, beam does not use 'lazy-loading' or other tricks that\nobfuscate performance. Because of this, the bulk of the functionality\ndealing with different types of relations is found in the querying\nsupport, rather than in the model declarations.\n\n\nPutting a Database Together\n\n\nBeam also requires you to give a type for your database. The database type\ncontains all the entities (tables or otherwise) that would be present in your\ndatabase. Our database only has one table right now, so it only contains one\nfield.\n\n\ndata\n \nExampleDb\n \nf\n\n    \n=\n \nExampleDb\n\n    \n{\n \npersons\n \n::\n \nf\n \n(\nTableEntity\n \nPersonT\n)\n\n    \n}\n \nderiving\n \nGeneric\n\n\ninstance\n \nDatabase\n \nExampleDb\n\n\n\nexampleDb\n \n::\n \nDatabaseSettings\n \nbe\n \nExampleDb\n\n\nexampleDb\n \n=\n \nautoDbSettings\n\n\n\n\n\n\nUsing your database\n\n\nLet's open up a SQLite database. Open up \nghci\n and import your module.\n\n\nPrelude\n \n:\nload Schema.hs\nPrelude Schema\n conn \n-\n open \nbeam-manual.db\n\n\n\n\n\n\nA quick note on backends\n\n\nBeam is backend-agnostic and doesn't provide any means to connect to a\ndatabase. Beam backend libraries usually use well-used Haskell\nlibraries to provide database connectivity. For example, the\n\nbeam-sqlite\n backend uses the \nsqlite-simple\n backend.\n\n\nBeam distinguishes each backend via type indexes. Each backend defines\na type that is used to enable backend-specific behavior. For example,\nthe \nbeam-sqlite\n backend ships with the \nSqlite\n type that is used to\ndistinguish sqlite specific constructs with generic or other\nbackend-specific ones.\n\n\nEach backend can have one or more 'syntaxes', which are particular\nways to query the database. While the \nbeam-core\n library ships with a\nstandard ANSI SQL builder, few real-world database implementations\nfully follow the standard. Most backends use their own custom syntax\ntype. Internally, beam uses a finally-tagless representation for\nsyntax trees that allow straightforward construction against any\nbackend.\n\n\nBeam offers backend-generic functions for the most common operations\nagainst databases. These functions are meant to fit the lowest common\ndenominator. For example, no control is offered over streaming results\nfrom SELECT statements. While these backend-generic functions are\nuseful for ad-hoc querying and development, it is wisest to use\nbackend-specific functions in production for maximum control. Refer to\nbackend-specific documentation for more information.\n\n\nFor our examples, we will use the \nbeam-sqlite\n backend and demonstrate\nusage of the beam standard query functions.\n\n\nInserting data\n\n\nFirst, let's connect to a sqlite database, and create our schema. The\n\nbeam-core\n does not offer any support for the SQL DDL language. There\nis a separate core library \nbeam-migrate\n that offers complete support\nfor ANSI-standard SQL DDL operations, as well as tools to manipulate\ndatabase schemas. See the section on migrations for more information.\n\n\nFor our example, we will simply issue a \nCREATE TABLE\n command\ndirectly against the database using \nsqlite-simple\n functionality:\n\n\nPrelude Schema\n execute_ conn \nCREATE TABLE persons ( first_name TEXT NOT NULL, last_name TEXT NOT NULL, age INT NOT NULL, PRIMARY KEY(first_name, last_name) )\n\n\n\n\n\n\nNow we can insert some data into our database. Beam ships with a\nfunction \nwithDatabase\n, with the following signature:\n\n\nwithDatabase\n \n::\n \nMonadBeam\n \nsyntax\n \nbe\n \nm\n \n=\n \nDbHandle\n \nbe\n \n-\n \nm\n \na\n \n-\n \nIO\n \na\n\n\n\n\n\n\nDbHandle be\n is a type family that refers to a backend-specific type\nfor referring to a particular database connection. For the\n\nbeam-sqlite\n backend \nDbHandle Sqlite ~\nDatabase.Sqlite.Simple.Connection\n. \n\n\nMonadBeam\n is a type class relating a particular syntax and backend\nto a monad we can use to execute data query and manipulation commands.\n\n\nLet's insert some data into our database. We are going to use the\n\nrunInsert\n function from \nMonadBeam\n.\n\n\nPrelude Schema\n :{\nPrelude Schema| withDatabase conn $ do\nPrelude Schema|   runInsert $ insert (persons exampleDb) $\nPrelude Schema|               insertValues [ Person \nBob\n \nSmith\n 50\nPrelude Schema|                            , Person \nAlice\n \nWong\n 55\nPrelude Schema|                            , Person \nJohn\n \nQuincy\n 30 ]\nPrelude Schema| :}\n\n\n\n\n\nThe \nrunInsert\n function has the type signature\n\n\nrunInsert\n \n::\n \nMonadBeam\n \nsyntax\n \nbe\n \nm\n \n=\n \nSqlInsert\n \nsyntax\n \n-\n \nm\n \n()\n\n\n\n\n\n\nSqlInsert syntax\n represents a SQL \nINSERT\n command in the given\n\nsyntax\n. We construct this value using the \ninsert\n function from\n\nDatabase.Beam.Query\n.\n\n\ninsert\n \n::\n \nIsSql92InsertSyntax\n \nsyntax\n \n=\n\n          \nDatabaseEntity\n \nbe\n \ndb\n \n(\nTableEntity\n \ntable\n)\n\n       \n-\n \nSql92InsertValuesSyntax\n \nsyntax\n\n       \n-\n \nSqlInsert\n \nsyntax\n\n\n\n\n\n\nIntuitively, \ninsert\n takes a database table descriptor and some\nvalues (particular to the given syntax) and returns a statement to\ninsert these values. \nSql92InsertValuesSyntax syntax\n always\nimplements the \nIsSql92InsertValuesSyntax\n typeclass, which is where\nwe get the \ninsertValues\n function from. \nIsSql92InsertValuesSyntax\n\nalso defines the \ninsertSelect\n function for inserting values from the\nresult of a \nSELECT\n statement. Other backends may provide other ways\nof specifying the source of values. This brings us to another point\n\n\nPrelude Schema\n runSelect (select (all_ (persons exampleDb)))\n[ Person { personFirstName = \nBob\n, personLastName=\nSmith\n, personAge=50 }, ... ]", 
            "title": "Models"
        }, 
        {
            "location": "/user-guide/models/#a-simple-model", 
            "text": "Let's define a simple model to represent a person. Open up a file named Schema.hs  and add the following.  {-# LANGUAGE GADTs #-}  {-# LANGUAGE DeriveGeneric #-}  module   Schema   where  import   Database.Beam  import   Database.Beam.SQLite  import   Database.SQLite.Simple  import   Data.Text   ( Text )  data   PersonT   f \n     =   Person \n     {   personFirstName   ::   Columnar   f   Text \n     ,   personLastName    ::   Columnar   f   Text \n     ,   personAge         ::   Columnar   f   Int \n     }   deriving   Generic  instance   Beamable   PersonT   Beam also requires that your tables have a primary key that can be used to\nuniquely identify each tuple in your relation. We tell beam about the primary\nkey by implementing the  Table  type class for your table.  instance   Table   PersonT   where \n   data   PrimaryKey   PersonT   f \n       =   PersonKey \n       {   personKeyFirstName   ::   Columnar   f   Text \n       ,   personKeyLastName    ::   Columnar   f   Text \n       }   deriving   Generic \n   primaryKey   person   =   PersonKey   ( personFirstName   person )   ( personLastName   person )    Note  Using the first and last name as a primary key is a bad idea, we use it here\nto illustrate using multiple fields as the primary key.    Tip  Many people find it useful to use the  Applicative  instance for  (- ) a  to\nwrite  primaryKey . For example, we could have written the above  primaryKey\nperson = PersonKey (personFirstName person) (personLastName person)  as primaryKey = PersonKey  $  personFirstName  *  personLastName .   For ease-of-use purposes we define some type synonyms for  PersonT  and PrimaryKey PersonT  and some convenient instances. These are not strictly\nrequired but make working with these tables much easier.  type   Person   =   PersonT   Identity  type   PersonKey   =   PrimaryKey   PersonT   Identity  deriving   instance   Show   Person ;   deriving   instance   Eq   Person  deriving   instance   Show   PersonKey ;   deriving   instance   Eq   PersonKey   Due to the magic of the  Columnar  type family, the  Person  type can\nbe thought of as having the following definition.  data   Person \n     =   Person \n     {   personFirstName   ::   Text \n     ,   personLastName    ::   Text \n     ,   personAge         ::   Int \n     }   deriving   ( Show ,   Eq )   This allows us to use your type definitions for Beam as regular\nHaskell data structures without wrapping/unwrapping.   Tip  Typing  Columnar  may become tiresome.  Database.Beam  also exports  C  as a\ntype alias for  Columnar , which may make writing models easier. Since  C \nmay cause name clashes, all examples are given using  Columnar .", 
            "title": "A Simple Model"
        }, 
        {
            "location": "/user-guide/models/#foreign-references", 
            "text": "Foreign references are also easily supported in models by simply\nembedding the  PrimaryKey  of the referred to table directly in the\nparent. For example, suppose we want to create a new model\nrepresenting a post by a user.  data   PostT   f \n     =   Post \n     {   postId         ::   Columnar   f   ( Auto   Int ) \n     ,   postPostedAt   ::   Columnar   f   LocalTime \n     ,   postContent    ::   Columnar   f   Text \n     ,   postPoster     ::   PrimaryKey   PersonT   f \n     }   deriving   Generic  instance   Beamable   PostT  instance   Table   PostT   where \n   data   PrimaryKey   PostT   f \n       =   PostId   ( Columnar   f   ( Auto   Int ))   deriving   Generic \n   primaryKey   =   PostId   .   postId  type   Post   =   PostT   Identity  type   PostId   =   PrimaryKey   PostT   Identity  deriving   instance   Show   Post ;   deriving   instance   Eq   Post  deriving   instance   Show   PostId ;   deriving   instance   Eq   PostId   The  Auto  type constructor is provided by  beam-core  for fields that\nare automatically assigned by the database. Internally,  Auto x  is\nsimply a newtype over  Maybe x . The guarantee is that all values of\ntype  Auto x  returned by beam in the result set will have a value,\nalthough this guarantee is not enforced at the type level (yet).", 
            "title": "Foreign references"
        }, 
        {
            "location": "/user-guide/models/#embedding", 
            "text": "Sometimes, we want to declare multiple models with fields in common. Beam allows\nyou to simple embed such fields in common types and embed those directly into\nmodels. For example, in the Chinook example schema (see beam-sqlite/examples/Chinook/Schema.hs ), we define the following structure for\naddresses.  data   AddressMixin   f \n   =   Address \n   {   address             ::   Columnar   f   ( Maybe   Text ) \n   ,   addressCity         ::   Columnar   f   ( Maybe   Text ) \n   ,   addressState        ::   Columnar   f   ( Maybe   Text ) \n   ,   addressCountry      ::   Columnar   f   ( Maybe   Text ) \n   ,   addressPostalCode   ::   Columnar   f   ( Maybe   Text ) \n   }   deriving   Generic  instance   Beamable   AddressMixin  type   Address   =   AddressMixin   Identity  deriving   instance   Show   ( AddressMixin   Identity )   We can then use  AddressMixin  in our models.  data   EmployeeT   f \n   =   Employee \n   {   employeeId          ::   Columnar   f   Int32 \n   ,   employeeLastName    ::   Columnar   f   Text \n   ,   employeeFirstName   ::   Columnar   f   Text \n   ,   employeeTitle       ::   Columnar   f   ( Maybe   Text ) \n   ,   employeeReportsTo   ::   PrimaryKey   EmployeeT   ( Nullable   f ) \n   ,   employeeBirthDate   ::   Columnar   f   ( Maybe   LocalTime ) \n   ,   employeeHireDate    ::   Columnar   f   ( Maybe   LocalTime ) \n   ,   employeeAddress     ::   AddressMixin   f \n   ,   employeePhone       ::   Columnar   f   ( Maybe   Text ) \n   ,   employeeFax         ::   Columnar   f   ( Maybe   Text ) \n   ,   employeeEmail       ::   Columnar   f   ( Maybe   Text ) \n   }   deriving   Generic  -- ...  data   CustomerT   f \n   =   Customer \n   {   customerId          ::   Columnar   f   Int32 \n   ,   customerFirstName   ::   Columnar   f   Text \n   ,   customerLastName    ::   Columnar   f   Text \n   ,   customerCompany     ::   Columnar   f   ( Maybe   Text ) \n   ,   customerAddress     ::   AddressMixin   f \n   ,   customerPhone       ::   Columnar   f   ( Maybe   Text ) \n   ,   customerFax         ::   Columnar   f   ( Maybe   Text ) \n   ,   customerEmail       ::   Columnar   f   Text \n   ,   customerSupportRep   ::   PrimaryKey   EmployeeT   ( Nullable   f ) \n   }   deriving   Generic", 
            "title": "Embedding"
        }, 
        {
            "location": "/user-guide/models/#defaults", 
            "text": "Based on your data type declarations, beam can already guess a lot\nabout your tables. For example, it already assumes that the personFirstName  field is accessible in SQL as  first_name . This\ndefaulting behavior makes it very easy to interact with typical\ndatabases.  For the easiest user experience, it's best to follow beam's\nconventions for declaring models. In particular, the defaulting\nmechanisms rely on each table type declaring only one constructor\nwhich has fields named in the camelCase style.  When defaulting the name of a table field or column, beam\nun-camelCases the field name (after dropping leading underscores) and\ndrops the first word. The remaining words are joined with\nunderscores. If there is only one component, it is not\ndropped. Trailing and internal underscores are preserved in the name\nand if the name consists solely of underscores, beam makes no\nchanges. A summary of these rules is given in the table below.     Haskell field name  Beam defaulted column name      personFirstName  first_name    _personLastName  last_name    name  name    first_name  first_name    _first_name  first_name    ___  (three underscores)  ___  (no changes)     Note that beam only uses lower case in field names. While typically\ncase does not matter for SQL queries, beam always quotes\nidentifiers. Many DBMS's are case-sensitive for quoted\nidentifiers. Thus, queries can sometimes fail if your tables use\nmixtures of lower- and upper-case to distinguish between fields.  All of these defaults can be overriden using the modifications system,\ndescribed in the  model reference .", 
            "title": "Defaults"
        }, 
        {
            "location": "/user-guide/models/#what-about-tables-without-primary-keys", 
            "text": "Tables without primary keys are considered bad style. However,\nsometimes you need to use beam with a schema that you have no control\nover. To declare a table without a primary key, simply instantiate the Table  class without overriding the defaults.", 
            "title": "What about tables without primary keys?"
        }, 
        {
            "location": "/user-guide/models/#more-complicated-relationships", 
            "text": "This is the extent of beam's support for defining models. Although\nsimilar packages in other languages provide support for declaring\none-to-many, many-to-one, and many-to-many relationships, beam's\nfocused is providing a direct mapping of relational database concepts\nto Haskell, not on abstracting away the complexities of database\nquerying. Thus, beam does not use 'lazy-loading' or other tricks that\nobfuscate performance. Because of this, the bulk of the functionality\ndealing with different types of relations is found in the querying\nsupport, rather than in the model declarations.", 
            "title": "More complicated relationships"
        }, 
        {
            "location": "/user-guide/models/#putting-a-database-together", 
            "text": "Beam also requires you to give a type for your database. The database type\ncontains all the entities (tables or otherwise) that would be present in your\ndatabase. Our database only has one table right now, so it only contains one\nfield.  data   ExampleDb   f \n     =   ExampleDb \n     {   persons   ::   f   ( TableEntity   PersonT ) \n     }   deriving   Generic  instance   Database   ExampleDb  exampleDb   ::   DatabaseSettings   be   ExampleDb  exampleDb   =   autoDbSettings", 
            "title": "Putting a Database Together"
        }, 
        {
            "location": "/user-guide/models/#using-your-database", 
            "text": "Let's open up a SQLite database. Open up  ghci  and import your module.  Prelude   : load Schema.hs\nPrelude Schema  conn  -  open  beam-manual.db", 
            "title": "Using your database"
        }, 
        {
            "location": "/user-guide/models/#a-quick-note-on-backends", 
            "text": "Beam is backend-agnostic and doesn't provide any means to connect to a\ndatabase. Beam backend libraries usually use well-used Haskell\nlibraries to provide database connectivity. For example, the beam-sqlite  backend uses the  sqlite-simple  backend.  Beam distinguishes each backend via type indexes. Each backend defines\na type that is used to enable backend-specific behavior. For example,\nthe  beam-sqlite  backend ships with the  Sqlite  type that is used to\ndistinguish sqlite specific constructs with generic or other\nbackend-specific ones.  Each backend can have one or more 'syntaxes', which are particular\nways to query the database. While the  beam-core  library ships with a\nstandard ANSI SQL builder, few real-world database implementations\nfully follow the standard. Most backends use their own custom syntax\ntype. Internally, beam uses a finally-tagless representation for\nsyntax trees that allow straightforward construction against any\nbackend.  Beam offers backend-generic functions for the most common operations\nagainst databases. These functions are meant to fit the lowest common\ndenominator. For example, no control is offered over streaming results\nfrom SELECT statements. While these backend-generic functions are\nuseful for ad-hoc querying and development, it is wisest to use\nbackend-specific functions in production for maximum control. Refer to\nbackend-specific documentation for more information.  For our examples, we will use the  beam-sqlite  backend and demonstrate\nusage of the beam standard query functions.", 
            "title": "A quick note on backends"
        }, 
        {
            "location": "/user-guide/models/#inserting-data", 
            "text": "First, let's connect to a sqlite database, and create our schema. The beam-core  does not offer any support for the SQL DDL language. There\nis a separate core library  beam-migrate  that offers complete support\nfor ANSI-standard SQL DDL operations, as well as tools to manipulate\ndatabase schemas. See the section on migrations for more information.  For our example, we will simply issue a  CREATE TABLE  command\ndirectly against the database using  sqlite-simple  functionality:  Prelude Schema  execute_ conn  CREATE TABLE persons ( first_name TEXT NOT NULL, last_name TEXT NOT NULL, age INT NOT NULL, PRIMARY KEY(first_name, last_name) )   Now we can insert some data into our database. Beam ships with a\nfunction  withDatabase , with the following signature:  withDatabase   ::   MonadBeam   syntax   be   m   =   DbHandle   be   -   m   a   -   IO   a   DbHandle be  is a type family that refers to a backend-specific type\nfor referring to a particular database connection. For the beam-sqlite  backend  DbHandle Sqlite ~\nDatabase.Sqlite.Simple.Connection .   MonadBeam  is a type class relating a particular syntax and backend\nto a monad we can use to execute data query and manipulation commands.  Let's insert some data into our database. We are going to use the runInsert  function from  MonadBeam .  Prelude Schema  :{\nPrelude Schema| withDatabase conn $ do\nPrelude Schema|   runInsert $ insert (persons exampleDb) $\nPrelude Schema|               insertValues [ Person  Bob   Smith  50\nPrelude Schema|                            , Person  Alice   Wong  55\nPrelude Schema|                            , Person  John   Quincy  30 ]\nPrelude Schema| :}  The  runInsert  function has the type signature  runInsert   ::   MonadBeam   syntax   be   m   =   SqlInsert   syntax   -   m   ()   SqlInsert syntax  represents a SQL  INSERT  command in the given syntax . We construct this value using the  insert  function from Database.Beam.Query .  insert   ::   IsSql92InsertSyntax   syntax   = \n           DatabaseEntity   be   db   ( TableEntity   table ) \n        -   Sql92InsertValuesSyntax   syntax \n        -   SqlInsert   syntax   Intuitively,  insert  takes a database table descriptor and some\nvalues (particular to the given syntax) and returns a statement to\ninsert these values.  Sql92InsertValuesSyntax syntax  always\nimplements the  IsSql92InsertValuesSyntax  typeclass, which is where\nwe get the  insertValues  function from.  IsSql92InsertValuesSyntax \nalso defines the  insertSelect  function for inserting values from the\nresult of a  SELECT  statement. Other backends may provide other ways\nof specifying the source of values. This brings us to another point  Prelude Schema  runSelect (select (all_ (persons exampleDb)))\n[ Person { personFirstName =  Bob , personLastName= Smith , personAge=50 }, ... ]", 
            "title": "Inserting data"
        }, 
        {
            "location": "/user-guide/databases/", 
            "text": "In addition to defining types for each of your tables, beam also\nrequires you to declare your database as a type with fields for\nholding all entities in your database. This includes more than just\ntables. For example, user-defined types that you would like to work\nwith must also be included in your database type.\n\n\nA simple database type\n\n\nLike tables, a database type takes a functor and applies it to each\nentity in the database. For example, a database type for the two\ntables defined above has the form.\n\n\ndata\n \nExampleDb\n \nf\n\n    \n=\n \nExampleDb\n\n    \n{\n \npersons\n \n::\n \nf\n \n(\nTableEntity\n \nPersonT\n)\n\n    \n,\n \nposts\n   \n::\n \nf\n \n(\nTableEntity\n \nPersonT\n)\n\n    \n}\n \nderiving\n \nGeneric\n\n\ninstance\n \nDatabase\n \nExampleDb\n\n\n\nexampleDb\n \n::\n \nDatabaseSettings\n \nbe\n \nExampleDb\n\n\nexampleDb\n \n=\n \ndefaultDbSettings\n\n\n\n\n\n\nOther database entities\n\n\nViews\n\n\n\n\nNote\n\n\nViews have not yet been implemented, but this is the expected syntax\n\n\n\n\nSome databases also offer the concept of 'views' -- pseudo-tables that\nare built from a pre-defined query. Suppose we wanted to create a view\nthat returned the latest comments and their respective posters.\n\n\ndata\n \nPostAndPosterView\n \nf\n\n    \n=\n \nPostAndPosterView\n\n    \n{\n \npost\n   \n::\n \nPostT\n \nf\n\n    \n,\n \nposter\n \n::\n \nPersonT\n \nf\n\n    \n}\n \nderiving\n \nGeneric\n\n\ninstance\n \nBeamable\n \nPostAndPosterView\n\n\n\n\n\n\nWe can include this in our database:\n\n\ndata\n \nExampleDb\n \nf\n\n    \n=\n \nExampleDb\n\n    \n{\n \npersons\n        \n::\n \nf\n \n(\nTableEntity\n \nPersonT\n)\n\n    \n,\n \nposts\n          \n::\n \nf\n \n(\nTableEntity\n \nPersonT\n)\n\n    \n,\n \npostAndPosters\n \n::\n \nf\n \n(\nViewEntity\n \nPostAndPosterView\n)\n\n    \n}\n \nderiving\n \nGeneric\n\n\n\n\n\n\nNow we can use \npostAndPosters\n wherever we'd use a table. Note that you do not\nneed to specify the definition of the view. The definition is not important to\naccess the view, so beam does not need to know about it at the type-level. If\nyou want to manipulate view definitions, use the migrations packgae.\n\n\nUnique constraints\n\n\n\n\nNote\n\n\nThis is the current implementation plan. Uniques are not currently implemented.\n\n\n\n\nThe \nTableEntityWithUnique\n database entity allows you to declare\ntables with additional uniqueness constraints (the primary key is\nconsidered to be unique by default).\n\n\nFor example, suppose you wanted to re-define the \nPersonT\n table with\nan additional unique e-mail and another unique phone column.\n\n\ndata\n \nPersonT\n \nf\n\n    \n=\n \nPerson\n\n    \n{\n \npersonFirstName\n \n::\n \nColumnar\n \nf\n \nText\n\n    \n,\n \npersonLastName\n  \n::\n \nColumnar\n \nf\n \nText\n\n    \n,\n \npersonAge\n       \n::\n \nColumnar\n \nf\n \nInt\n\n    \n,\n \npersonEmail\n     \n::\n \nColumnar\n \nf\n \nText\n\n    \n,\n \npersonPhone\n     \n::\n \nColumnar\n \nf\n \nText\n\n    \n}\n \nderiving\n \nGeneric\n\n\n\ndata\n \nPersonByEmail\n \nf\n \n=\n \nPersonByEmail\n \n(\nColumnar\n \nf\n \nText\n)\n\n\ndata\n \nPersonByPhone\n \nf\n \n=\n \nPersonByPhone\n \n(\nColumnar\n \nf\n \nText\n)\n\n\n\n\n\n\nNow, use \nTableEntityWithUnique\n to declare the table.\n\n\ndata\n \nExampleDb\n \nf\n\n    \n=\n \nExampleDb\n\n    \n{\n \npersons\n        \n::\n \nf\n \n(\nTableEntityWithUnique\n \nPersonT\n \n[PersonByEmail, PersonByPhone]\n)\n\n    \n,\n \nposts\n          \n::\n \nf\n \n(\nTableEntity\n \nPersonT\n)\n\n    \n,\n \npostAndPosters\n \n::\n \nf\n \n(\nViewEntity\n \nPostAndPosterView\n)\n\n    \n}\n \nderiving\n \nGeneric\n\n\n\n\n\n\nBeam will not complain about this definition, but you will need to\ndeclare additional instances in order to actually use the unique\nconstraints.\n\n\ninstance\n \nUnique\n \nPersonT\n \nPersonByEmail\n \nwhere\n\n  \nmkUnique\n \n=\n \nPersonByEmail\n \n.\n \npersonEmail\n\n\n\ninstance\n \nUnique\n \nPersonT\n \nPersonByPhone\n \nwhere\n\n  \nmkUnique\n \n=\n \nPersonByPhone\n \n.\n \npersonPhone\n\n\n\n\n\n\nTODO\n: Should the unique constraints be declared at the database or table level?\n\n\nDomain types\n\n\nDomain types are a way of creating new database types with additional\nconstraints. Beam supports declaring these types as part of your\ndatabase, so they can be used anywhere a data type can. In order to\nuse your domain type, you need to supply beam a Haskell newtype that\nis used to represent values of this type in Haskell.\n\n\nTriggers\n\n\nTODO\n\n\nCharacter sets\n\n\nTODO\n\n\nCollations\n\n\nTODO\n\n\nTranslations\n\n\nTODO\n\n\nDatabase descriptors\n\n\nIn order to interact with the database, beam needs to know more about\nthe data structure, it also needs to know how to refer to each entity\nin your database. For the most part, beam can figure out the names for\nyou using its Generics-based defaulting mechanims. Once you have a\ndatabase type defined, you can create a database descriptor using the\n\ndefaultDbSettings\n function.\n\n\nFor example, to create a backend-agnostic database descriptor for the\n\nExampleDb\n type:\n\n\nexampleDb\n \n::\n \nDatabaseSettings\n \nbe\n \nExampleDb\n\n\nexampleDb\n \n=\n \ndefaultDbSettings\n\n\n\n\n\n\nNow, we can use the entities in \nexampleDb\n to write queries. The\nrules for name defaulting for database entities are the same as those\nfor \ntable fields", 
            "title": "Databases"
        }, 
        {
            "location": "/user-guide/databases/#a-simple-database-type", 
            "text": "Like tables, a database type takes a functor and applies it to each\nentity in the database. For example, a database type for the two\ntables defined above has the form.  data   ExampleDb   f \n     =   ExampleDb \n     {   persons   ::   f   ( TableEntity   PersonT ) \n     ,   posts     ::   f   ( TableEntity   PersonT ) \n     }   deriving   Generic  instance   Database   ExampleDb  exampleDb   ::   DatabaseSettings   be   ExampleDb  exampleDb   =   defaultDbSettings", 
            "title": "A simple database type"
        }, 
        {
            "location": "/user-guide/databases/#other-database-entities", 
            "text": "", 
            "title": "Other database entities"
        }, 
        {
            "location": "/user-guide/databases/#views", 
            "text": "Note  Views have not yet been implemented, but this is the expected syntax   Some databases also offer the concept of 'views' -- pseudo-tables that\nare built from a pre-defined query. Suppose we wanted to create a view\nthat returned the latest comments and their respective posters.  data   PostAndPosterView   f \n     =   PostAndPosterView \n     {   post     ::   PostT   f \n     ,   poster   ::   PersonT   f \n     }   deriving   Generic  instance   Beamable   PostAndPosterView   We can include this in our database:  data   ExampleDb   f \n     =   ExampleDb \n     {   persons          ::   f   ( TableEntity   PersonT ) \n     ,   posts            ::   f   ( TableEntity   PersonT ) \n     ,   postAndPosters   ::   f   ( ViewEntity   PostAndPosterView ) \n     }   deriving   Generic   Now we can use  postAndPosters  wherever we'd use a table. Note that you do not\nneed to specify the definition of the view. The definition is not important to\naccess the view, so beam does not need to know about it at the type-level. If\nyou want to manipulate view definitions, use the migrations packgae.", 
            "title": "Views"
        }, 
        {
            "location": "/user-guide/databases/#unique-constraints", 
            "text": "Note  This is the current implementation plan. Uniques are not currently implemented.   The  TableEntityWithUnique  database entity allows you to declare\ntables with additional uniqueness constraints (the primary key is\nconsidered to be unique by default).  For example, suppose you wanted to re-define the  PersonT  table with\nan additional unique e-mail and another unique phone column.  data   PersonT   f \n     =   Person \n     {   personFirstName   ::   Columnar   f   Text \n     ,   personLastName    ::   Columnar   f   Text \n     ,   personAge         ::   Columnar   f   Int \n     ,   personEmail       ::   Columnar   f   Text \n     ,   personPhone       ::   Columnar   f   Text \n     }   deriving   Generic  data   PersonByEmail   f   =   PersonByEmail   ( Columnar   f   Text )  data   PersonByPhone   f   =   PersonByPhone   ( Columnar   f   Text )   Now, use  TableEntityWithUnique  to declare the table.  data   ExampleDb   f \n     =   ExampleDb \n     {   persons          ::   f   ( TableEntityWithUnique   PersonT   [PersonByEmail, PersonByPhone] ) \n     ,   posts            ::   f   ( TableEntity   PersonT ) \n     ,   postAndPosters   ::   f   ( ViewEntity   PostAndPosterView ) \n     }   deriving   Generic   Beam will not complain about this definition, but you will need to\ndeclare additional instances in order to actually use the unique\nconstraints.  instance   Unique   PersonT   PersonByEmail   where \n   mkUnique   =   PersonByEmail   .   personEmail  instance   Unique   PersonT   PersonByPhone   where \n   mkUnique   =   PersonByPhone   .   personPhone   TODO : Should the unique constraints be declared at the database or table level?", 
            "title": "Unique constraints"
        }, 
        {
            "location": "/user-guide/databases/#domain-types", 
            "text": "Domain types are a way of creating new database types with additional\nconstraints. Beam supports declaring these types as part of your\ndatabase, so they can be used anywhere a data type can. In order to\nuse your domain type, you need to supply beam a Haskell newtype that\nis used to represent values of this type in Haskell.", 
            "title": "Domain types"
        }, 
        {
            "location": "/user-guide/databases/#triggers", 
            "text": "TODO", 
            "title": "Triggers"
        }, 
        {
            "location": "/user-guide/databases/#character-sets", 
            "text": "TODO", 
            "title": "Character sets"
        }, 
        {
            "location": "/user-guide/databases/#collations", 
            "text": "TODO", 
            "title": "Collations"
        }, 
        {
            "location": "/user-guide/databases/#translations", 
            "text": "TODO", 
            "title": "Translations"
        }, 
        {
            "location": "/user-guide/databases/#database-descriptors", 
            "text": "In order to interact with the database, beam needs to know more about\nthe data structure, it also needs to know how to refer to each entity\nin your database. For the most part, beam can figure out the names for\nyou using its Generics-based defaulting mechanims. Once you have a\ndatabase type defined, you can create a database descriptor using the defaultDbSettings  function.  For example, to create a backend-agnostic database descriptor for the ExampleDb  type:  exampleDb   ::   DatabaseSettings   be   ExampleDb  exampleDb   =   defaultDbSettings   Now, we can use the entities in  exampleDb  to write queries. The\nrules for name defaulting for database entities are the same as those\nfor  table fields", 
            "title": "Database descriptors"
        }, 
        {
            "location": "/user-guide/queries/basic/", 
            "text": "Given our database definition and database descriptor, we can query database\nentities and retrieve data. Before we discuss writing queries, we will take a\nlook at some of the important query types.\n\n\nData types\n\n\nThe \nQ\n data type\n\n\nBeam queries are built using the \nQ\n data type. \nQ\n's signature is as follows\n\n\ndata\n \nQ\n \nsyntax\n \ndb\n \ns\n \na\n\n\n\n\n\n\nIn this definition\n\n\n\n\n\n\nsyntax\n is the particular dialect of SQL this \nQ\n monad will evaluate to.\n  Often times, this is any instance of \nIsSql92SelectSyntax\n, but sometimes you\n  use syntax-specific features. For example, if you want to use named windows in\n  postgres, you'll likely have to specialize this to \nPgSelectSyntax\n from\n  \nDatabase.Beam.Postgres.Syntax\n.\n\n\n\n\n\n\ndb\n is the type of the database (as we defined above). This is used to ensure\n  you only query database entities that are in scope in this database.\n\n\n\n\n\n\ns\n is the scope parameter. For the most part, you'll write your queries so\n  that they work over all \ns\n. Beam manipulates this parameter internally to\n  ensure that the fields in your expressions are always in scope at run-time.\n\n\n\n\n\n\na\n is the type of the result of the query.\n\n\n\n\n\n\nThe \nQGenExpr\n type\n\n\nWhile \nQ\n represents the result of whole queries (entire \nSELECT\ns for example),\n\nQGenExpr\n represents the type of SQL expressions. \nQGenExpr\n also takes some\ntype parameters:\n\n\ndata\n \nQGenExpr\n \ncontext\n \nsyntax\n \ns\n \na\n\n\n\n\n\n\n\n\n\n\ncontext\n is the particular way in which this expression is being used. For\n  example, expressions containing aggregates have \ncontext ~ QAggregateContext\n.\n  Expressions returning scalar values have \ncontext ~ QValueContext\n.\n\n\n\n\n\n\nsyntax\n is the particular SQL dialect this expression is written in. Note\n  that this is usually different than the \nsyntax\n for \nQ\n, because \nQ\n's syntax\n  refers to a particular syntax for \nSELECT\n expressions (a type implementing\n  \nIsSql92SelectSyntax\n), while \nQGenExpr\n's syntax usually refers to an\n  expression syntax (a type implementing \nIsSql92ExpressionSyntax\n). Of course,\n  since syntaxes are related, you can get from a \nQ\n \nSELECT\n syntax to a\n  \nQGenExpr\n \nsyntax\n with the \nSql92SelectExpressionSyntax\n type family.\n\n\n\n\n\n\nThus, a \nQGenExpr\n with syntax \nSql92SelectExpressionSyntax select\n can be\n  used in the \nFILTER\n clause of a query with type \nQ select db s a\n.\n\n\n\n\n\n\ns\n is a scoping parameter, which will match the \ns\n in \nQ\n.\n\n\n\n\n\n\na\n is the type of this expression. For example, expressions returning SQL\n  \nint\n values, will have Haskell type \nInt\n. This ensures that your SQL query\n  won't fail at run-time with a type error.\n\n\n\n\n\n\nBeam defines some specializations of \nQGenExpr\n for common uses.\n\n\ntype\n \nQExpr\n \n=\n \nQGenExpr\n \nQValueContext\n\n\ntype\n \nQAgg\n \n=\n \nQGenExpr\n \nQAggregateContext\n\n\ntype\n \nQOrd\n \n=\n \nQGenExpr\n \nQOrderingContext\n\n\ntype\n \nQWindowExpr\n \n=\n \nQGenExpr\n \nQWindowingContext\n\n\ntype\n \nQWindowFrame\n \n=\n \nQGenExpr\n \nQWindowFrameContext\n\n\ntype\n \nQGroupExpr\n \n=\n \nQGenExpr\n \nQGroupingContext\n\n\n\n\n\n\nThus, value expressions can be given the simpler type of \nQExpr syntax s a\n.\nExpressions containing aggregates are typed as \nQAgg syntax s a\n.\n\n\nA note on type inference\n\n\nThese types may seem incredibly complicated. Indeed, the safety that beam tries\nto provide requires these scary-looking types.\n\n\nBut alas, do not fear! Beam is also designed to assist type inference. For the\nmost part, you will rarely need to annotate these types in your code.\nOccassionally you will need to provide a type for the result of an expression.\nFor example, \nSELECT\ning just the literal \n1\n may cause an ambiguity, because\nthe compiler won't know which \nIntegral\n type to use. Beam provides an easy\nutility function \nas_\n for this. With \n-XTypeApplications\n enabled,\n\n\nas_\n \n@\nInt\n \n(\nambiguous\n \nexpression\n)\n\n\n\n\n\n\nensures that \nambiguous expression\n has the type \nQGenExpr ctxt syntax s Int\n\nwith the \nctxt\n, \nsyntax\n, and \ns\n types appropriately inferred.\n\n\nSimple queries\n\n\nThe easiest query is simply getting all the rows in a specific table. If you\nhave a database object (something with type \nDatabaseSettings be db\n) with some\ntable or view entities, you can use the \nall_\n function to retrieve all rows in\na specific table or view.\n\n\nFor example, to retrieve all \nPersonT\n entries in the \nexampleDb\n we defined in\nthe last section, we can say \n\n\nall_\n \n(\npersons\n \nexampleDb\n)\n \n::\n \nQ\n \nsyntax\n \nExampleDb\n \ns\n \n(\nPersonT\n \n(\nQExpr\n \ns\n))\n\n\n\n\n\n\n\n\nNote\n\n\nWe give the full type of the query here for illustrative purposes only. There \nis no need to do so in your own code\n\n\n\n\nTwo things to note. Firstly, here \nPersonT\n is parameterized over the \nQExpr s\n\nhigher-kinded type. This means that each field in \nPersonT\n now contains a SQL\nexpression instead of a Haskell value. This is the magic that our parameterized\ntypes allow.\n\n\nThus,\n\n\npersonFirstName\n \n(\nall_\n \n(\npersons\n \nexampleDb\n))\n \n::\n \nQExpr\n \ns\n \nText\n\n\n\n\n\n\nand\n\n\npersonFirstName\n \n(\nPerson\n \nJohn\n \nSmith\n \n23\n \njohn.smith@example.com\n \n8888888888\n \n::\n \nPerson\n)\n \n::\n \nText\n\n\n\n\n\n\nSecondly, the field type has the same scope variable as the entire query. This\nmeans, it can only be used in the scope of this query. You will never be able to\ninspect the type of \ns\n from outside \nQ\n.\n\n\nOnce we have a query in terms of \nQ\n, we can use the \nselect\n function from\n\nDatabase.Beam.Query\n to turn it into a select statement that can be run against\nthe backend.\n\n\nselect\n \n(\nall_\n \n(\npersons\n \nexampleDb\n))\n \n::\n \n(\n...\n)\n \n=\n \nSqlSelect\n \nsyntax\n \nPerson\n\n\n\n\n\n\nThe \n...\n in the context represents a bunch of requirements for \nsyntax\n that\nGHC will generate.\n\n\nNormally, you'd ship this select statement off to a backend to run, but for the\npurposes of this tutorial, we can also ask beam to dump what the standard SQL\nexpression this query encodes.\n\n\ndumpSqlSelect\n \n(\nall_\n \n(\npersons\n \nexampleDb\n))\n\n\nSELECT\n \nt0\n.\nfirst_name\n \nAS\n \nres0\n,\n \nt0\n.\nlast_name\n \nAS\n \nres1\n,\n \nt0\n.\nage\n \nAS\n \nres2\n,\n \nt0\n.\nemail\n \nAS\n \nres3\n,\n \nt0\n.\nphone\n \nAS\n \nres4\n \nFROM\n \nperson\n \nAS\n \nt0\n\n\n\n\n\n\nInternally, \ndumpSqlSelect\n uses a \nbeam-core\n provided syntax to generate\nstandard ANSI SQL expressions. Note that these expressions should not be shipped\nto a backend directly, as they may not be escaped properly. Still, it is useful\nto see what would run.\n\n\nA note on composability\n\n\nAll beam queries are \ncomposable\n. This means that you can freely mix values of\ntype \nQ\n in whichever way typechecks and expect a reasonable SQL query. This\ndiffers from the behavior of SQL, where the syntax for composing queries depends\non the structure of that query.\n\n\nFor example, suppose you wanted to fetch all rows of a table, filter them by a\ncondition, and then limit the amount of rows returned. In beam, \n\n\nConnecting to a database\n\n\nOkay, so we can print out a SQL statement, but how do we execute it against a\ndatabase? Beam provides a convenient \nMonadBeam\n type class that allows us to\nwrite queries in a backend agnostic manner. This is good-enough for most\napplications and preserves portability across databases. However, \nMonadBeam\n\ndoes not support features specific to each backend, nor does it guarantee the\nhighest-performance. Most backends provide additional methods to query a\ndatabase, and you should prefer these if you've committed to a particular\nbackend. For tutorial purposes, we will use the \nbeam-sqlite\n backend.\n\n\nFirst, install \nbeam-sqlite\n with \ncabal\n or \nstack\n:\n\n\n$ cabal install beam-sqlite\n\n# or\n\n$ stack install beam-sqlite\n\n\n\n\n\nNow, load \nbeam-sqlite\n in GHCi. \n\n\nPrelude\n \nimport\n \nDatabase.Beam.Sqlite\n\n\nPrelude\n \nDatabase\n.\nBeam\n.\nSqlite\n \n\n\n\n\n\nNow, in another terminal, load the example database provided. \n\n\n$ sqlite3 basics.db \n beam-sqlite/examples/basics.sql\n\n\n\n\n\nNow, back in GHCi, we can create a connection to this database.\n\n\nPrelude Database.Beam.Sqlite\n basics \n-\n open \nbasics.db\n\nPrelude Database.Beam.Sqlite\n withDatabase basics \n$\n runSelectReturningList \n(\nselect \n(\nall_ \n(\npersons exampleDb\n)))\n\n\n[\n \n..\n \n]\n\n\n\n\n\n\nThe \nrunSelectReturningList\n function takes a \nSqlSelect\n for the given syntax\nand returns the results via a list.\n\n\nVoil\u00e0! We've successfully created our first query and run it against an example\ndatabase. We have now seen the major functionalities of the beam library. In the\nnext section we'll explore more advanced querying and using relationships\nbetween tables.", 
            "title": "Basic Queries"
        }, 
        {
            "location": "/user-guide/queries/basic/#data-types", 
            "text": "", 
            "title": "Data types"
        }, 
        {
            "location": "/user-guide/queries/basic/#the-q-data-type", 
            "text": "Beam queries are built using the  Q  data type.  Q 's signature is as follows  data   Q   syntax   db   s   a   In this definition    syntax  is the particular dialect of SQL this  Q  monad will evaluate to.\n  Often times, this is any instance of  IsSql92SelectSyntax , but sometimes you\n  use syntax-specific features. For example, if you want to use named windows in\n  postgres, you'll likely have to specialize this to  PgSelectSyntax  from\n   Database.Beam.Postgres.Syntax .    db  is the type of the database (as we defined above). This is used to ensure\n  you only query database entities that are in scope in this database.    s  is the scope parameter. For the most part, you'll write your queries so\n  that they work over all  s . Beam manipulates this parameter internally to\n  ensure that the fields in your expressions are always in scope at run-time.    a  is the type of the result of the query.", 
            "title": "The Q data type"
        }, 
        {
            "location": "/user-guide/queries/basic/#the-qgenexpr-type", 
            "text": "While  Q  represents the result of whole queries (entire  SELECT s for example), QGenExpr  represents the type of SQL expressions.  QGenExpr  also takes some\ntype parameters:  data   QGenExpr   context   syntax   s   a     context  is the particular way in which this expression is being used. For\n  example, expressions containing aggregates have  context ~ QAggregateContext .\n  Expressions returning scalar values have  context ~ QValueContext .    syntax  is the particular SQL dialect this expression is written in. Note\n  that this is usually different than the  syntax  for  Q , because  Q 's syntax\n  refers to a particular syntax for  SELECT  expressions (a type implementing\n   IsSql92SelectSyntax ), while  QGenExpr 's syntax usually refers to an\n  expression syntax (a type implementing  IsSql92ExpressionSyntax ). Of course,\n  since syntaxes are related, you can get from a  Q   SELECT  syntax to a\n   QGenExpr   syntax  with the  Sql92SelectExpressionSyntax  type family.    Thus, a  QGenExpr  with syntax  Sql92SelectExpressionSyntax select  can be\n  used in the  FILTER  clause of a query with type  Q select db s a .    s  is a scoping parameter, which will match the  s  in  Q .    a  is the type of this expression. For example, expressions returning SQL\n   int  values, will have Haskell type  Int . This ensures that your SQL query\n  won't fail at run-time with a type error.    Beam defines some specializations of  QGenExpr  for common uses.  type   QExpr   =   QGenExpr   QValueContext  type   QAgg   =   QGenExpr   QAggregateContext  type   QOrd   =   QGenExpr   QOrderingContext  type   QWindowExpr   =   QGenExpr   QWindowingContext  type   QWindowFrame   =   QGenExpr   QWindowFrameContext  type   QGroupExpr   =   QGenExpr   QGroupingContext   Thus, value expressions can be given the simpler type of  QExpr syntax s a .\nExpressions containing aggregates are typed as  QAgg syntax s a .", 
            "title": "The QGenExpr type"
        }, 
        {
            "location": "/user-guide/queries/basic/#a-note-on-type-inference", 
            "text": "These types may seem incredibly complicated. Indeed, the safety that beam tries\nto provide requires these scary-looking types.  But alas, do not fear! Beam is also designed to assist type inference. For the\nmost part, you will rarely need to annotate these types in your code.\nOccassionally you will need to provide a type for the result of an expression.\nFor example,  SELECT ing just the literal  1  may cause an ambiguity, because\nthe compiler won't know which  Integral  type to use. Beam provides an easy\nutility function  as_  for this. With  -XTypeApplications  enabled,  as_   @ Int   ( ambiguous   expression )   ensures that  ambiguous expression  has the type  QGenExpr ctxt syntax s Int \nwith the  ctxt ,  syntax , and  s  types appropriately inferred.", 
            "title": "A note on type inference"
        }, 
        {
            "location": "/user-guide/queries/basic/#simple-queries", 
            "text": "The easiest query is simply getting all the rows in a specific table. If you\nhave a database object (something with type  DatabaseSettings be db ) with some\ntable or view entities, you can use the  all_  function to retrieve all rows in\na specific table or view.  For example, to retrieve all  PersonT  entries in the  exampleDb  we defined in\nthe last section, we can say   all_   ( persons   exampleDb )   ::   Q   syntax   ExampleDb   s   ( PersonT   ( QExpr   s ))    Note  We give the full type of the query here for illustrative purposes only. There \nis no need to do so in your own code   Two things to note. Firstly, here  PersonT  is parameterized over the  QExpr s \nhigher-kinded type. This means that each field in  PersonT  now contains a SQL\nexpression instead of a Haskell value. This is the magic that our parameterized\ntypes allow.  Thus,  personFirstName   ( all_   ( persons   exampleDb ))   ::   QExpr   s   Text   and  personFirstName   ( Person   John   Smith   23   john.smith@example.com   8888888888   ::   Person )   ::   Text   Secondly, the field type has the same scope variable as the entire query. This\nmeans, it can only be used in the scope of this query. You will never be able to\ninspect the type of  s  from outside  Q .  Once we have a query in terms of  Q , we can use the  select  function from Database.Beam.Query  to turn it into a select statement that can be run against\nthe backend.  select   ( all_   ( persons   exampleDb ))   ::   ( ... )   =   SqlSelect   syntax   Person   The  ...  in the context represents a bunch of requirements for  syntax  that\nGHC will generate.  Normally, you'd ship this select statement off to a backend to run, but for the\npurposes of this tutorial, we can also ask beam to dump what the standard SQL\nexpression this query encodes.  dumpSqlSelect   ( all_   ( persons   exampleDb ))  SELECT   t0 . first_name   AS   res0 ,   t0 . last_name   AS   res1 ,   t0 . age   AS   res2 ,   t0 . email   AS   res3 ,   t0 . phone   AS   res4   FROM   person   AS   t0   Internally,  dumpSqlSelect  uses a  beam-core  provided syntax to generate\nstandard ANSI SQL expressions. Note that these expressions should not be shipped\nto a backend directly, as they may not be escaped properly. Still, it is useful\nto see what would run.", 
            "title": "Simple queries"
        }, 
        {
            "location": "/user-guide/queries/basic/#a-note-on-composability", 
            "text": "All beam queries are  composable . This means that you can freely mix values of\ntype  Q  in whichever way typechecks and expect a reasonable SQL query. This\ndiffers from the behavior of SQL, where the syntax for composing queries depends\non the structure of that query.  For example, suppose you wanted to fetch all rows of a table, filter them by a\ncondition, and then limit the amount of rows returned. In beam,", 
            "title": "A note on composability"
        }, 
        {
            "location": "/user-guide/queries/basic/#connecting-to-a-database", 
            "text": "Okay, so we can print out a SQL statement, but how do we execute it against a\ndatabase? Beam provides a convenient  MonadBeam  type class that allows us to\nwrite queries in a backend agnostic manner. This is good-enough for most\napplications and preserves portability across databases. However,  MonadBeam \ndoes not support features specific to each backend, nor does it guarantee the\nhighest-performance. Most backends provide additional methods to query a\ndatabase, and you should prefer these if you've committed to a particular\nbackend. For tutorial purposes, we will use the  beam-sqlite  backend.  First, install  beam-sqlite  with  cabal  or  stack :  $ cabal install beam-sqlite # or \n$ stack install beam-sqlite  Now, load  beam-sqlite  in GHCi.   Prelude   import   Database.Beam.Sqlite  Prelude   Database . Beam . Sqlite    Now, in another terminal, load the example database provided.   $ sqlite3 basics.db   beam-sqlite/examples/basics.sql  Now, back in GHCi, we can create a connection to this database.  Prelude Database.Beam.Sqlite  basics  -  open  basics.db \nPrelude Database.Beam.Sqlite  withDatabase basics  $  runSelectReturningList  ( select  ( all_  ( persons exampleDb )))  [   ..   ]   The  runSelectReturningList  function takes a  SqlSelect  for the given syntax\nand returns the results via a list.  Voil\u00e0! We've successfully created our first query and run it against an example\ndatabase. We have now seen the major functionalities of the beam library. In the\nnext section we'll explore more advanced querying and using relationships\nbetween tables.", 
            "title": "Connecting to a database"
        }, 
        {
            "location": "/user-guide/queries/select/", 
            "text": "We've seen how to create simple queries from our schema. Beam supports other\nclauses in the SQL SELECT statement.\n\n\nFor these examples, we're going to use the \nbeam-sqlite\n backend with the\nprovided sample Chinook database. The Chinook database schema is modeled after a\nfictional record store. It provides several tables containing information on the\nmusic as well as the billing operations. Thus, it provides a good 'real-world'\ndemonstration of beam's capabalities.\n\n\nFirst, create a SQLite database from the included example.\n\n\n$\n sqlite3 chinook.db \n beam-sqlite/examples/chinook.sql\n\n\n\n\n\nNow, load the chinook database schema in GHCi.\n\n\nPrelude\n \nDatabase\n.\nBeam\n.\nSqlite\n \n:\nload\n \nbeam\n-\nsqlite\n/\nexamples\n/\nChinook\n/\nSchema\n.\nhs\n\n\nPrelude\n \nChinook\n.\nSchema\n \nchinook\n \n-\n \nopen\n \nchinook.db\n\n\n\n\n\n\nOne more thing, before we see more complex examples, let's define a quick\nutility function.\n\n\nPrelude\n \nChinook\n.\nSchema\n \nlet\n \nwithConnectionTutorial\n \n=\n \nwithDatabaseDebug\n \nputStrLn\n \nchinook\n\n\n\n\n\n\nLet's test it!\n\n\nWe can run all our queries like:\n\n\nwithConnectionTutorial\n \n$\n \nrunSelectReturningList\n \n$\n \nselect\n \n$\n \nquery\n\n\n\n\n\n\nLet's select all the tracks.\n\n\nwithConnectionTutorial\n \n$\n \nrunSelectReturningList\n \n$\n \nselect\n \n$\n \nall_\n \n(\ntrack\n \nchinookDb\n)\n\n\n\n\n\n\nFor the rest of the guide, we will also show the generated SQL code for both\nsqlite and postgres.\n\n\n\n                \n\n                    \n        \n\n            \n        \n\n            \nHaskell\n\n        \n\n    \n        \n\n            \nSqlite3\n\n        \n\n    \n        \n\n            \nPostgres\n\n        \n\n    \n        \n\n    \n        \n\n            \n        \n\n            \nall_\n \n(\ntrack\n \nchinookDb\n)\n\n\n\n\n        \n\n    \n        \n\n            \nSELECT \nt0\n.\nTrackId\n AS \nres0\n,\n\n\n       \nt0\n.\nName\n AS \nres1\n,\n\n\n       \nt0\n.\nAlbumId\n AS \nres2\n,\n\n\n       \nt0\n.\nMediaTypeId\n AS \nres3\n,\n\n\n       \nt0\n.\nGenreId\n AS \nres4\n,\n\n\n       \nt0\n.\nComposer\n AS \nres5\n,\n\n\n       \nt0\n.\nMilliseconds\n AS \nres6\n,\n\n\n       \nt0\n.\nBytes\n AS \nres7\n,\n\n\n       \nt0\n.\nUnitPrice\n AS \nres8\n\n\nFROM \nTrack\n AS \nt0\n\n\n\n\n        \n\n    \n        \n\n            \nSELECT\n \nt0\n.\nTrackId\n \nAS\n \nres0\n,\n\n       \nt0\n.\nName\n \nAS\n \nres1\n,\n\n       \nt0\n.\nAlbumId\n \nAS\n \nres2\n,\n\n       \nt0\n.\nMediaTypeId\n \nAS\n \nres3\n,\n\n       \nt0\n.\nGenreId\n \nAS\n \nres4\n,\n\n       \nt0\n.\nComposer\n \nAS\n \nres5\n,\n\n       \nt0\n.\nMilliseconds\n \nAS\n \nres6\n,\n\n       \nt0\n.\nBytes\n \nAS\n \nres7\n,\n\n       \nt0\n.\nUnitPrice\n \nAS\n \nres8\n\n\nFROM\n \nTrack\n \nAS\n \nt0\n\n\n\n\n        \n\n    \n        \n\n    \n                \n\n                    \n\n\nWHERE\n clause\n\n\nWe've seen how to use \nall_\n to select all rows of a table. Sometimes, you would\nlike to filter results based on the result of some condition. For example,\nperhaps you would like to fetch all customers whose names start with \"Jo\". We\ncan filter over results using the \nfilter_\n function.\n\n\n\n                \n\n                    \n        \n\n            \n        \n\n            \nHaskell\n\n        \n\n    \n        \n\n            \nSqlite3\n\n        \n\n    \n        \n\n            \nPostgres\n\n        \n\n    \n        \n\n    \n        \n\n            \n        \n\n            \nfilter_\n \n(\n\\\ncustomer\n \n-\n \ncustomerFirstName\n \ncustomer\n \n`\nlike_\n`\n \nJo%\n)\n \n$\n\n\nall_\n \n(\ncustomer\n \nchinookDb\n)\n\n\n\n\n        \n\n    \n        \n\n            \nSELECT \nt0\n.\nCustomerId\n AS \nres0\n,\n\n\n       \nt0\n.\nFirstName\n AS \nres1\n,\n\n\n       \nt0\n.\nLastName\n AS \nres2\n,\n\n\n       \nt0\n.\nCompany\n AS \nres3\n,\n\n\n       \nt0\n.\nAddress\n AS \nres4\n,\n\n\n       \nt0\n.\nCity\n AS \nres5\n,\n\n\n       \nt0\n.\nState\n AS \nres6\n,\n\n\n       \nt0\n.\nCountry\n AS \nres7\n,\n\n\n       \nt0\n.\nPostalCode\n AS \nres8\n,\n\n\n       \nt0\n.\nPhone\n AS \nres9\n,\n\n\n       \nt0\n.\nFax\n AS \nres10\n,\n\n\n       \nt0\n.\nEmail\n AS \nres11\n,\n\n\n       \nt0\n.\nSupportRepId\n AS \nres12\n\n\nFROM \nCustomer\n AS \nt0\n\n\nWHERE (\nt0\n.\nFirstName\n) LIKE (?)\n\n\n\n\n        \n\n    \n        \n\n            \nSELECT\n \nt0\n.\nCustomerId\n \nAS\n \nres0\n,\n\n       \nt0\n.\nFirstName\n \nAS\n \nres1\n,\n\n       \nt0\n.\nLastName\n \nAS\n \nres2\n,\n\n       \nt0\n.\nCompany\n \nAS\n \nres3\n,\n\n       \nt0\n.\nAddress\n \nAS\n \nres4\n,\n\n       \nt0\n.\nCity\n \nAS\n \nres5\n,\n\n       \nt0\n.\nState\n \nAS\n \nres6\n,\n\n       \nt0\n.\nCountry\n \nAS\n \nres7\n,\n\n       \nt0\n.\nPostalCode\n \nAS\n \nres8\n,\n\n       \nt0\n.\nPhone\n \nAS\n \nres9\n,\n\n       \nt0\n.\nFax\n \nAS\n \nres10\n,\n\n       \nt0\n.\nEmail\n \nAS\n \nres11\n,\n\n       \nt0\n.\nSupportRepId\n \nAS\n \nres12\n\n\nFROM\n \nCustomer\n \nAS\n \nt0\n\n\nWHERE\n \n(\nt0\n.\nFirstName\n)\n \nLIKE\n \n(\nJo%\n)\n\n\n\n\n        \n\n    \n        \n\n    \n                \n\n                    \n\n\nYou can use \n(\n.)\n and \n(||.)\n to combine boolean expressions, as you'd expect.\nFor example, to select all customers whose first name begins with \"Jo\", last\nname begins with \"S\", and who live in either California or Washington:\n\n\n\n                \n\n                    \n        \n\n            \n        \n\n            \nHaskell\n\n        \n\n    \n        \n\n            \nSqlite3\n\n        \n\n    \n        \n\n            \nPostgres\n\n        \n\n    \n        \n\n    \n        \n\n            \n        \n\n            \nfilter_\n \n(\n\\\ncustomer\n \n-\n \n((\ncustomerFirstName\n \ncustomer\n \n`\nlike_\n`\n \nJo%\n)\n \n.\n \n(\ncustomerLastName\n \ncustomer\n \n`\nlike_\n`\n \nS%\n))\n \n.\n\n                      \n(\naddressState\n \n(\ncustomerAddress\n \ncustomer\n)\n \n==.\n \njust_\n \nCA\n \n||.\n \naddressState\n \n(\ncustomerAddress\n \ncustomer\n)\n \n==.\n \njust_\n \nWA\n))\n \n$\n\n        \nall_\n \n(\ncustomer\n \nchinookDb\n)\n\n\n\n\n        \n\n    \n        \n\n            \nSELECT \nt0\n.\nCustomerId\n AS \nres0\n,\n\n\n       \nt0\n.\nFirstName\n AS \nres1\n,\n\n\n       \nt0\n.\nLastName\n AS \nres2\n,\n\n\n       \nt0\n.\nCompany\n AS \nres3\n,\n\n\n       \nt0\n.\nAddress\n AS \nres4\n,\n\n\n       \nt0\n.\nCity\n AS \nres5\n,\n\n\n       \nt0\n.\nState\n AS \nres6\n,\n\n\n       \nt0\n.\nCountry\n AS \nres7\n,\n\n\n       \nt0\n.\nPostalCode\n AS \nres8\n,\n\n\n       \nt0\n.\nPhone\n AS \nres9\n,\n\n\n       \nt0\n.\nFax\n AS \nres10\n,\n\n\n       \nt0\n.\nEmail\n AS \nres11\n,\n\n\n       \nt0\n.\nSupportRepId\n AS \nres12\n\n\nFROM \nCustomer\n AS \nt0\n\n\nWHERE (((\nt0\n.\nFirstName\n) LIKE (?))\n\n\n       AND ((\nt0\n.\nLastName\n) LIKE (?)))\n\n\n  AND (((\nt0\n.\nState\n)=(?))\n\n\n       OR ((\nt0\n.\nState\n)=(?)))\n\n\n\n\n        \n\n    \n        \n\n            \nSELECT\n \nt0\n.\nCustomerId\n \nAS\n \nres0\n,\n\n       \nt0\n.\nFirstName\n \nAS\n \nres1\n,\n\n       \nt0\n.\nLastName\n \nAS\n \nres2\n,\n\n       \nt0\n.\nCompany\n \nAS\n \nres3\n,\n\n       \nt0\n.\nAddress\n \nAS\n \nres4\n,\n\n       \nt0\n.\nCity\n \nAS\n \nres5\n,\n\n       \nt0\n.\nState\n \nAS\n \nres6\n,\n\n       \nt0\n.\nCountry\n \nAS\n \nres7\n,\n\n       \nt0\n.\nPostalCode\n \nAS\n \nres8\n,\n\n       \nt0\n.\nPhone\n \nAS\n \nres9\n,\n\n       \nt0\n.\nFax\n \nAS\n \nres10\n,\n\n       \nt0\n.\nEmail\n \nAS\n \nres11\n,\n\n       \nt0\n.\nSupportRepId\n \nAS\n \nres12\n\n\nFROM\n \nCustomer\n \nAS\n \nt0\n\n\nWHERE\n \n(((\nt0\n.\nFirstName\n)\n \nLIKE\n \n(\nJo%\n))\n\n       \nAND\n \n((\nt0\n.\nLastName\n)\n \nLIKE\n \n(\nS%\n)))\n\n  \nAND\n \n(((\nt0\n.\nState\n)\n \n=\n \n(\nCA\n))\n\n       \nOR\n \n((\nt0\n.\nState\n)\n \n=\n \n(\nWA\n)))\n\n\n\n\n        \n\n    \n        \n\n    \n                \n\n                    \n\n\n\n\nNote\n\n\nWe had to use the \njust_\n function above to compare\n\naddressState (customerAddress customer)\n. This is because \naddressState\n(customerAddress customer)\n represents a nullable column which beam types as\n\nMaybe Text\n. Just as in Haskell, we need to explicitly unwrap the \nMaybe\n\ntype. This is an example of beam offering stronger typing than SQL itself.\n\n\n\n\nLIMIT\n/\nOFFSET\n support\n\n\nThe \nlimit_\n and \noffset_\n functions can be used to truncate the result set at a\ncertain length and fetch different portions of the result. They correspond to\nthe \nLIMIT\n and \nOFFSET\n SQL constructs.\n\n\n\n                \n\n                    \n        \n\n            \n        \n\n            \nHaskell\n\n        \n\n    \n        \n\n            \nSqlite3\n\n        \n\n    \n        \n\n            \nPostgres\n\n        \n\n    \n        \n\n    \n        \n\n            \n        \n\n            \nlimit_\n \n10\n \n$\n \noffset_\n \n100\n \n$\n\n\nfilter_\n \n(\n\\\ncustomer\n \n-\n \n((\ncustomerFirstName\n \ncustomer\n \n`\nlike_\n`\n \nJo%\n)\n \n.\n \n(\ncustomerLastName\n \ncustomer\n \n`\nlike_\n`\n \nS%\n))\n \n.\n\n                      \n(\naddressState\n \n(\ncustomerAddress\n \ncustomer\n)\n \n==.\n \njust_\n \nCA\n \n||.\n \naddressState\n \n(\ncustomerAddress\n \ncustomer\n)\n \n==.\n \njust_\n \nWA\n))\n \n$\n\n        \nall_\n \n(\ncustomer\n \nchinookDb\n)\n\n\n\n\n        \n\n    \n        \n\n            \nSELECT \nt0\n.\nCustomerId\n AS \nres0\n,\n\n\n       \nt0\n.\nFirstName\n AS \nres1\n,\n\n\n       \nt0\n.\nLastName\n AS \nres2\n,\n\n\n       \nt0\n.\nCompany\n AS \nres3\n,\n\n\n       \nt0\n.\nAddress\n AS \nres4\n,\n\n\n       \nt0\n.\nCity\n AS \nres5\n,\n\n\n       \nt0\n.\nState\n AS \nres6\n,\n\n\n       \nt0\n.\nCountry\n AS \nres7\n,\n\n\n       \nt0\n.\nPostalCode\n AS \nres8\n,\n\n\n       \nt0\n.\nPhone\n AS \nres9\n,\n\n\n       \nt0\n.\nFax\n AS \nres10\n,\n\n\n       \nt0\n.\nEmail\n AS \nres11\n,\n\n\n       \nt0\n.\nSupportRepId\n AS \nres12\n\n\nFROM \nCustomer\n AS \nt0\n\n\nWHERE (((\nt0\n.\nFirstName\n) LIKE (?))\n\n\n       AND ((\nt0\n.\nLastName\n) LIKE (?)))\n\n\n  AND (((\nt0\n.\nState\n)=(?))\n\n\n       OR ((\nt0\n.\nState\n)=(?)))\n\n\nLIMIT 10\n\n\nOFFSET 100\n\n\n\n\n        \n\n    \n        \n\n            \nSELECT\n \nt0\n.\nCustomerId\n \nAS\n \nres0\n,\n\n       \nt0\n.\nFirstName\n \nAS\n \nres1\n,\n\n       \nt0\n.\nLastName\n \nAS\n \nres2\n,\n\n       \nt0\n.\nCompany\n \nAS\n \nres3\n,\n\n       \nt0\n.\nAddress\n \nAS\n \nres4\n,\n\n       \nt0\n.\nCity\n \nAS\n \nres5\n,\n\n       \nt0\n.\nState\n \nAS\n \nres6\n,\n\n       \nt0\n.\nCountry\n \nAS\n \nres7\n,\n\n       \nt0\n.\nPostalCode\n \nAS\n \nres8\n,\n\n       \nt0\n.\nPhone\n \nAS\n \nres9\n,\n\n       \nt0\n.\nFax\n \nAS\n \nres10\n,\n\n       \nt0\n.\nEmail\n \nAS\n \nres11\n,\n\n       \nt0\n.\nSupportRepId\n \nAS\n \nres12\n\n\nFROM\n \nCustomer\n \nAS\n \nt0\n\n\nWHERE\n \n(((\nt0\n.\nFirstName\n)\n \nLIKE\n \n(\nJo%\n))\n\n       \nAND\n \n((\nt0\n.\nLastName\n)\n \nLIKE\n \n(\nS%\n)))\n\n  \nAND\n \n(((\nt0\n.\nState\n)\n \n=\n \n(\nCA\n))\n\n       \nOR\n \n((\nt0\n.\nState\n)\n \n=\n \n(\nWA\n)))\n\n\nLIMIT\n \n10\n\n\nOFFSET\n \n100\n\n\n\n\n        \n\n    \n        \n\n    \n                \n\n                    \n\n\n\n\nNote\n\n\nNested \nlimit_\ns and \noffset_\ns compose in the way you'd expect without\ngenerating extraneous subqueries.\n\n\n\n\n\n\nWarning\n\n\nNote that the order of the \nlimit_\n and \noffset_\n functions matter.\nOffseting an already limited result is not the same as limiting an offseted\nresult. For example, if you offset three rows into a limited set of five\nresults, you will get at most two rows. On the other hand, if you offset\nthree rows and then limit the result to the next five, you may get up to\nfive. Beam will generate exactly the query you specify. Notice the\ndifference below, where the order of the clauses made beam generate a query\nthat returns no results.\n\n\n\n\n\n                \n\n                    \n        \n\n            \n        \n\n            \nHaskell\n\n        \n\n    \n        \n\n            \nSqlite3\n\n        \n\n    \n        \n\n            \nPostgres\n\n        \n\n    \n        \n\n    \n        \n\n            \n        \n\n            \noffset_\n \n100\n \n$\n \nlimit_\n \n10\n \n$\n\n\nfilter_\n \n(\n\\\ncustomer\n \n-\n \n((\ncustomerFirstName\n \ncustomer\n \n`\nlike_\n`\n \nJo%\n)\n \n.\n \n(\ncustomerLastName\n \ncustomer\n \n`\nlike_\n`\n \nS%\n))\n \n.\n\n                      \n(\naddressState\n \n(\ncustomerAddress\n \ncustomer\n)\n \n==.\n \njust_\n \nCA\n \n||.\n \naddressState\n \n(\ncustomerAddress\n \ncustomer\n)\n \n==.\n \njust_\n \nWA\n))\n \n$\n\n        \nall_\n \n(\ncustomer\n \nchinookDb\n)\n\n\n\n\n        \n\n    \n        \n\n            \nSELECT \nt0\n.\nCustomerId\n AS \nres0\n,\n\n\n       \nt0\n.\nFirstName\n AS \nres1\n,\n\n\n       \nt0\n.\nLastName\n AS \nres2\n,\n\n\n       \nt0\n.\nCompany\n AS \nres3\n,\n\n\n       \nt0\n.\nAddress\n AS \nres4\n,\n\n\n       \nt0\n.\nCity\n AS \nres5\n,\n\n\n       \nt0\n.\nState\n AS \nres6\n,\n\n\n       \nt0\n.\nCountry\n AS \nres7\n,\n\n\n       \nt0\n.\nPostalCode\n AS \nres8\n,\n\n\n       \nt0\n.\nPhone\n AS \nres9\n,\n\n\n       \nt0\n.\nFax\n AS \nres10\n,\n\n\n       \nt0\n.\nEmail\n AS \nres11\n,\n\n\n       \nt0\n.\nSupportRepId\n AS \nres12\n\n\nFROM \nCustomer\n AS \nt0\n\n\nWHERE (((\nt0\n.\nFirstName\n) LIKE (?))\n\n\n       AND ((\nt0\n.\nLastName\n) LIKE (?)))\n\n\n  AND (((\nt0\n.\nState\n)=(?))\n\n\n       OR ((\nt0\n.\nState\n)=(?)))\n\n\nLIMIT 0\n\n\nOFFSET 100\n\n\n\n\n        \n\n    \n        \n\n            \nSELECT\n \nt0\n.\nCustomerId\n \nAS\n \nres0\n,\n\n       \nt0\n.\nFirstName\n \nAS\n \nres1\n,\n\n       \nt0\n.\nLastName\n \nAS\n \nres2\n,\n\n       \nt0\n.\nCompany\n \nAS\n \nres3\n,\n\n       \nt0\n.\nAddress\n \nAS\n \nres4\n,\n\n       \nt0\n.\nCity\n \nAS\n \nres5\n,\n\n       \nt0\n.\nState\n \nAS\n \nres6\n,\n\n       \nt0\n.\nCountry\n \nAS\n \nres7\n,\n\n       \nt0\n.\nPostalCode\n \nAS\n \nres8\n,\n\n       \nt0\n.\nPhone\n \nAS\n \nres9\n,\n\n       \nt0\n.\nFax\n \nAS\n \nres10\n,\n\n       \nt0\n.\nEmail\n \nAS\n \nres11\n,\n\n       \nt0\n.\nSupportRepId\n \nAS\n \nres12\n\n\nFROM\n \nCustomer\n \nAS\n \nt0\n\n\nWHERE\n \n(((\nt0\n.\nFirstName\n)\n \nLIKE\n \n(\nJo%\n))\n\n       \nAND\n \n((\nt0\n.\nLastName\n)\n \nLIKE\n \n(\nS%\n)))\n\n  \nAND\n \n(((\nt0\n.\nState\n)\n \n=\n \n(\nCA\n))\n\n       \nOR\n \n((\nt0\n.\nState\n)\n \n=\n \n(\nWA\n)))\n\n\nLIMIT\n \n0\n\n\nOFFSET\n \n100", 
            "title": "More complex SELECTs"
        }, 
        {
            "location": "/user-guide/queries/select/#where-clause", 
            "text": "We've seen how to use  all_  to select all rows of a table. Sometimes, you would\nlike to filter results based on the result of some condition. For example,\nperhaps you would like to fetch all customers whose names start with \"Jo\". We\ncan filter over results using the  filter_  function.  \n                 \n                    \n         \n            \n         \n             Haskell \n         \n    \n         \n             Sqlite3 \n         \n    \n         \n             Postgres \n         \n    \n         \n    \n         \n            \n         \n             filter_   ( \\ customer   -   customerFirstName   customer   ` like_ `   Jo% )   $  all_   ( customer   chinookDb )  \n\n         \n    \n         \n             SELECT  t0 . CustomerId  AS  res0 ,          t0 . FirstName  AS  res1 ,          t0 . LastName  AS  res2 ,          t0 . Company  AS  res3 ,          t0 . Address  AS  res4 ,          t0 . City  AS  res5 ,          t0 . State  AS  res6 ,          t0 . Country  AS  res7 ,          t0 . PostalCode  AS  res8 ,          t0 . Phone  AS  res9 ,          t0 . Fax  AS  res10 ,          t0 . Email  AS  res11 ,          t0 . SupportRepId  AS  res12  FROM  Customer  AS  t0  WHERE ( t0 . FirstName ) LIKE (?)  \n\n         \n    \n         \n             SELECT   t0 . CustomerId   AS   res0 , \n        t0 . FirstName   AS   res1 , \n        t0 . LastName   AS   res2 , \n        t0 . Company   AS   res3 , \n        t0 . Address   AS   res4 , \n        t0 . City   AS   res5 , \n        t0 . State   AS   res6 , \n        t0 . Country   AS   res7 , \n        t0 . PostalCode   AS   res8 , \n        t0 . Phone   AS   res9 , \n        t0 . Fax   AS   res10 , \n        t0 . Email   AS   res11 , \n        t0 . SupportRepId   AS   res12  FROM   Customer   AS   t0  WHERE   ( t0 . FirstName )   LIKE   ( Jo% )  \n\n         \n    \n         \n    \n                 \n                      You can use  ( .)  and  (||.)  to combine boolean expressions, as you'd expect.\nFor example, to select all customers whose first name begins with \"Jo\", last\nname begins with \"S\", and who live in either California or Washington:  \n                 \n                    \n         \n            \n         \n             Haskell \n         \n    \n         \n             Sqlite3 \n         \n    \n         \n             Postgres \n         \n    \n         \n    \n         \n            \n         \n             filter_   ( \\ customer   -   (( customerFirstName   customer   ` like_ `   Jo% )   .   ( customerLastName   customer   ` like_ `   S% ))   . \n                       ( addressState   ( customerAddress   customer )   ==.   just_   CA   ||.   addressState   ( customerAddress   customer )   ==.   just_   WA ))   $ \n         all_   ( customer   chinookDb )  \n\n         \n    \n         \n             SELECT  t0 . CustomerId  AS  res0 ,          t0 . FirstName  AS  res1 ,          t0 . LastName  AS  res2 ,          t0 . Company  AS  res3 ,          t0 . Address  AS  res4 ,          t0 . City  AS  res5 ,          t0 . State  AS  res6 ,          t0 . Country  AS  res7 ,          t0 . PostalCode  AS  res8 ,          t0 . Phone  AS  res9 ,          t0 . Fax  AS  res10 ,          t0 . Email  AS  res11 ,          t0 . SupportRepId  AS  res12  FROM  Customer  AS  t0  WHERE ((( t0 . FirstName ) LIKE (?))         AND (( t0 . LastName ) LIKE (?)))    AND ((( t0 . State )=(?))         OR (( t0 . State )=(?)))  \n\n         \n    \n         \n             SELECT   t0 . CustomerId   AS   res0 , \n        t0 . FirstName   AS   res1 , \n        t0 . LastName   AS   res2 , \n        t0 . Company   AS   res3 , \n        t0 . Address   AS   res4 , \n        t0 . City   AS   res5 , \n        t0 . State   AS   res6 , \n        t0 . Country   AS   res7 , \n        t0 . PostalCode   AS   res8 , \n        t0 . Phone   AS   res9 , \n        t0 . Fax   AS   res10 , \n        t0 . Email   AS   res11 , \n        t0 . SupportRepId   AS   res12  FROM   Customer   AS   t0  WHERE   ((( t0 . FirstName )   LIKE   ( Jo% )) \n        AND   (( t0 . LastName )   LIKE   ( S% ))) \n   AND   ((( t0 . State )   =   ( CA )) \n        OR   (( t0 . State )   =   ( WA )))  \n\n         \n    \n         \n    \n                 \n                       Note  We had to use the  just_  function above to compare addressState (customerAddress customer) . This is because  addressState\n(customerAddress customer)  represents a nullable column which beam types as Maybe Text . Just as in Haskell, we need to explicitly unwrap the  Maybe \ntype. This is an example of beam offering stronger typing than SQL itself.", 
            "title": "WHERE clause"
        }, 
        {
            "location": "/user-guide/queries/select/#limitoffset-support", 
            "text": "The  limit_  and  offset_  functions can be used to truncate the result set at a\ncertain length and fetch different portions of the result. They correspond to\nthe  LIMIT  and  OFFSET  SQL constructs.  \n                 \n                    \n         \n            \n         \n             Haskell \n         \n    \n         \n             Sqlite3 \n         \n    \n         \n             Postgres \n         \n    \n         \n    \n         \n            \n         \n             limit_   10   $   offset_   100   $  filter_   ( \\ customer   -   (( customerFirstName   customer   ` like_ `   Jo% )   .   ( customerLastName   customer   ` like_ `   S% ))   . \n                       ( addressState   ( customerAddress   customer )   ==.   just_   CA   ||.   addressState   ( customerAddress   customer )   ==.   just_   WA ))   $ \n         all_   ( customer   chinookDb )  \n\n         \n    \n         \n             SELECT  t0 . CustomerId  AS  res0 ,          t0 . FirstName  AS  res1 ,          t0 . LastName  AS  res2 ,          t0 . Company  AS  res3 ,          t0 . Address  AS  res4 ,          t0 . City  AS  res5 ,          t0 . State  AS  res6 ,          t0 . Country  AS  res7 ,          t0 . PostalCode  AS  res8 ,          t0 . Phone  AS  res9 ,          t0 . Fax  AS  res10 ,          t0 . Email  AS  res11 ,          t0 . SupportRepId  AS  res12  FROM  Customer  AS  t0  WHERE ((( t0 . FirstName ) LIKE (?))         AND (( t0 . LastName ) LIKE (?)))    AND ((( t0 . State )=(?))         OR (( t0 . State )=(?)))  LIMIT 10  OFFSET 100  \n\n         \n    \n         \n             SELECT   t0 . CustomerId   AS   res0 , \n        t0 . FirstName   AS   res1 , \n        t0 . LastName   AS   res2 , \n        t0 . Company   AS   res3 , \n        t0 . Address   AS   res4 , \n        t0 . City   AS   res5 , \n        t0 . State   AS   res6 , \n        t0 . Country   AS   res7 , \n        t0 . PostalCode   AS   res8 , \n        t0 . Phone   AS   res9 , \n        t0 . Fax   AS   res10 , \n        t0 . Email   AS   res11 , \n        t0 . SupportRepId   AS   res12  FROM   Customer   AS   t0  WHERE   ((( t0 . FirstName )   LIKE   ( Jo% )) \n        AND   (( t0 . LastName )   LIKE   ( S% ))) \n   AND   ((( t0 . State )   =   ( CA )) \n        OR   (( t0 . State )   =   ( WA )))  LIMIT   10  OFFSET   100  \n\n         \n    \n         \n    \n                 \n                       Note  Nested  limit_ s and  offset_ s compose in the way you'd expect without\ngenerating extraneous subqueries.    Warning  Note that the order of the  limit_  and  offset_  functions matter.\nOffseting an already limited result is not the same as limiting an offseted\nresult. For example, if you offset three rows into a limited set of five\nresults, you will get at most two rows. On the other hand, if you offset\nthree rows and then limit the result to the next five, you may get up to\nfive. Beam will generate exactly the query you specify. Notice the\ndifference below, where the order of the clauses made beam generate a query\nthat returns no results.   \n                 \n                    \n         \n            \n         \n             Haskell \n         \n    \n         \n             Sqlite3 \n         \n    \n         \n             Postgres \n         \n    \n         \n    \n         \n            \n         \n             offset_   100   $   limit_   10   $  filter_   ( \\ customer   -   (( customerFirstName   customer   ` like_ `   Jo% )   .   ( customerLastName   customer   ` like_ `   S% ))   . \n                       ( addressState   ( customerAddress   customer )   ==.   just_   CA   ||.   addressState   ( customerAddress   customer )   ==.   just_   WA ))   $ \n         all_   ( customer   chinookDb )  \n\n         \n    \n         \n             SELECT  t0 . CustomerId  AS  res0 ,          t0 . FirstName  AS  res1 ,          t0 . LastName  AS  res2 ,          t0 . Company  AS  res3 ,          t0 . Address  AS  res4 ,          t0 . City  AS  res5 ,          t0 . State  AS  res6 ,          t0 . Country  AS  res7 ,          t0 . PostalCode  AS  res8 ,          t0 . Phone  AS  res9 ,          t0 . Fax  AS  res10 ,          t0 . Email  AS  res11 ,          t0 . SupportRepId  AS  res12  FROM  Customer  AS  t0  WHERE ((( t0 . FirstName ) LIKE (?))         AND (( t0 . LastName ) LIKE (?)))    AND ((( t0 . State )=(?))         OR (( t0 . State )=(?)))  LIMIT 0  OFFSET 100  \n\n         \n    \n         \n             SELECT   t0 . CustomerId   AS   res0 , \n        t0 . FirstName   AS   res1 , \n        t0 . LastName   AS   res2 , \n        t0 . Company   AS   res3 , \n        t0 . Address   AS   res4 , \n        t0 . City   AS   res5 , \n        t0 . State   AS   res6 , \n        t0 . Country   AS   res7 , \n        t0 . PostalCode   AS   res8 , \n        t0 . Phone   AS   res9 , \n        t0 . Fax   AS   res10 , \n        t0 . Email   AS   res11 , \n        t0 . SupportRepId   AS   res12  FROM   Customer   AS   t0  WHERE   ((( t0 . FirstName )   LIKE   ( Jo% )) \n        AND   (( t0 . LastName )   LIKE   ( S% ))) \n   AND   ((( t0 . State )   =   ( CA )) \n        OR   (( t0 . State )   =   ( WA )))  LIMIT   0  OFFSET   100", 
            "title": "LIMIT/OFFSET support"
        }, 
        {
            "location": "/user-guide/queries/ordering/", 
            "text": "Usually, queries are ordered before \nLIMIT\n and \nOFFSET\n are applied. Beam\nsupports the standard SQL \nORDER BY\n construct through the \norderBy_\n function.\n\n\norderBy_\n works like the Haskell function \nsortBy\n, wich some restructions. Its\nfirst argument is a function which takes as input the output of the given query.\nThe function should return a sorting key, which is either a single sort ordering\nor a tuple of them. A sort ordering specifies an expression and a direction by\nwhich to sort. The result is then sorted lexicographically based on these sort\nexpressions. The second argument to \norderBy_\n is the query whose results to\nsort.\n\n\nUse the \nasc_\n and \ndesc_\n functions to specify the sort ordering over an\narbitrary expression.\n\n\nFor example, to get the first ten albums when sorted lexicographically, use\n\n\n\n                \n\n                    \n        \n\n            \n        \n\n            \nHaskell\n\n        \n\n    \n        \n\n            \nSqlite3\n\n        \n\n    \n        \n\n            \nPostgres\n\n        \n\n    \n        \n\n    \n        \n\n            \n        \n\n            \nlimit_\n \n10\n \n$\n\n\norderBy_\n \n(\nasc_\n \n.\n \nalbumTitle\n)\n \n$\n\n\nall_\n \n(\nalbum\n \nchinookDb\n)\n\n\n\n\n        \n\n    \n        \n\n            \nSELECT \nt0\n.\nAlbumId\n AS \nres0\n,\n\n\n       \nt0\n.\nTitle\n AS \nres1\n,\n\n\n       \nt0\n.\nArtistId\n AS \nres2\n\n\nFROM \nAlbum\n AS \nt0\n\n\nORDER BY \nt0\n.\nTitle\n ASC\n\n\nLIMIT 10\n\n\n\n\n        \n\n    \n        \n\n            \nSELECT\n \nt0\n.\nAlbumId\n \nAS\n \nres0\n,\n\n       \nt0\n.\nTitle\n \nAS\n \nres1\n,\n\n       \nt0\n.\nArtistId\n \nAS\n \nres2\n\n\nFROM\n \nAlbum\n \nAS\n \nt0\n\n\nORDER\n \nBY\n \nt0\n.\nTitle\n \nASC\n\n\nLIMIT\n \n10\n\n\n\n\n        \n\n    \n        \n\n    \n                \n\n                    \n\n\nAgain, note that the ordering in which you apply the \nlimit_\n and \norderBy_\n\nmatters. In general, you want to sort before you limit or offset, to keep your\nresult set stable. However, if you really want to sort a limited number of\narbitrarily chosen rows, you can use a different ordering.\n\n\n\n                \n\n                    \n        \n\n            \n        \n\n            \nHaskell\n\n        \n\n    \n        \n\n            \nSqlite3\n\n        \n\n    \n        \n\n            \nPostgres\n\n        \n\n    \n        \n\n    \n        \n\n            \n        \n\n            \norderBy_\n \n(\nasc_\n \n.\n \nalbumTitle\n)\n \n$\n\n\nlimit_\n \n10\n \n$\n\n\nall_\n \n(\nalbum\n \nchinookDb\n)\n\n\n\n\n        \n\n    \n        \n\n            \nSELECT \nt0\n.\nres0\n AS \nres0\n,\n\n\n       \nt0\n.\nres1\n AS \nres1\n,\n\n\n       \nt0\n.\nres2\n AS \nres2\n\n\nFROM\n\n\n  (SELECT \nt0\n.\nAlbumId\n AS \nres0\n,\n\n\n          \nt0\n.\nTitle\n AS \nres1\n,\n\n\n          \nt0\n.\nArtistId\n AS \nres2\n\n\n   FROM \nAlbum\n AS \nt0\n\n\n   LIMIT 10) AS \nt0\n\n\nORDER BY \nt0\n.\nres1\n ASC\n\n\n\n\n        \n\n    \n        \n\n            \nSELECT\n \nt0\n.\nres0\n \nAS\n \nres0\n,\n\n       \nt0\n.\nres1\n \nAS\n \nres1\n,\n\n       \nt0\n.\nres2\n \nAS\n \nres2\n\n\nFROM\n\n  \n(\nSELECT\n \nt0\n.\nAlbumId\n \nAS\n \nres0\n,\n\n          \nt0\n.\nTitle\n \nAS\n \nres1\n,\n\n          \nt0\n.\nArtistId\n \nAS\n \nres2\n\n   \nFROM\n \nAlbum\n \nAS\n \nt0\n\n   \nLIMIT\n \n10\n)\n \nAS\n \nt0\n\n\nORDER\n \nBY\n \nt0\n.\nres1\n \nASC", 
            "title": "Ordering"
        }, 
        {
            "location": "/user-guide/queries/relationships/", 
            "text": "Relational databases are so-named because they're good at expressing relations\namong data and providing related data in queries. Beam exposes these features in\nits DSL.\n\n\nFor these examples, we're going to use the \nbeam-sqlite\n backend with the\nprovided sample Chinook database.\n\n\nFirst, create a SQLite database from the included example.\n\n\n sqlite3 chinook.db \n beam-sqlite/examples/chinook.sql\n\n\n\n\n\nNow, load the chinook database schema in GHCi.\n\n\nPrelude\n \nDatabase\n.\nBeam\n.\nSqlite\n \n:\nload\n \nbeam\n-\nsqlite\n/\nexamples\n/\nChinook\n/\nSchema\n.\nhs\n\n\nPrelude\n \nChinook\n.\nSchema\n \nchinook\n \n-\n \nopen\n \nchinook.db\n\n\n\n\n\n\nOne more thing, before we explore how beam handles relationships. Before we do, let's define a quick utility function.\n\n\nPrelude\n \nChinook\n.\nSchema\n \nlet\n \nwithConnectionTutorial\n \n=\n \nwithDatabaseDebug\n \nputStrLn\n \nchinook\n\n\n\n\n\n\nThis function prints each of our queries to standard output before running them.\nUsing this function will let us see what SQL is executing.\n\n\nOne-to-many\n\n\nBeam supports querying for one-to-many joins. For example, to get every\n\nInvoiceLine\n for each \nInvoice\n, use the \noneToMany_\n combinator.\n\n\n\n                \n\n                    \n        \n\n            \n        \n\n            \nHaskell\n\n        \n\n    \n        \n\n            \nSqlite3\n\n        \n\n    \n        \n\n            \nPostgres\n\n        \n\n    \n        \n\n    \n        \n\n            \n        \n\n            \ndo\n \ni\n \n-\n \nall_\n \n(\ninvoice\n \nchinookDb\n)\n\n   \nln\n \n-\n \noneToMany_\n \n(\ninvoiceLine\n \nchinookDb\n)\n \ninvoiceLineInvoice\n \ni\n\n   \npure\n \n(\ni\n,\n \nln\n)\n\n\n\n\n        \n\n    \n        \n\n            \nSELECT \nt0\n.\nInvoiceId\n AS \nres0\n,\n\n\n       \nt0\n.\nCustomerId\n AS \nres1\n,\n\n\n       \nt0\n.\nInvoiceDate\n AS \nres2\n,\n\n\n       \nt0\n.\nBillingAddress\n AS \nres3\n,\n\n\n       \nt0\n.\nBillingCity\n AS \nres4\n,\n\n\n       \nt0\n.\nBillingState\n AS \nres5\n,\n\n\n       \nt0\n.\nBillingCountry\n AS \nres6\n,\n\n\n       \nt0\n.\nBillingPostalCode\n AS \nres7\n,\n\n\n       \nt0\n.\nTotal\n AS \nres8\n,\n\n\n       \nt1\n.\nInvoiceLineId\n AS \nres9\n,\n\n\n       \nt1\n.\nInvoiceId\n AS \nres10\n,\n\n\n       \nt1\n.\nTrackId\n AS \nres11\n,\n\n\n       \nt1\n.\nUnitPrice\n AS \nres12\n,\n\n\n       \nt1\n.\nQuantity\n AS \nres13\n\n\nFROM \nInvoice\n AS \nt0\n\n\nINNER JOIN \nInvoiceLine\n AS \nt1\n ON (\nt1\n.\nInvoiceId\n)=(\nt0\n.\nInvoiceId\n)\n\n\n\n\n        \n\n    \n        \n\n            \nSELECT\n \nt0\n.\nInvoiceId\n \nAS\n \nres0\n,\n\n       \nt0\n.\nCustomerId\n \nAS\n \nres1\n,\n\n       \nt0\n.\nInvoiceDate\n \nAS\n \nres2\n,\n\n       \nt0\n.\nBillingAddress\n \nAS\n \nres3\n,\n\n       \nt0\n.\nBillingCity\n \nAS\n \nres4\n,\n\n       \nt0\n.\nBillingState\n \nAS\n \nres5\n,\n\n       \nt0\n.\nBillingCountry\n \nAS\n \nres6\n,\n\n       \nt0\n.\nBillingPostalCode\n \nAS\n \nres7\n,\n\n       \nt0\n.\nTotal\n \nAS\n \nres8\n,\n\n       \nt1\n.\nInvoiceLineId\n \nAS\n \nres9\n,\n\n       \nt1\n.\nInvoiceId\n \nAS\n \nres10\n,\n\n       \nt1\n.\nTrackId\n \nAS\n \nres11\n,\n\n       \nt1\n.\nUnitPrice\n \nAS\n \nres12\n,\n\n       \nt1\n.\nQuantity\n \nAS\n \nres13\n\n\nFROM\n \nInvoice\n \nAS\n \nt0\n\n\nINNER\n \nJOIN\n \nInvoiceLine\n \nAS\n \nt1\n \nON\n \n(\nt1\n.\nInvoiceId\n)\n \n=\n \n(\nt0\n.\nInvoiceId\n)\n\n\n\n\n        \n\n    \n        \n\n    \n                \n\n                    \n\n\nOr, if you have an actual \nInvoice\n (called \noneInvoice\n) and you want all the\nassociated \nInvoiceLine\ns, you can use \nval_\n to convert \noneInvoice\n to the SQL\nexpression level.\n\n\noneToMany_\n \n(\ninvoiceLine\n \nchinookDb\n)\n \ninvoiceLineInvoice\n \n(\nval_\n \ni\n)\n\n\n\n\n\n\nIf you find yourself repeating yourself constantly, you can define a helper.\n\n\ninvoiceLines_\n \n::\n \nOneToMany\n \nInvoiceT\n \nInvoiceLineT\n\n\ninvoiceLines_\n \n=\n \noneToMany_\n \n(\ninvoiceLine\n \nchinookDb\n)\n \ninvoiceLineInvoice\n\n\n\n\n\n\nThen the above queries become\n\n\ndo\n \ni\n \n-\n \nall_\n \n(\ninvoice\n \nchinookDb\n)\n\n   \nln\n \n-\n \ninvoiceLines_\n \ni\n\n\n\n\n\n\nand\n\n\ninvoiceLines\n \n(\nval_\n \ni\n)\n\n\n\n\n\n\nNullable columns\n\n\nIf you have a nullable foreign key in your many table, you can use\n\noneToManyOptional_\n and \nOneToManyOptional\n, respectively. For example, \n\n\nOne-to-one\n\n\nOne to one relationships are a special case of one to many relationships, save\nfor a unique constraint on one column. Thus, there are no special constructs for\none-to-one relationships.\n\n\nFor convenience, \noneToOne_\n and \nOneToOne\n are equivalent to \noneToMany_\n and\n\nOneToMany\n. Additionally, \noneToMaybe_\n and \nOneToMaybe\n correspond to\n\noneToManyOptional_\n and \nOneToManyOptional\n.\n\n\nMany-to-many\n\n\nMany to many relationships require a linking table, with foreign keys to each\ntable part of the relationship.\n\n\nThe \nmanyToMany_\n construct can be used to fetch both, one, or no sides of a\nmany-to-many relationship.\n\n\nmanyToMany_\n\n  \n::\n \n(\n \nDatabase\n \ndb\n,\n \nTable\n \njoinThrough\n\n     \n,\n \nTable\n \nleft\n,\n \nTable\n \nright\n\n     \n,\n \nSql92SelectSanityCheck\n \nsyntax\n\n     \n,\n \nIsSql92SelectSyntax\n \nsyntax\n\n\n     \n,\n \nSqlEq\n \n(\nQExpr\n \n(\nSql92SelectExpressionSyntax\n \nsyntax\n)\n \ns\n)\n \n(\nPrimaryKey\n \nleft\n \n(\nQExpr\n \n(\nSql92SelectExpressionSyntax\n \nsyntax\n)\n \ns\n))\n\n     \n,\n \nSqlEq\n \n(\nQExpr\n \n(\nSql92SelectExpressionSyntax\n \nsyntax\n)\n \ns\n)\n \n(\nPrimaryKey\n \nright\n \n(\nQExpr\n \n(\nSql92SelectExpressionSyntax\n \nsyntax\n)\n \ns\n))\n \n)\n\n  \n=\n \nDatabaseEntity\n \nbe\n \ndb\n \n(\nTableEntity\n \njoinThrough\n)\n\n  \n-\n \n(\njoinThrough\n \n(\nQExpr\n \n(\nSql92SelectExpressionSyntax\n \nsyntax\n)\n \ns\n)\n \n-\n \nPrimaryKey\n \nleft\n \n(\nQExpr\n \n(\nSql92SelectExpressionSyntax\n \nsyntax\n)\n \ns\n))\n\n  \n-\n \n(\njoinThrough\n \n(\nQExpr\n \n(\nSql92SelectExpressionSyntax\n \nsyntax\n)\n \ns\n)\n \n-\n \nPrimaryKey\n \nright\n \n(\nQExpr\n \n(\nSql92SelectExpressionSyntax\n \nsyntax\n)\n \ns\n))\n\n  \n-\n \nQ\n \nsyntax\n \ndb\n \ns\n \n(\nleft\n \n(\nQExpr\n \n(\nSql92SelectExpressionSyntax\n \nsyntax\n)\n \ns\n))\n \n-\n \nQ\n \nsyntax\n \ndb\n \ns\n \n(\nright\n \n(\nQExpr\n \n(\nSql92SelectExpressionSyntax\n \nsyntax\n)\n \ns\n))\n\n  \n-\n \nQ\n \nsyntax\n \ndb\n \ns\n \n(\nleft\n \n(\nQExpr\n \n(\nSql92SelectExpressionSyntax\n \nsyntax\n)\n \ns\n),\n \nright\n \n(\nQExpr\n \n(\nSql92SelectExpressionSyntax\n \nsyntax\n)\n \ns\n))\n\n\n\n\n\n\nThis reads: for any database \ndb\n; tables \njoinThrough\n, \nleft\n, and \nright\n;\nand sane select syntax \nsyntax\n, where the primary keys of \nleft\n and \nright\n\nare comparable as value expressions and we have some way of extracting a primary\nkey of \nleft\n and \nright\n from \njoinThrough\n, associate all entries of \nleft\n\nwith those of \nright\n through \njoinThrough\n and return the results of \nleft\n and\n\nright\n.\n\n\nThe Chinook database associates multiple tracks with a playlist via the\n\nplaylist_track\n table. For example, to get all tracks from the playlists named\neither \"Movies\" or \"Music\".\n\n\n\n                \n\n                    \n        \n\n            \n        \n\n            \nHaskell\n\n        \n\n    \n        \n\n            \nSqlite3\n\n        \n\n    \n        \n\n            \nPostgres\n\n        \n\n    \n        \n\n    \n        \n\n            \n        \n\n            \nmanyToMany_\n \n(\nplaylistTrack\n \nchinookDb\n)\n\n            \nplaylistTrackPlaylistId\n \nplaylistTrackTrackId\n\n\n            \n(\nfilter_\n \n(\n\\\np\n \n-\n \nplaylistName\n \np\n \n==.\n \njust_\n \n(\nval_\n \nMusic\n)\n \n||.\n\n                            \nplaylistName\n \np\n \n==.\n \njust_\n \n(\nval_\n \nMovies\n))\n\n                     \n(\nall_\n \n(\nplaylist\n \nchinookDb\n)))\n\n\n            \n(\nall_\n \n(\ntrack\n \nchinookDb\n))\n\n\n\n\n        \n\n    \n        \n\n            \nSELECT \nt0\n.\nPlaylistId\n AS \nres0\n,\n\n\n       \nt0\n.\nName\n AS \nres1\n,\n\n\n       \nt1\n.\nTrackId\n AS \nres2\n,\n\n\n       \nt1\n.\nName\n AS \nres3\n,\n\n\n       \nt1\n.\nAlbumId\n AS \nres4\n,\n\n\n       \nt1\n.\nMediaTypeId\n AS \nres5\n,\n\n\n       \nt1\n.\nGenreId\n AS \nres6\n,\n\n\n       \nt1\n.\nComposer\n AS \nres7\n,\n\n\n       \nt1\n.\nMilliseconds\n AS \nres8\n,\n\n\n       \nt1\n.\nBytes\n AS \nres9\n,\n\n\n       \nt1\n.\nUnitPrice\n AS \nres10\n\n\nFROM \nPlaylist\n AS \nt0\n\n\nINNER JOIN \nTrack\n AS \nt1\n\n\nINNER JOIN \nPlaylistTrack\n AS \nt2\n ON ((\nt2\n.\nPlaylistId\n)=(\nt0\n.\nPlaylistId\n))\n\n\nAND ((\nt2\n.\nTrackId\n)=(\nt1\n.\nTrackId\n))\n\n\nWHERE ((\nt0\n.\nName\n)=(?))\n\n\n  OR ((\nt0\n.\nName\n)=(?))\n\n\n\n\n        \n\n    \n        \n\n            \nSELECT\n \nt0\n.\nPlaylistId\n \nAS\n \nres0\n,\n\n       \nt0\n.\nName\n \nAS\n \nres1\n,\n\n       \nt1\n.\nTrackId\n \nAS\n \nres2\n,\n\n       \nt1\n.\nName\n \nAS\n \nres3\n,\n\n       \nt1\n.\nAlbumId\n \nAS\n \nres4\n,\n\n       \nt1\n.\nMediaTypeId\n \nAS\n \nres5\n,\n\n       \nt1\n.\nGenreId\n \nAS\n \nres6\n,\n\n       \nt1\n.\nComposer\n \nAS\n \nres7\n,\n\n       \nt1\n.\nMilliseconds\n \nAS\n \nres8\n,\n\n       \nt1\n.\nBytes\n \nAS\n \nres9\n,\n\n       \nt1\n.\nUnitPrice\n \nAS\n \nres10\n\n\nFROM\n \nPlaylist\n \nAS\n \nt0\n\n\nINNER\n \nJOIN\n \nTrack\n \nAS\n \nt1\n\n\nINNER\n \nJOIN\n \nPlaylistTrack\n \nAS\n \nt2\n \nON\n \n((\nt2\n.\nPlaylistId\n)\n \n=\n \n(\nt0\n.\nPlaylistId\n))\n\n\nAND\n \n((\nt2\n.\nTrackId\n)\n \n=\n \n(\nt1\n.\nTrackId\n))\n\n\nWHERE\n \n((\nt0\n.\nName\n)\n \n=\n \n(\nMusic\n))\n\n  \nOR\n \n((\nt0\n.\nName\n)\n \n=\n \n(\nMovies\n))\n\n\n\n\n        \n\n    \n        \n\n    \n                \n\n                    \n\n\nMany-to-many with arbitrary data\n\n\nSometimes you want to have additional data for each relationship. For this, use\n\nmanyToManyPassthrough_\n.\n\n\nmanyToManyPassthrough_\n\n  \n::\n \n(\n \nDatabase\n \ndb\n,\n \nTable\n \njoinThrough\n\n     \n,\n \nTable\n \nleft\n,\n \nTable\n \nright\n\n     \n,\n \nSql92SelectSanityCheck\n \nsyntax\n\n     \n,\n \nIsSql92SelectSyntax\n \nsyntax\n\n\n     \n,\n \nSqlEq\n \n(\nQExpr\n \n(\nSql92SelectExpressionSyntax\n \nsyntax\n)\n \ns\n)\n \n(\nPrimaryKey\n \nleft\n \n(\nQExpr\n \n(\nSql92SelectExpressionSyntax\n \nsyntax\n)\n \ns\n))\n\n     \n,\n \nSqlEq\n \n(\nQExpr\n \n(\nSql92SelectExpressionSyntax\n \nsyntax\n)\n \ns\n)\n \n(\nPrimaryKey\n \nright\n \n(\nQExpr\n \n(\nSql92SelectExpressionSyntax\n \nsyntax\n)\n \ns\n))\n \n)\n\n  \n=\n \nDatabaseEntity\n \nbe\n \ndb\n \n(\nTableEntity\n \njoinThrough\n)\n\n  \n-\n \n(\njoinThrough\n \n(\nQExpr\n \n(\nSql92SelectExpressionSyntax\n \nsyntax\n)\n \ns\n)\n \n-\n \nPrimaryKey\n \nleft\n \n(\nQExpr\n \n(\nSql92SelectExpressionSyntax\n \nsyntax\n)\n \ns\n))\n\n  \n-\n \n(\njoinThrough\n \n(\nQExpr\n \n(\nSql92SelectExpressionSyntax\n \nsyntax\n)\n \ns\n)\n \n-\n \nPrimaryKey\n \nright\n \n(\nQExpr\n \n(\nSql92SelectExpressionSyntax\n \nsyntax\n)\n \ns\n))\n\n  \n-\n \nQ\n \nsyntax\n \ndb\n \ns\n \n(\nleft\n \n(\nQExpr\n \n(\nSql92SelectExpressionSyntax\n \nsyntax\n)\n \ns\n))\n\n  \n-\n \nQ\n \nsyntax\n \ndb\n \ns\n \n(\nright\n \n(\nQExpr\n \n(\nSql92SelectExpressionSyntax\n \nsyntax\n)\n \ns\n))\n\n  \n-\n \nQ\n \nsyntax\n \ndb\n \ns\n \n(\n \njoinThrough\n \n(\nQExpr\n \n(\nSql92SelectExpressionSyntax\n \nsyntax\n)\n \ns\n)\n\n                   \n,\n \nleft\n \n(\nQExpr\n \n(\nSql92SelectExpressionSyntax\n \nsyntax\n)\n \ns\n)\n\n                   \n,\n \nright\n \n(\nQExpr\n \n(\nSql92SelectExpressionSyntax\n \nsyntax\n)\n \ns\n))\n\n\n\n\n\n\nUnder the hood \nmanyToMany_\n is defined simply as \n\n\nmanyToMany_\n \n=\n \nfmap\n \n(\n\\\n(\n_\n,\n \nleft\n,\n \nright\n)\n \n-\n \n(\nleft\n,\n \nright\n))\n \nmanyToManyPassthrough_\n\n\n\n\n\n\nDeclaring many-to-many relationships\n\n\nLike one-to-many relationships, beam allows you to extract commonly used\nmany-to-many relationships, via the \nManyToMany\n type.\n\n\nFor example, the playlist/track relationship above can be defined as follows\n\n\nplaylistTrackRelationship\n \n::\n \nManyToMany\n \nChinookDb\n \nPlaylistT\n \nTrackT\n\n\nplaylistTrackRelationshipu\n \n=\n\n  \nmanyToMany_\n \n(\nplaylistTrack\n \nchinookDb\n)\n \nplaylistTrackPlaylistId\n \nplaylistTrackTrackId\n\n\n\n\n\n\nAnd we can use it as expected:\n\n\n\n                \n\n                    \n        \n\n            \n        \n\n            \nHaskell\n\n        \n\n    \n        \n\n            \nSqlite3\n\n        \n\n    \n        \n\n            \nPostgres\n\n        \n\n    \n        \n\n    \n        \n\n            \n        \n\n            \nplaylistTrackRelationship\n\n    \n(\nfilter_\n \n(\n\\\np\n \n-\n \nplaylistName\n \np\n \n==.\n \njust_\n \n(\nval_\n \nMusic\n)\n \n||.\n\n                    \nplaylistName\n \np\n \n==.\n \njust_\n \n(\nval_\n \nMovies\n))\n\n             \n(\nall_\n \n(\nplaylist\n \nchinookDb\n)))\n\n\n    \n(\nall_\n \n(\ntrack\n \nchinookDb\n))\n\n\n\n\n        \n\n    \n        \n\n            \nSELECT \nt0\n.\nPlaylistId\n AS \nres0\n,\n\n\n       \nt0\n.\nName\n AS \nres1\n,\n\n\n       \nt1\n.\nTrackId\n AS \nres2\n,\n\n\n       \nt1\n.\nName\n AS \nres3\n,\n\n\n       \nt1\n.\nAlbumId\n AS \nres4\n,\n\n\n       \nt1\n.\nMediaTypeId\n AS \nres5\n,\n\n\n       \nt1\n.\nGenreId\n AS \nres6\n,\n\n\n       \nt1\n.\nComposer\n AS \nres7\n,\n\n\n       \nt1\n.\nMilliseconds\n AS \nres8\n,\n\n\n       \nt1\n.\nBytes\n AS \nres9\n,\n\n\n       \nt1\n.\nUnitPrice\n AS \nres10\n\n\nFROM \nPlaylist\n AS \nt0\n\n\nINNER JOIN \nTrack\n AS \nt1\n\n\nINNER JOIN \nPlaylistTrack\n AS \nt2\n ON ((\nt2\n.\nPlaylistId\n)=(\nt0\n.\nPlaylistId\n))\n\n\nAND ((\nt2\n.\nTrackId\n)=(\nt1\n.\nTrackId\n))\n\n\nWHERE ((\nt0\n.\nName\n)=(?))\n\n\n  OR ((\nt0\n.\nName\n)=(?))\n\n\n\n\n        \n\n    \n        \n\n            \nSELECT\n \nt0\n.\nPlaylistId\n \nAS\n \nres0\n,\n\n       \nt0\n.\nName\n \nAS\n \nres1\n,\n\n       \nt1\n.\nTrackId\n \nAS\n \nres2\n,\n\n       \nt1\n.\nName\n \nAS\n \nres3\n,\n\n       \nt1\n.\nAlbumId\n \nAS\n \nres4\n,\n\n       \nt1\n.\nMediaTypeId\n \nAS\n \nres5\n,\n\n       \nt1\n.\nGenreId\n \nAS\n \nres6\n,\n\n       \nt1\n.\nComposer\n \nAS\n \nres7\n,\n\n       \nt1\n.\nMilliseconds\n \nAS\n \nres8\n,\n\n       \nt1\n.\nBytes\n \nAS\n \nres9\n,\n\n       \nt1\n.\nUnitPrice\n \nAS\n \nres10\n\n\nFROM\n \nPlaylist\n \nAS\n \nt0\n\n\nINNER\n \nJOIN\n \nTrack\n \nAS\n \nt1\n\n\nINNER\n \nJOIN\n \nPlaylistTrack\n \nAS\n \nt2\n \nON\n \n((\nt2\n.\nPlaylistId\n)\n \n=\n \n(\nt0\n.\nPlaylistId\n))\n\n\nAND\n \n((\nt2\n.\nTrackId\n)\n \n=\n \n(\nt1\n.\nTrackId\n))\n\n\nWHERE\n \n((\nt0\n.\nName\n)\n \n=\n \n(\nMusic\n))\n\n  \nOR\n \n((\nt0\n.\nName\n)\n \n=\n \n(\nMovies\n))\n\n\n\n\n        \n\n    \n        \n\n    \n                \n\n                    \n\n\nManyToManyThrough\n is the equivalent for \nmanyToManyThrough_\n, except it takes\nanother table parameter for the 'through' table.\n\n\nArbitrary Joins\n\n\nJoins with arbitrary conditions can be specified using the \njoin_\n construct.\nFor example, \noneToMany_\n is implemented as\n\n\noneToMany_\n \nrel\n \ngetKey\n \ntbl\n \n=\n\n  \njoin_\n \nrel\n \n(\n\\\nrel\n \n-\n \ngetKey\n \nrel\n \n==.\n \npk\n \ntbl\n)\n\n\n\n\n\n\nThus, the invoice example above could be rewritten. For example, instead of \n\n\ndo\n \ni\n \n-\n \nall_\n \n(\ninvoice\n \nchinookDb\n)\n\n   \nln\n \n-\n \noneToMany_\n \n(\ninvoiceLine\n \nchinookDb\n)\n \ninvoiceLineInvoice\n \ni\n\n   \npure\n \n(\ni\n,\n \nln\n)\n\n\n\n\n\n\nWe could write\n\n\n\n                \n\n                    \n        \n\n            \n        \n\n            \nHaskell\n\n        \n\n    \n        \n\n            \nSqlite3\n\n        \n\n    \n        \n\n            \nPostgres\n\n        \n\n    \n        \n\n    \n        \n\n            \n        \n\n            \ndo\n \ni\n \n-\n \nall_\n \n(\ninvoice\n \nchinookDb\n)\n\n   \nln\n \n-\n \njoin_\n \n(\ninvoiceLine\n \nchinookDb\n)\n \n(\n\\\nline\n \n-\n \ninvoiceLineInvoice\n \nline\n \n==.\n \nprimaryKey\n \ni\n)\n\n   \npure\n \n(\ni\n,\n \nln\n)\n\n\n\n\n        \n\n    \n        \n\n            \nSELECT \nt0\n.\nInvoiceId\n AS \nres0\n,\n\n\n       \nt0\n.\nCustomerId\n AS \nres1\n,\n\n\n       \nt0\n.\nInvoiceDate\n AS \nres2\n,\n\n\n       \nt0\n.\nBillingAddress\n AS \nres3\n,\n\n\n       \nt0\n.\nBillingCity\n AS \nres4\n,\n\n\n       \nt0\n.\nBillingState\n AS \nres5\n,\n\n\n       \nt0\n.\nBillingCountry\n AS \nres6\n,\n\n\n       \nt0\n.\nBillingPostalCode\n AS \nres7\n,\n\n\n       \nt0\n.\nTotal\n AS \nres8\n,\n\n\n       \nt1\n.\nInvoiceLineId\n AS \nres9\n,\n\n\n       \nt1\n.\nInvoiceId\n AS \nres10\n,\n\n\n       \nt1\n.\nTrackId\n AS \nres11\n,\n\n\n       \nt1\n.\nUnitPrice\n AS \nres12\n,\n\n\n       \nt1\n.\nQuantity\n AS \nres13\n\n\nFROM \nInvoice\n AS \nt0\n\n\nINNER JOIN \nInvoiceLine\n AS \nt1\n ON (\nt1\n.\nInvoiceId\n)=(\nt0\n.\nInvoiceId\n)\n\n\n\n\n        \n\n    \n        \n\n            \nSELECT\n \nt0\n.\nInvoiceId\n \nAS\n \nres0\n,\n\n       \nt0\n.\nCustomerId\n \nAS\n \nres1\n,\n\n       \nt0\n.\nInvoiceDate\n \nAS\n \nres2\n,\n\n       \nt0\n.\nBillingAddress\n \nAS\n \nres3\n,\n\n       \nt0\n.\nBillingCity\n \nAS\n \nres4\n,\n\n       \nt0\n.\nBillingState\n \nAS\n \nres5\n,\n\n       \nt0\n.\nBillingCountry\n \nAS\n \nres6\n,\n\n       \nt0\n.\nBillingPostalCode\n \nAS\n \nres7\n,\n\n       \nt0\n.\nTotal\n \nAS\n \nres8\n,\n\n       \nt1\n.\nInvoiceLineId\n \nAS\n \nres9\n,\n\n       \nt1\n.\nInvoiceId\n \nAS\n \nres10\n,\n\n       \nt1\n.\nTrackId\n \nAS\n \nres11\n,\n\n       \nt1\n.\nUnitPrice\n \nAS\n \nres12\n,\n\n       \nt1\n.\nQuantity\n \nAS\n \nres13\n\n\nFROM\n \nInvoice\n \nAS\n \nt0\n\n\nINNER\n \nJOIN\n \nInvoiceLine\n \nAS\n \nt1\n \nON\n \n(\nt1\n.\nInvoiceId\n)\n \n=\n \n(\nt0\n.\nInvoiceId\n)\n\n\n\n\n        \n\n    \n        \n\n    \n                \n\n                    \n\n\nOuter joins\n\n\nLeft and right joins\n\n\nLeft joins with arbitrary conditions can be specified with the \nleftJoin_\n\nconstruct. \nleftJoin_\n takes a table and a join condition. It associates each\nresult record with a record of the table given or an fully NULL row of that\ntable in case no row matches. For this reason, the result of \nleftJoin_\n has an\nextra \nNullable\n column tag, which converts each field into the corresponding\n\nMaybe\n type.\n\n\n\n\nNote\n\n\nThe table parameter passed in as the join condition does not have a \n\nNullable\n column tag. The join condition should be written as if a \nconcrete row from that table exists.\n\n\n\n\nFor example, to get every artist along with their albums, but always including\nevery artist, use \nleftJoin_\n as follows.\n\n\n\n                \n\n                    \n        \n\n            \n        \n\n            \nHaskell\n\n        \n\n    \n        \n\n            \nSqlite3\n\n        \n\n    \n        \n\n            \nPostgres\n\n        \n\n    \n        \n\n    \n        \n\n            \n        \n\n            \ndo\n \nartist\n \n-\n \nall_\n \n(\nartist\n \nchinookDb\n)\n\n   \nalbum\n  \n-\n \nleftJoin_\n \n(\nalbum\n \nchinookDb\n)\n \n(\n\\\nalbum\n \n-\n \nalbumArtist\n \nalbum\n \n==.\n \nprimaryKey\n \nartist\n)\n\n   \npure\n \n(\nartist\n,\n \nalbum\n)\n\n\n\n\n        \n\n    \n        \n\n            \nSELECT \nt0\n.\nArtistId\n AS \nres0\n,\n\n\n       \nt0\n.\nName\n AS \nres1\n,\n\n\n       \nt1\n.\nAlbumId\n AS \nres2\n,\n\n\n       \nt1\n.\nTitle\n AS \nres3\n,\n\n\n       \nt1\n.\nArtistId\n AS \nres4\n\n\nFROM \nArtist\n AS \nt0\n\n\nLEFT JOIN \nAlbum\n AS \nt1\n ON (\nt1\n.\nArtistId\n)=(\nt0\n.\nArtistId\n)\n\n\n\n\n        \n\n    \n        \n\n            \nSELECT\n \nt0\n.\nArtistId\n \nAS\n \nres0\n,\n\n       \nt0\n.\nName\n \nAS\n \nres1\n,\n\n       \nt1\n.\nAlbumId\n \nAS\n \nres2\n,\n\n       \nt1\n.\nTitle\n \nAS\n \nres3\n,\n\n       \nt1\n.\nArtistId\n \nAS\n \nres4\n\n\nFROM\n \nArtist\n \nAS\n \nt0\n\n\nLEFT\n \nJOIN\n \nAlbum\n \nAS\n \nt1\n \nON\n \n(\nt1\n.\nArtistId\n)\n \n=\n \n(\nt0\n.\nArtistId\n)\n\n\n\n\n        \n\n    \n        \n\n    \n                \n\n                    \n\n\nRight joins are not yet supported. They can always be rewritten as left joins.\nIf you have a compelling use case, please file an issue!\n\n\nFull Outer joins\n\n\n\n\nTODO\n\n\nouterJoin_\n not yet supported\n\n\n\n\nFull outer joins are supported via the \nouterJoin_\n construct.\n\n\nSubqueries\n\n\nSometimes you want to join against a \nsubquery\n rather than a table. For the\nmost part, beam will automatically figure out when certain queries need to be\nwritten using subqueries. For example, to join two result sets cointaining a SQL\nLIMIT, you would normally have to write both queries as subqueries. In beam, you\ncan write such queries as you'd expect. The library takes care of creating\nsubqueries as expected.\n\n\nFor example, the following query generates the code you'd expect.\n\n\n\n                \n\n                    \n        \n\n            \n        \n\n            \nHaskell\n\n        \n\n    \n        \n\n            \nSqlite3\n\n        \n\n    \n        \n\n            \nPostgres\n\n        \n\n    \n        \n\n    \n        \n\n            \n        \n\n            \ndo\n \ni\n \n-\n \nlimit_\n \n10\n \n$\n \nall_\n \n(\ninvoice\n \nchinookDb\n)\n\n   \nline\n \n-\n \ninvoiceLines\n \ni\n\n   \npure\n \n(\ni\n,\n \nline\n)\n\n\n\n\n        \n\n    \n        \n\n            \nSELECT \nt0\n.\nres0\n AS \nres0\n,\n\n\n       \nt0\n.\nres1\n AS \nres1\n,\n\n\n       \nt0\n.\nres2\n AS \nres2\n,\n\n\n       \nt0\n.\nres3\n AS \nres3\n,\n\n\n       \nt0\n.\nres4\n AS \nres4\n,\n\n\n       \nt0\n.\nres5\n AS \nres5\n,\n\n\n       \nt0\n.\nres6\n AS \nres6\n,\n\n\n       \nt0\n.\nres7\n AS \nres7\n,\n\n\n       \nt0\n.\nres8\n AS \nres8\n,\n\n\n       \nt1\n.\nInvoiceLineId\n AS \nres9\n,\n\n\n       \nt1\n.\nInvoiceId\n AS \nres10\n,\n\n\n       \nt1\n.\nTrackId\n AS \nres11\n,\n\n\n       \nt1\n.\nUnitPrice\n AS \nres12\n,\n\n\n       \nt1\n.\nQuantity\n AS \nres13\n\n\nFROM\n\n\n  (SELECT \nt0\n.\nInvoiceId\n AS \nres0\n,\n\n\n          \nt0\n.\nCustomerId\n AS \nres1\n,\n\n\n          \nt0\n.\nInvoiceDate\n AS \nres2\n,\n\n\n          \nt0\n.\nBillingAddress\n AS \nres3\n,\n\n\n          \nt0\n.\nBillingCity\n AS \nres4\n,\n\n\n          \nt0\n.\nBillingState\n AS \nres5\n,\n\n\n          \nt0\n.\nBillingCountry\n AS \nres6\n,\n\n\n          \nt0\n.\nBillingPostalCode\n AS \nres7\n,\n\n\n          \nt0\n.\nTotal\n AS \nres8\n\n\n   FROM \nInvoice\n AS \nt0\n\n\n   LIMIT 10) AS \nt0\n\n\nINNER JOIN \nInvoiceLine\n AS \nt1\n ON (\nt1\n.\nInvoiceId\n)=(\nt0\n.\nres0\n)\n\n\n\n\n        \n\n    \n        \n\n            \nSELECT\n \nt0\n.\nres0\n \nAS\n \nres0\n,\n\n       \nt0\n.\nres1\n \nAS\n \nres1\n,\n\n       \nt0\n.\nres2\n \nAS\n \nres2\n,\n\n       \nt0\n.\nres3\n \nAS\n \nres3\n,\n\n       \nt0\n.\nres4\n \nAS\n \nres4\n,\n\n       \nt0\n.\nres5\n \nAS\n \nres5\n,\n\n       \nt0\n.\nres6\n \nAS\n \nres6\n,\n\n       \nt0\n.\nres7\n \nAS\n \nres7\n,\n\n       \nt0\n.\nres8\n \nAS\n \nres8\n,\n\n       \nt1\n.\nInvoiceLineId\n \nAS\n \nres9\n,\n\n       \nt1\n.\nInvoiceId\n \nAS\n \nres10\n,\n\n       \nt1\n.\nTrackId\n \nAS\n \nres11\n,\n\n       \nt1\n.\nUnitPrice\n \nAS\n \nres12\n,\n\n       \nt1\n.\nQuantity\n \nAS\n \nres13\n\n\nFROM\n\n  \n(\nSELECT\n \nt0\n.\nInvoiceId\n \nAS\n \nres0\n,\n\n          \nt0\n.\nCustomerId\n \nAS\n \nres1\n,\n\n          \nt0\n.\nInvoiceDate\n \nAS\n \nres2\n,\n\n          \nt0\n.\nBillingAddress\n \nAS\n \nres3\n,\n\n          \nt0\n.\nBillingCity\n \nAS\n \nres4\n,\n\n          \nt0\n.\nBillingState\n \nAS\n \nres5\n,\n\n          \nt0\n.\nBillingCountry\n \nAS\n \nres6\n,\n\n          \nt0\n.\nBillingPostalCode\n \nAS\n \nres7\n,\n\n          \nt0\n.\nTotal\n \nAS\n \nres8\n\n   \nFROM\n \nInvoice\n \nAS\n \nt0\n\n   \nLIMIT\n \n10\n)\n \nAS\n \nt0\n\n\nINNER\n \nJOIN\n \nInvoiceLine\n \nAS\n \nt1\n \nON\n \n(\nt1\n.\nInvoiceId\n)\n \n=\n \n(\nt0\n.\nres0\n)\n\n\n\n\n        \n\n    \n        \n\n    \n                \n\n                    \n\n\nIf you need to (for efficiency for example), you can also generate subqueries\nexplicitly, using \nsubselect_\n. The \nsubselect_\n will force a new query to be\noutput in most cases. For simple queries, such as \nall_\n, \nsubselect_\n will have\nno effect.\n\n\n\n                \n\n                    \n        \n\n            \n        \n\n            \nHaskell\n\n        \n\n    \n        \n\n            \nSqlite3\n\n        \n\n    \n        \n\n            \nPostgres\n\n        \n\n    \n        \n\n    \n        \n\n            \n        \n\n            \n-- Same as above, but with explicit sub select\n\n\ndo\n \ni\n \n-\n \nsubselect_\n \n$\n \nlimit_\n \n10\n \n$\n \nall_\n \n(\ninvoice\n \nchinookDb\n)\n\n   \nline\n \n-\n \ninvoiceLines\n \ni\n\n   \npure\n \n(\ni\n,\n \nline\n)\n\n\n\n\n        \n\n    \n        \n\n            \nSELECT \nt0\n.\nres0\n AS \nres0\n,\n\n\n       \nt0\n.\nres1\n AS \nres1\n,\n\n\n       \nt0\n.\nres2\n AS \nres2\n,\n\n\n       \nt0\n.\nres3\n AS \nres3\n,\n\n\n       \nt0\n.\nres4\n AS \nres4\n,\n\n\n       \nt0\n.\nres5\n AS \nres5\n,\n\n\n       \nt0\n.\nres6\n AS \nres6\n,\n\n\n       \nt0\n.\nres7\n AS \nres7\n,\n\n\n       \nt0\n.\nres8\n AS \nres8\n,\n\n\n       \nt1\n.\nInvoiceLineId\n AS \nres9\n,\n\n\n       \nt1\n.\nInvoiceId\n AS \nres10\n,\n\n\n       \nt1\n.\nTrackId\n AS \nres11\n,\n\n\n       \nt1\n.\nUnitPrice\n AS \nres12\n,\n\n\n       \nt1\n.\nQuantity\n AS \nres13\n\n\nFROM\n\n\n  (SELECT \nt0\n.\nInvoiceId\n AS \nres0\n,\n\n\n          \nt0\n.\nCustomerId\n AS \nres1\n,\n\n\n          \nt0\n.\nInvoiceDate\n AS \nres2\n,\n\n\n          \nt0\n.\nBillingAddress\n AS \nres3\n,\n\n\n          \nt0\n.\nBillingCity\n AS \nres4\n,\n\n\n          \nt0\n.\nBillingState\n AS \nres5\n,\n\n\n          \nt0\n.\nBillingCountry\n AS \nres6\n,\n\n\n          \nt0\n.\nBillingPostalCode\n AS \nres7\n,\n\n\n          \nt0\n.\nTotal\n AS \nres8\n\n\n   FROM \nInvoice\n AS \nt0\n\n\n   LIMIT 10) AS \nt0\n\n\nINNER JOIN \nInvoiceLine\n AS \nt1\n ON (\nt1\n.\nInvoiceId\n)=(\nt0\n.\nres0\n)\n\n\n\n\n        \n\n    \n        \n\n            \nSELECT\n \nt0\n.\nres0\n \nAS\n \nres0\n,\n\n       \nt0\n.\nres1\n \nAS\n \nres1\n,\n\n       \nt0\n.\nres2\n \nAS\n \nres2\n,\n\n       \nt0\n.\nres3\n \nAS\n \nres3\n,\n\n       \nt0\n.\nres4\n \nAS\n \nres4\n,\n\n       \nt0\n.\nres5\n \nAS\n \nres5\n,\n\n       \nt0\n.\nres6\n \nAS\n \nres6\n,\n\n       \nt0\n.\nres7\n \nAS\n \nres7\n,\n\n       \nt0\n.\nres8\n \nAS\n \nres8\n,\n\n       \nt1\n.\nInvoiceLineId\n \nAS\n \nres9\n,\n\n       \nt1\n.\nInvoiceId\n \nAS\n \nres10\n,\n\n       \nt1\n.\nTrackId\n \nAS\n \nres11\n,\n\n       \nt1\n.\nUnitPrice\n \nAS\n \nres12\n,\n\n       \nt1\n.\nQuantity\n \nAS\n \nres13\n\n\nFROM\n\n  \n(\nSELECT\n \nt0\n.\nInvoiceId\n \nAS\n \nres0\n,\n\n          \nt0\n.\nCustomerId\n \nAS\n \nres1\n,\n\n          \nt0\n.\nInvoiceDate\n \nAS\n \nres2\n,\n\n          \nt0\n.\nBillingAddress\n \nAS\n \nres3\n,\n\n          \nt0\n.\nBillingCity\n \nAS\n \nres4\n,\n\n          \nt0\n.\nBillingState\n \nAS\n \nres5\n,\n\n          \nt0\n.\nBillingCountry\n \nAS\n \nres6\n,\n\n          \nt0\n.\nBillingPostalCode\n \nAS\n \nres7\n,\n\n          \nt0\n.\nTotal\n \nAS\n \nres8\n\n   \nFROM\n \nInvoice\n \nAS\n \nt0\n\n   \nLIMIT\n \n10\n)\n \nAS\n \nt0\n\n\nINNER\n \nJOIN\n \nInvoiceLine\n \nAS\n \nt1\n \nON\n \n(\nt1\n.\nInvoiceId\n)\n \n=\n \n(\nt0\n.\nres0\n)", 
            "title": "Relationships"
        }, 
        {
            "location": "/user-guide/queries/relationships/#one-to-many", 
            "text": "Beam supports querying for one-to-many joins. For example, to get every InvoiceLine  for each  Invoice , use the  oneToMany_  combinator.  \n                 \n                    \n         \n            \n         \n             Haskell \n         \n    \n         \n             Sqlite3 \n         \n    \n         \n             Postgres \n         \n    \n         \n    \n         \n            \n         \n             do   i   -   all_   ( invoice   chinookDb ) \n    ln   -   oneToMany_   ( invoiceLine   chinookDb )   invoiceLineInvoice   i \n    pure   ( i ,   ln )  \n\n         \n    \n         \n             SELECT  t0 . InvoiceId  AS  res0 ,          t0 . CustomerId  AS  res1 ,          t0 . InvoiceDate  AS  res2 ,          t0 . BillingAddress  AS  res3 ,          t0 . BillingCity  AS  res4 ,          t0 . BillingState  AS  res5 ,          t0 . BillingCountry  AS  res6 ,          t0 . BillingPostalCode  AS  res7 ,          t0 . Total  AS  res8 ,          t1 . InvoiceLineId  AS  res9 ,          t1 . InvoiceId  AS  res10 ,          t1 . TrackId  AS  res11 ,          t1 . UnitPrice  AS  res12 ,          t1 . Quantity  AS  res13  FROM  Invoice  AS  t0  INNER JOIN  InvoiceLine  AS  t1  ON ( t1 . InvoiceId )=( t0 . InvoiceId )  \n\n         \n    \n         \n             SELECT   t0 . InvoiceId   AS   res0 , \n        t0 . CustomerId   AS   res1 , \n        t0 . InvoiceDate   AS   res2 , \n        t0 . BillingAddress   AS   res3 , \n        t0 . BillingCity   AS   res4 , \n        t0 . BillingState   AS   res5 , \n        t0 . BillingCountry   AS   res6 , \n        t0 . BillingPostalCode   AS   res7 , \n        t0 . Total   AS   res8 , \n        t1 . InvoiceLineId   AS   res9 , \n        t1 . InvoiceId   AS   res10 , \n        t1 . TrackId   AS   res11 , \n        t1 . UnitPrice   AS   res12 , \n        t1 . Quantity   AS   res13  FROM   Invoice   AS   t0  INNER   JOIN   InvoiceLine   AS   t1   ON   ( t1 . InvoiceId )   =   ( t0 . InvoiceId )  \n\n         \n    \n         \n    \n                 \n                      Or, if you have an actual  Invoice  (called  oneInvoice ) and you want all the\nassociated  InvoiceLine s, you can use  val_  to convert  oneInvoice  to the SQL\nexpression level.  oneToMany_   ( invoiceLine   chinookDb )   invoiceLineInvoice   ( val_   i )   If you find yourself repeating yourself constantly, you can define a helper.  invoiceLines_   ::   OneToMany   InvoiceT   InvoiceLineT  invoiceLines_   =   oneToMany_   ( invoiceLine   chinookDb )   invoiceLineInvoice   Then the above queries become  do   i   -   all_   ( invoice   chinookDb ) \n    ln   -   invoiceLines_   i   and  invoiceLines   ( val_   i )", 
            "title": "One-to-many"
        }, 
        {
            "location": "/user-guide/queries/relationships/#nullable-columns", 
            "text": "If you have a nullable foreign key in your many table, you can use oneToManyOptional_  and  OneToManyOptional , respectively. For example,", 
            "title": "Nullable columns"
        }, 
        {
            "location": "/user-guide/queries/relationships/#one-to-one", 
            "text": "One to one relationships are a special case of one to many relationships, save\nfor a unique constraint on one column. Thus, there are no special constructs for\none-to-one relationships.  For convenience,  oneToOne_  and  OneToOne  are equivalent to  oneToMany_  and OneToMany . Additionally,  oneToMaybe_  and  OneToMaybe  correspond to oneToManyOptional_  and  OneToManyOptional .", 
            "title": "One-to-one"
        }, 
        {
            "location": "/user-guide/queries/relationships/#many-to-many", 
            "text": "Many to many relationships require a linking table, with foreign keys to each\ntable part of the relationship.  The  manyToMany_  construct can be used to fetch both, one, or no sides of a\nmany-to-many relationship.  manyToMany_ \n   ::   (   Database   db ,   Table   joinThrough \n      ,   Table   left ,   Table   right \n      ,   Sql92SelectSanityCheck   syntax \n      ,   IsSql92SelectSyntax   syntax \n\n      ,   SqlEq   ( QExpr   ( Sql92SelectExpressionSyntax   syntax )   s )   ( PrimaryKey   left   ( QExpr   ( Sql92SelectExpressionSyntax   syntax )   s )) \n      ,   SqlEq   ( QExpr   ( Sql92SelectExpressionSyntax   syntax )   s )   ( PrimaryKey   right   ( QExpr   ( Sql92SelectExpressionSyntax   syntax )   s ))   ) \n   =   DatabaseEntity   be   db   ( TableEntity   joinThrough ) \n   -   ( joinThrough   ( QExpr   ( Sql92SelectExpressionSyntax   syntax )   s )   -   PrimaryKey   left   ( QExpr   ( Sql92SelectExpressionSyntax   syntax )   s )) \n   -   ( joinThrough   ( QExpr   ( Sql92SelectExpressionSyntax   syntax )   s )   -   PrimaryKey   right   ( QExpr   ( Sql92SelectExpressionSyntax   syntax )   s )) \n   -   Q   syntax   db   s   ( left   ( QExpr   ( Sql92SelectExpressionSyntax   syntax )   s ))   -   Q   syntax   db   s   ( right   ( QExpr   ( Sql92SelectExpressionSyntax   syntax )   s )) \n   -   Q   syntax   db   s   ( left   ( QExpr   ( Sql92SelectExpressionSyntax   syntax )   s ),   right   ( QExpr   ( Sql92SelectExpressionSyntax   syntax )   s ))   This reads: for any database  db ; tables  joinThrough ,  left , and  right ;\nand sane select syntax  syntax , where the primary keys of  left  and  right \nare comparable as value expressions and we have some way of extracting a primary\nkey of  left  and  right  from  joinThrough , associate all entries of  left \nwith those of  right  through  joinThrough  and return the results of  left  and right .  The Chinook database associates multiple tracks with a playlist via the playlist_track  table. For example, to get all tracks from the playlists named\neither \"Movies\" or \"Music\".  \n                 \n                    \n         \n            \n         \n             Haskell \n         \n    \n         \n             Sqlite3 \n         \n    \n         \n             Postgres \n         \n    \n         \n    \n         \n            \n         \n             manyToMany_   ( playlistTrack   chinookDb ) \n             playlistTrackPlaylistId   playlistTrackTrackId \n\n             ( filter_   ( \\ p   -   playlistName   p   ==.   just_   ( val_   Music )   ||. \n                             playlistName   p   ==.   just_   ( val_   Movies )) \n                      ( all_   ( playlist   chinookDb ))) \n\n             ( all_   ( track   chinookDb ))  \n\n         \n    \n         \n             SELECT  t0 . PlaylistId  AS  res0 ,          t0 . Name  AS  res1 ,          t1 . TrackId  AS  res2 ,          t1 . Name  AS  res3 ,          t1 . AlbumId  AS  res4 ,          t1 . MediaTypeId  AS  res5 ,          t1 . GenreId  AS  res6 ,          t1 . Composer  AS  res7 ,          t1 . Milliseconds  AS  res8 ,          t1 . Bytes  AS  res9 ,          t1 . UnitPrice  AS  res10  FROM  Playlist  AS  t0  INNER JOIN  Track  AS  t1  INNER JOIN  PlaylistTrack  AS  t2  ON (( t2 . PlaylistId )=( t0 . PlaylistId ))  AND (( t2 . TrackId )=( t1 . TrackId ))  WHERE (( t0 . Name )=(?))    OR (( t0 . Name )=(?))  \n\n         \n    \n         \n             SELECT   t0 . PlaylistId   AS   res0 , \n        t0 . Name   AS   res1 , \n        t1 . TrackId   AS   res2 , \n        t1 . Name   AS   res3 , \n        t1 . AlbumId   AS   res4 , \n        t1 . MediaTypeId   AS   res5 , \n        t1 . GenreId   AS   res6 , \n        t1 . Composer   AS   res7 , \n        t1 . Milliseconds   AS   res8 , \n        t1 . Bytes   AS   res9 , \n        t1 . UnitPrice   AS   res10  FROM   Playlist   AS   t0  INNER   JOIN   Track   AS   t1  INNER   JOIN   PlaylistTrack   AS   t2   ON   (( t2 . PlaylistId )   =   ( t0 . PlaylistId ))  AND   (( t2 . TrackId )   =   ( t1 . TrackId ))  WHERE   (( t0 . Name )   =   ( Music )) \n   OR   (( t0 . Name )   =   ( Movies ))", 
            "title": "Many-to-many"
        }, 
        {
            "location": "/user-guide/queries/relationships/#many-to-many-with-arbitrary-data", 
            "text": "Sometimes you want to have additional data for each relationship. For this, use manyToManyPassthrough_ .  manyToManyPassthrough_ \n   ::   (   Database   db ,   Table   joinThrough \n      ,   Table   left ,   Table   right \n      ,   Sql92SelectSanityCheck   syntax \n      ,   IsSql92SelectSyntax   syntax \n\n      ,   SqlEq   ( QExpr   ( Sql92SelectExpressionSyntax   syntax )   s )   ( PrimaryKey   left   ( QExpr   ( Sql92SelectExpressionSyntax   syntax )   s )) \n      ,   SqlEq   ( QExpr   ( Sql92SelectExpressionSyntax   syntax )   s )   ( PrimaryKey   right   ( QExpr   ( Sql92SelectExpressionSyntax   syntax )   s ))   ) \n   =   DatabaseEntity   be   db   ( TableEntity   joinThrough ) \n   -   ( joinThrough   ( QExpr   ( Sql92SelectExpressionSyntax   syntax )   s )   -   PrimaryKey   left   ( QExpr   ( Sql92SelectExpressionSyntax   syntax )   s )) \n   -   ( joinThrough   ( QExpr   ( Sql92SelectExpressionSyntax   syntax )   s )   -   PrimaryKey   right   ( QExpr   ( Sql92SelectExpressionSyntax   syntax )   s )) \n   -   Q   syntax   db   s   ( left   ( QExpr   ( Sql92SelectExpressionSyntax   syntax )   s )) \n   -   Q   syntax   db   s   ( right   ( QExpr   ( Sql92SelectExpressionSyntax   syntax )   s )) \n   -   Q   syntax   db   s   (   joinThrough   ( QExpr   ( Sql92SelectExpressionSyntax   syntax )   s ) \n                    ,   left   ( QExpr   ( Sql92SelectExpressionSyntax   syntax )   s ) \n                    ,   right   ( QExpr   ( Sql92SelectExpressionSyntax   syntax )   s ))   Under the hood  manyToMany_  is defined simply as   manyToMany_   =   fmap   ( \\ ( _ ,   left ,   right )   -   ( left ,   right ))   manyToManyPassthrough_", 
            "title": "Many-to-many with arbitrary data"
        }, 
        {
            "location": "/user-guide/queries/relationships/#declaring-many-to-many-relationships", 
            "text": "Like one-to-many relationships, beam allows you to extract commonly used\nmany-to-many relationships, via the  ManyToMany  type.  For example, the playlist/track relationship above can be defined as follows  playlistTrackRelationship   ::   ManyToMany   ChinookDb   PlaylistT   TrackT  playlistTrackRelationshipu   = \n   manyToMany_   ( playlistTrack   chinookDb )   playlistTrackPlaylistId   playlistTrackTrackId   And we can use it as expected:  \n                 \n                    \n         \n            \n         \n             Haskell \n         \n    \n         \n             Sqlite3 \n         \n    \n         \n             Postgres \n         \n    \n         \n    \n         \n            \n         \n             playlistTrackRelationship \n     ( filter_   ( \\ p   -   playlistName   p   ==.   just_   ( val_   Music )   ||. \n                     playlistName   p   ==.   just_   ( val_   Movies )) \n              ( all_   ( playlist   chinookDb ))) \n\n     ( all_   ( track   chinookDb ))  \n\n         \n    \n         \n             SELECT  t0 . PlaylistId  AS  res0 ,          t0 . Name  AS  res1 ,          t1 . TrackId  AS  res2 ,          t1 . Name  AS  res3 ,          t1 . AlbumId  AS  res4 ,          t1 . MediaTypeId  AS  res5 ,          t1 . GenreId  AS  res6 ,          t1 . Composer  AS  res7 ,          t1 . Milliseconds  AS  res8 ,          t1 . Bytes  AS  res9 ,          t1 . UnitPrice  AS  res10  FROM  Playlist  AS  t0  INNER JOIN  Track  AS  t1  INNER JOIN  PlaylistTrack  AS  t2  ON (( t2 . PlaylistId )=( t0 . PlaylistId ))  AND (( t2 . TrackId )=( t1 . TrackId ))  WHERE (( t0 . Name )=(?))    OR (( t0 . Name )=(?))  \n\n         \n    \n         \n             SELECT   t0 . PlaylistId   AS   res0 , \n        t0 . Name   AS   res1 , \n        t1 . TrackId   AS   res2 , \n        t1 . Name   AS   res3 , \n        t1 . AlbumId   AS   res4 , \n        t1 . MediaTypeId   AS   res5 , \n        t1 . GenreId   AS   res6 , \n        t1 . Composer   AS   res7 , \n        t1 . Milliseconds   AS   res8 , \n        t1 . Bytes   AS   res9 , \n        t1 . UnitPrice   AS   res10  FROM   Playlist   AS   t0  INNER   JOIN   Track   AS   t1  INNER   JOIN   PlaylistTrack   AS   t2   ON   (( t2 . PlaylistId )   =   ( t0 . PlaylistId ))  AND   (( t2 . TrackId )   =   ( t1 . TrackId ))  WHERE   (( t0 . Name )   =   ( Music )) \n   OR   (( t0 . Name )   =   ( Movies ))  \n\n         \n    \n         \n    \n                 \n                      ManyToManyThrough  is the equivalent for  manyToManyThrough_ , except it takes\nanother table parameter for the 'through' table.", 
            "title": "Declaring many-to-many relationships"
        }, 
        {
            "location": "/user-guide/queries/relationships/#arbitrary-joins", 
            "text": "Joins with arbitrary conditions can be specified using the  join_  construct.\nFor example,  oneToMany_  is implemented as  oneToMany_   rel   getKey   tbl   = \n   join_   rel   ( \\ rel   -   getKey   rel   ==.   pk   tbl )   Thus, the invoice example above could be rewritten. For example, instead of   do   i   -   all_   ( invoice   chinookDb ) \n    ln   -   oneToMany_   ( invoiceLine   chinookDb )   invoiceLineInvoice   i \n    pure   ( i ,   ln )   We could write  \n                 \n                    \n         \n            \n         \n             Haskell \n         \n    \n         \n             Sqlite3 \n         \n    \n         \n             Postgres \n         \n    \n         \n    \n         \n            \n         \n             do   i   -   all_   ( invoice   chinookDb ) \n    ln   -   join_   ( invoiceLine   chinookDb )   ( \\ line   -   invoiceLineInvoice   line   ==.   primaryKey   i ) \n    pure   ( i ,   ln )  \n\n         \n    \n         \n             SELECT  t0 . InvoiceId  AS  res0 ,          t0 . CustomerId  AS  res1 ,          t0 . InvoiceDate  AS  res2 ,          t0 . BillingAddress  AS  res3 ,          t0 . BillingCity  AS  res4 ,          t0 . BillingState  AS  res5 ,          t0 . BillingCountry  AS  res6 ,          t0 . BillingPostalCode  AS  res7 ,          t0 . Total  AS  res8 ,          t1 . InvoiceLineId  AS  res9 ,          t1 . InvoiceId  AS  res10 ,          t1 . TrackId  AS  res11 ,          t1 . UnitPrice  AS  res12 ,          t1 . Quantity  AS  res13  FROM  Invoice  AS  t0  INNER JOIN  InvoiceLine  AS  t1  ON ( t1 . InvoiceId )=( t0 . InvoiceId )  \n\n         \n    \n         \n             SELECT   t0 . InvoiceId   AS   res0 , \n        t0 . CustomerId   AS   res1 , \n        t0 . InvoiceDate   AS   res2 , \n        t0 . BillingAddress   AS   res3 , \n        t0 . BillingCity   AS   res4 , \n        t0 . BillingState   AS   res5 , \n        t0 . BillingCountry   AS   res6 , \n        t0 . BillingPostalCode   AS   res7 , \n        t0 . Total   AS   res8 , \n        t1 . InvoiceLineId   AS   res9 , \n        t1 . InvoiceId   AS   res10 , \n        t1 . TrackId   AS   res11 , \n        t1 . UnitPrice   AS   res12 , \n        t1 . Quantity   AS   res13  FROM   Invoice   AS   t0  INNER   JOIN   InvoiceLine   AS   t1   ON   ( t1 . InvoiceId )   =   ( t0 . InvoiceId )", 
            "title": "Arbitrary Joins"
        }, 
        {
            "location": "/user-guide/queries/relationships/#outer-joins", 
            "text": "", 
            "title": "Outer joins"
        }, 
        {
            "location": "/user-guide/queries/relationships/#left-and-right-joins", 
            "text": "Left joins with arbitrary conditions can be specified with the  leftJoin_ \nconstruct.  leftJoin_  takes a table and a join condition. It associates each\nresult record with a record of the table given or an fully NULL row of that\ntable in case no row matches. For this reason, the result of  leftJoin_  has an\nextra  Nullable  column tag, which converts each field into the corresponding Maybe  type.   Note  The table parameter passed in as the join condition does not have a  Nullable  column tag. The join condition should be written as if a \nconcrete row from that table exists.   For example, to get every artist along with their albums, but always including\nevery artist, use  leftJoin_  as follows.  \n                 \n                    \n         \n            \n         \n             Haskell \n         \n    \n         \n             Sqlite3 \n         \n    \n         \n             Postgres \n         \n    \n         \n    \n         \n            \n         \n             do   artist   -   all_   ( artist   chinookDb ) \n    album    -   leftJoin_   ( album   chinookDb )   ( \\ album   -   albumArtist   album   ==.   primaryKey   artist ) \n    pure   ( artist ,   album )  \n\n         \n    \n         \n             SELECT  t0 . ArtistId  AS  res0 ,          t0 . Name  AS  res1 ,          t1 . AlbumId  AS  res2 ,          t1 . Title  AS  res3 ,          t1 . ArtistId  AS  res4  FROM  Artist  AS  t0  LEFT JOIN  Album  AS  t1  ON ( t1 . ArtistId )=( t0 . ArtistId )  \n\n         \n    \n         \n             SELECT   t0 . ArtistId   AS   res0 , \n        t0 . Name   AS   res1 , \n        t1 . AlbumId   AS   res2 , \n        t1 . Title   AS   res3 , \n        t1 . ArtistId   AS   res4  FROM   Artist   AS   t0  LEFT   JOIN   Album   AS   t1   ON   ( t1 . ArtistId )   =   ( t0 . ArtistId )  \n\n         \n    \n         \n    \n                 \n                      Right joins are not yet supported. They can always be rewritten as left joins.\nIf you have a compelling use case, please file an issue!", 
            "title": "Left and right joins"
        }, 
        {
            "location": "/user-guide/queries/relationships/#full-outer-joins", 
            "text": "TODO  outerJoin_  not yet supported   Full outer joins are supported via the  outerJoin_  construct.", 
            "title": "Full Outer joins"
        }, 
        {
            "location": "/user-guide/queries/relationships/#subqueries", 
            "text": "Sometimes you want to join against a  subquery  rather than a table. For the\nmost part, beam will automatically figure out when certain queries need to be\nwritten using subqueries. For example, to join two result sets cointaining a SQL\nLIMIT, you would normally have to write both queries as subqueries. In beam, you\ncan write such queries as you'd expect. The library takes care of creating\nsubqueries as expected.  For example, the following query generates the code you'd expect.  \n                 \n                    \n         \n            \n         \n             Haskell \n         \n    \n         \n             Sqlite3 \n         \n    \n         \n             Postgres \n         \n    \n         \n    \n         \n            \n         \n             do   i   -   limit_   10   $   all_   ( invoice   chinookDb ) \n    line   -   invoiceLines   i \n    pure   ( i ,   line )  \n\n         \n    \n         \n             SELECT  t0 . res0  AS  res0 ,          t0 . res1  AS  res1 ,          t0 . res2  AS  res2 ,          t0 . res3  AS  res3 ,          t0 . res4  AS  res4 ,          t0 . res5  AS  res5 ,          t0 . res6  AS  res6 ,          t0 . res7  AS  res7 ,          t0 . res8  AS  res8 ,          t1 . InvoiceLineId  AS  res9 ,          t1 . InvoiceId  AS  res10 ,          t1 . TrackId  AS  res11 ,          t1 . UnitPrice  AS  res12 ,          t1 . Quantity  AS  res13  FROM    (SELECT  t0 . InvoiceId  AS  res0 ,             t0 . CustomerId  AS  res1 ,             t0 . InvoiceDate  AS  res2 ,             t0 . BillingAddress  AS  res3 ,             t0 . BillingCity  AS  res4 ,             t0 . BillingState  AS  res5 ,             t0 . BillingCountry  AS  res6 ,             t0 . BillingPostalCode  AS  res7 ,             t0 . Total  AS  res8     FROM  Invoice  AS  t0     LIMIT 10) AS  t0  INNER JOIN  InvoiceLine  AS  t1  ON ( t1 . InvoiceId )=( t0 . res0 )  \n\n         \n    \n         \n             SELECT   t0 . res0   AS   res0 , \n        t0 . res1   AS   res1 , \n        t0 . res2   AS   res2 , \n        t0 . res3   AS   res3 , \n        t0 . res4   AS   res4 , \n        t0 . res5   AS   res5 , \n        t0 . res6   AS   res6 , \n        t0 . res7   AS   res7 , \n        t0 . res8   AS   res8 , \n        t1 . InvoiceLineId   AS   res9 , \n        t1 . InvoiceId   AS   res10 , \n        t1 . TrackId   AS   res11 , \n        t1 . UnitPrice   AS   res12 , \n        t1 . Quantity   AS   res13  FROM \n   ( SELECT   t0 . InvoiceId   AS   res0 , \n           t0 . CustomerId   AS   res1 , \n           t0 . InvoiceDate   AS   res2 , \n           t0 . BillingAddress   AS   res3 , \n           t0 . BillingCity   AS   res4 , \n           t0 . BillingState   AS   res5 , \n           t0 . BillingCountry   AS   res6 , \n           t0 . BillingPostalCode   AS   res7 , \n           t0 . Total   AS   res8 \n    FROM   Invoice   AS   t0 \n    LIMIT   10 )   AS   t0  INNER   JOIN   InvoiceLine   AS   t1   ON   ( t1 . InvoiceId )   =   ( t0 . res0 )  \n\n         \n    \n         \n    \n                 \n                      If you need to (for efficiency for example), you can also generate subqueries\nexplicitly, using  subselect_ . The  subselect_  will force a new query to be\noutput in most cases. For simple queries, such as  all_ ,  subselect_  will have\nno effect.  \n                 \n                    \n         \n            \n         \n             Haskell \n         \n    \n         \n             Sqlite3 \n         \n    \n         \n             Postgres \n         \n    \n         \n    \n         \n            \n         \n             -- Same as above, but with explicit sub select  do   i   -   subselect_   $   limit_   10   $   all_   ( invoice   chinookDb ) \n    line   -   invoiceLines   i \n    pure   ( i ,   line )  \n\n         \n    \n         \n             SELECT  t0 . res0  AS  res0 ,          t0 . res1  AS  res1 ,          t0 . res2  AS  res2 ,          t0 . res3  AS  res3 ,          t0 . res4  AS  res4 ,          t0 . res5  AS  res5 ,          t0 . res6  AS  res6 ,          t0 . res7  AS  res7 ,          t0 . res8  AS  res8 ,          t1 . InvoiceLineId  AS  res9 ,          t1 . InvoiceId  AS  res10 ,          t1 . TrackId  AS  res11 ,          t1 . UnitPrice  AS  res12 ,          t1 . Quantity  AS  res13  FROM    (SELECT  t0 . InvoiceId  AS  res0 ,             t0 . CustomerId  AS  res1 ,             t0 . InvoiceDate  AS  res2 ,             t0 . BillingAddress  AS  res3 ,             t0 . BillingCity  AS  res4 ,             t0 . BillingState  AS  res5 ,             t0 . BillingCountry  AS  res6 ,             t0 . BillingPostalCode  AS  res7 ,             t0 . Total  AS  res8     FROM  Invoice  AS  t0     LIMIT 10) AS  t0  INNER JOIN  InvoiceLine  AS  t1  ON ( t1 . InvoiceId )=( t0 . res0 )  \n\n         \n    \n         \n             SELECT   t0 . res0   AS   res0 , \n        t0 . res1   AS   res1 , \n        t0 . res2   AS   res2 , \n        t0 . res3   AS   res3 , \n        t0 . res4   AS   res4 , \n        t0 . res5   AS   res5 , \n        t0 . res6   AS   res6 , \n        t0 . res7   AS   res7 , \n        t0 . res8   AS   res8 , \n        t1 . InvoiceLineId   AS   res9 , \n        t1 . InvoiceId   AS   res10 , \n        t1 . TrackId   AS   res11 , \n        t1 . UnitPrice   AS   res12 , \n        t1 . Quantity   AS   res13  FROM \n   ( SELECT   t0 . InvoiceId   AS   res0 , \n           t0 . CustomerId   AS   res1 , \n           t0 . InvoiceDate   AS   res2 , \n           t0 . BillingAddress   AS   res3 , \n           t0 . BillingCity   AS   res4 , \n           t0 . BillingState   AS   res5 , \n           t0 . BillingCountry   AS   res6 , \n           t0 . BillingPostalCode   AS   res7 , \n           t0 . Total   AS   res8 \n    FROM   Invoice   AS   t0 \n    LIMIT   10 )   AS   t0  INNER   JOIN   InvoiceLine   AS   t1   ON   ( t1 . InvoiceId )   =   ( t0 . res0 )", 
            "title": "Subqueries"
        }, 
        {
            "location": "/user-guide/queries/aggregates/", 
            "text": "You can use the \naggregate_\n function to group your result set and compute\naggregates within the group. You can think of \naggregate_\n as a souped up\nversion of Haskell's \ngroupBy\n.\n\n\nYou use \naggregate_\n by specifying an underlying query to run and a function\nthat produces an aggregation projection. An aggregation projection is either a\nvalue of type \nQAgg syntax s a\n, a value of type \nQGroupExpr syntax s a\n, or a\ntuple of such values. Any \nQGenExpr\n that uses an aggregate function is\nautomatically assigned the \nQAgg syntax s a\n type. Any \nQGenExpr\n that contains\nthe \ngroup_\n combinator is given the type \nQGroupExpr\n.\n\n\nDuring query generation, the expressions of type \nQGroupExpr\n are added to the\n\nGROUP BY\n clause, and expressions of type \nQAgg\n are treated as aggregation to\nbe computed.\n\n\nThe result of the \naggregate_\n lifts all the \nQAgg\ns and \nQGroupExpr\ns to\n'regular' value-level \nQExpr\ns, so the result of \naggregate_\n can be used in\nexpressions as usual.\n\n\nSimple aggregate usage\n\n\nSuppose we wanted to count the number of genres in our database.\n\n\n\n                \n\n                    \n        \n\n            \n        \n\n            \nHaskell\n\n        \n\n    \n        \n\n            \nSqlite3\n\n        \n\n    \n        \n\n            \nPostgres\n\n        \n\n    \n        \n\n    \n        \n\n            \n        \n\n            \naggregate_\n \n(\n\\\n_\n \n-\n \ncountAll_\n)\n \n(\nall_\n \n(\ngenre\n \nchinookDb\n))\n\n\n\n\n        \n\n    \n        \n\n            \nSELECT COUNT(*) AS \nres0\n\n\nFROM \nGenre\n AS \nt0\n\n\n\n\n        \n\n    \n        \n\n            \nSELECT\n \nCOUNT\n(\n*\n)\n \nAS\n \nres0\n\n\nFROM\n \nGenre\n \nAS\n \nt0\n\n\n\n\n        \n\n    \n        \n\n    \n                \n\n                    \n\n\nAdding a GROUP BY clause\n\n\nAbove, SQL used the default grouping, which puts all rows in one group. We can\nalso specify columns and expressions to group by. For example, if we wanted to\ncount the number of tracks for each genre, we can use the \ngroup_\n function to\ngroup by the genre.\n\n\n\n                \n\n                    \n        \n\n            \n        \n\n            \nHaskell\n\n        \n\n    \n        \n\n            \nSqlite3\n\n        \n\n    \n        \n\n            \nPostgres\n\n        \n\n    \n        \n\n    \n        \n\n            \n        \n\n            \naggregate_\n \n(\n\\\n(\ngenre\n,\n \ntrack\n)\n \n-\n \n(\ngroup_\n \ngenre\n,\n \nas_\n \n@\nInt\n \n$\n \ncount_\n \n(\ntrackId\n \ntrack\n)))\n\n           \n((,)\n \n$\n \nall_\n \n(\ngenre\n \nchinookDb\n)\n \n*\n \nall_\n \n(\ntrack\n \nchinookDb\n))\n\n\n\n\n        \n\n    \n        \n\n            \nSELECT \nt0\n.\nGenreId\n AS \nres0\n,\n\n\n       \nt0\n.\nName\n AS \nres1\n,\n\n\n       COUNT(\nt1\n.\nTrackId\n) AS \nres2\n\n\nFROM \nGenre\n AS \nt0\n\n\nINNER JOIN \nTrack\n AS \nt1\n\n\nGROUP BY \nt0\n.\nGenreId\n,\n\n\n         \nt0\n.\nName\n\n\n\n\n        \n\n    \n        \n\n            \nSELECT\n \nt0\n.\nGenreId\n \nAS\n \nres0\n,\n\n       \nt0\n.\nName\n \nAS\n \nres1\n,\n\n       \nCOUNT\n(\nt1\n.\nTrackId\n)\n \nAS\n \nres2\n\n\nFROM\n \nGenre\n \nAS\n \nt0\n\n\nINNER\n \nJOIN\n \nTrack\n \nAS\n \nt1\n\n\nGROUP\n \nBY\n \nt0\n.\nGenreId\n,\n\n         \nt0\n.\nName\n\n\n\n\n        \n\n    \n        \n\n    \n                \n\n                    \n\n\n\n\nTip\n\n\ncount_\n can return any \nIntegral\n type. Adding the explicit \nas_ @Int\n above \nprevents an ambiguous type error.\n\n\n\n\nSQL compatibility\n\n\nAbove, we demonstrated the use of \ncount_\n and \ncountAll_\n which map to the\nappropriate SQL aggregates. Beam supports all of the other standard SQL92\naggregates.\n\n\nIn general, SQL aggregates are named similarly in beam and SQL. As usual, the\naggregate function in beam is suffixed by an underscore. For example, \nsum_\n\ncorresponds to the SQL aggregate \nSUM\n.\n\n\nSQL also allows you to specify set quantifiers for each aggregate. Beam supports\nthese as well. By convention, versions of aggregates that take in an optional\nset quantifier are suffixed by \nOver\n. For example \nSUM(DISTINCT x)\n can be\nwritten \nsumOver_ distinctInGroup_ x\n. The universally quantified version of\neach aggregate is obtained by using the \nallInGroup_\n quantifier. Thus, \nsum_ ==\nsumOver_ allInGroup_\n. Because \nALL\n is the default set quantifier, beam does\nnot typically generate it in queries. If, for some reason, you would like beam\nto be explicit about it, you can use the \nallInGroupExplicitly_\n quantifier.\n\n\n\n                \n\n                    \n        \n\n            \n        \n\n            \nHaskell\n\n        \n\n    \n        \n\n            \nSqlite3\n\n        \n\n    \n        \n\n            \nPostgres\n\n        \n\n    \n        \n\n    \n        \n\n            \n        \n\n            \naggregate_\n \n(\n\\\n(\ngenre\n,\n \ntrack\n)\n \n-\n\n              \n(\n \ngroup_\n \ngenre\n\n              \n,\n \nas_\n \n@\nInt\n \n$\n \ncountOver_\n \ndistinctInGroup_\n \n(\ntrackUnitPrice\n \ntrack\n)\n\n              \n,\n \nsumOver_\n \nallInGroupExplicitly_\n \n(\ntrackMilliseconds\n \ntrack\n)\n \n`\ndiv_\n`\n \n1000\n \n))\n \n$\n\n           \n((,)\n \n$\n \nall_\n \n(\ngenre\n \nchinookDb\n)\n \n*\n \nall_\n \n(\ntrack\n \nchinookDb\n))\n\n\n\n\n        \n\n    \n        \n\n            \nSELECT \nt0\n.\nGenreId\n AS \nres0\n,\n\n\n       \nt0\n.\nName\n AS \nres1\n,\n\n\n       COUNT(DISTINCT \nt1\n.\nUnitPrice\n) AS \nres2\n,\n\n\n       (SUM(ALL \nt1\n.\nMilliseconds\n)) / (?) AS \nres3\n\n\nFROM \nGenre\n AS \nt0\n\n\nINNER JOIN \nTrack\n AS \nt1\n\n\nGROUP BY \nt0\n.\nGenreId\n,\n\n\n         \nt0\n.\nName\n\n\n\n\n        \n\n    \n        \n\n            \nSELECT\n \nt0\n.\nGenreId\n \nAS\n \nres0\n,\n\n       \nt0\n.\nName\n \nAS\n \nres1\n,\n\n       \nCOUNT\n(\nDISTINCT\n \nt1\n.\nUnitPrice\n)\n \nAS\n \nres2\n,\n\n       \n(\nSUM\n(\nALL\n \nt1\n.\nMilliseconds\n))\n \n/\n \n(\n1000\n)\n \nAS\n \nres3\n\n\nFROM\n \nGenre\n \nAS\n \nt0\n\n\nINNER\n \nJOIN\n \nTrack\n \nAS\n \nt1\n\n\nGROUP\n \nBY\n \nt0\n.\nGenreId\n,\n\n         \nt0\n.\nName\n\n\n\n\n        \n\n    \n        \n\n    \n                \n\n                    \n\n\nThe \nbeam-core\n library supports the standard SQL aggregation functions.\nIndividual backends are likely to support the full range of aggregates available\non that backend (if not, please send a bug report).\n\n\n\n\n\n\n\n\nSQL Aggregate\n\n\nRelevant standard\n\n\nUnquantified beam function\n\n\nQuantified beam function\n\n\n\n\n\n\n\n\n\n\nSUM\n\n\nSQL92\n\n\nsum_\n\n\nsumOver_\n\n\n\n\n\n\nMIN\n\n\nSQL92\n\n\nmin_\n\n\nminOver_\n\n\n\n\n\n\nMAX\n\n\nSQL92\n\n\nmax_\n\n\nmaxOver_\n\n\n\n\n\n\nAVG\n\n\nSQL92\n\n\navg_\n\n\navgOver_\n\n\n\n\n\n\nCOUNT(x)\n\n\nSQL92\n\n\ncount_\n\n\ncountOver_\n\n\n\n\n\n\nCOUNT(*)\n\n\nSQL92\n\n\ncountAll_\n\n\nN/A\n\n\n\n\n\n\nEVERY(x)\n\n\nSQL99\n\n\nevery_\n\n\neveryOver_\n\n\n\n\n\n\nANY(x)/SOME(x)\n\n\nSQL99\n\n\nany_\n, \nsome_\n\n\nanyOver_\n, \nsomeOver_\n\n\n\n\n\n\n\n\nThe \nHAVING\n clause\n\n\nSQL allows users to specify a \nHAVING\n condition to filter results based on the\ncomputed result of an aggregate. Beam fully supports \nHAVIVG\n clauses, but does\nnot use any special syntax. Simply use \nfilter_\n or \nguard_\n as usual, and beam\nwill add a \nHAVING\n clause if it forms legal SQL. Otherwise, beam will create a\nsubselect and add a \nWHERE\n clause. Either way, this is transparent to the user.\n\n\n\n                \n\n                    \n        \n\n            \n        \n\n            \nHaskell\n\n        \n\n    \n        \n\n            \nSqlite3\n\n        \n\n    \n        \n\n            \nPostgres\n\n        \n\n    \n        \n\n    \n        \n\n            \n        \n\n            \n-- Only return results for genres whose total track length is over 5 minutes\n\n\nfilter_\n \n(\n\\\n(\ngenre\n,\n \ndistinctPriceCount\n,\n \ntotalTrackLength\n)\n \n-\n \ntotalTrackLength\n \n=.\n \n300000\n)\n \n$\n\n\naggregate_\n \n(\n\\\n(\ngenre\n,\n \ntrack\n)\n \n-\n\n              \n(\n \ngroup_\n \ngenre\n\n              \n,\n \nas_\n \n@\nInt\n \n$\n \ncountOver_\n \ndistinctInGroup_\n \n(\ntrackUnitPrice\n \ntrack\n)\n\n              \n,\n \nsumOver_\n \nallInGroupExplicitly_\n \n(\ntrackMilliseconds\n \ntrack\n)\n \n`\ndiv_\n`\n \n1000\n \n))\n \n$\n\n           \n((,)\n \n$\n \nall_\n \n(\ngenre\n \nchinookDb\n)\n \n*\n \nall_\n \n(\ntrack\n \nchinookDb\n))\n\n\n\n\n        \n\n    \n        \n\n            \nSELECT \nt0\n.\nGenreId\n AS \nres0\n,\n\n\n       \nt0\n.\nName\n AS \nres1\n,\n\n\n       COUNT(DISTINCT \nt1\n.\nUnitPrice\n) AS \nres2\n,\n\n\n       (SUM(ALL \nt1\n.\nMilliseconds\n)) / (?) AS \nres3\n\n\nFROM \nGenre\n AS \nt0\n\n\nINNER JOIN \nTrack\n AS \nt1\n\n\nGROUP BY \nt0\n.\nGenreId\n,\n\n\n         \nt0\n.\nName\n\n\nHAVING ((SUM(ALL \nt1\n.\nMilliseconds\n)) / (?))\n=(?)\n\n\n\n\n        \n\n    \n        \n\n            \nSELECT\n \nt0\n.\nGenreId\n \nAS\n \nres0\n,\n\n       \nt0\n.\nName\n \nAS\n \nres1\n,\n\n       \nCOUNT\n(\nDISTINCT\n \nt1\n.\nUnitPrice\n)\n \nAS\n \nres2\n,\n\n       \n(\nSUM\n(\nALL\n \nt1\n.\nMilliseconds\n))\n \n/\n \n(\n1000\n)\n \nAS\n \nres3\n\n\nFROM\n \nGenre\n \nAS\n \nt0\n\n\nINNER\n \nJOIN\n \nTrack\n \nAS\n \nt1\n\n\nGROUP\n \nBY\n \nt0\n.\nGenreId\n,\n\n         \nt0\n.\nName\n\n\nHAVING\n \n((\nSUM\n(\nALL\n \nt1\n.\nMilliseconds\n))\n \n/\n \n(\n1000\n))\n \n=\n \n(\n300000\n)\n\n\n\n\n        \n\n    \n        \n\n    \n                \n\n                    \n\n\nBeam will also handle the \nfilter_\n correctly in the presence of more\ncomplicated queries. For example, we can now join our aggregate on genres back\nover tracks.\n\n\n\n                \n\n                    \n        \n\n            \n        \n\n            \nHaskell\n\n        \n\n    \n        \n\n            \nSqlite3\n\n        \n\n    \n        \n\n            \nPostgres\n\n        \n\n    \n        \n\n    \n        \n\n            \n        \n\n            \n-- Only return results for genres whose total track length is over 5 minutes\n\n\nfilter_\n \n(\n\\\n(\ngenre\n,\n \ntrack\n,\n \ndistinctPriceCount\n,\n \ntotalTrackLength\n)\n \n-\n \ntotalTrackLength\n \n=.\n \n300000\n)\n \n$\n\n\ndo\n \n(\ngenre\n,\n \npriceCnt\n,\n \ntrackLength\n)\n \n-\n\n            \naggregate_\n \n(\n\\\n(\ngenre\n,\n \ntrack\n)\n \n-\n\n                          \n(\n \ngroup_\n \ngenre\n\n                          \n,\n \nas_\n \n@\nInt\n \n$\n \ncountOver_\n \ndistinctInGroup_\n \n(\ntrackUnitPrice\n \ntrack\n)\n\n                          \n,\n \nsumOver_\n \nallInGroupExplicitly_\n \n(\ntrackMilliseconds\n \ntrack\n)\n \n`\ndiv_\n`\n \n1000\n \n))\n \n$\n\n            \n((,)\n \n$\n \nall_\n \n(\ngenre\n \nchinookDb\n)\n \n*\n \nall_\n \n(\ntrack\n \nchinookDb\n))\n\n   \ntrack\n \n-\n \njoin_\n \n(\ntrack\n \nchinookDb\n)\n \n(\n\\\ntrack\n \n-\n \ntrackGenreId\n \ntrack\n \n==.\n \njust_\n \n(\npk\n \ngenre\n))\n\n   \npure\n \n(\ngenre\n,\n \ntrack\n,\n \npriceCnt\n,\n \ntrackLength\n)\n\n\n\n\n        \n\n    \n        \n\n            \nSELECT \nt0\n.\nres0\n AS \nres0\n,\n\n\n       \nt0\n.\nres1\n AS \nres1\n,\n\n\n       \nt1\n.\nTrackId\n AS \nres2\n,\n\n\n       \nt1\n.\nName\n AS \nres3\n,\n\n\n       \nt1\n.\nAlbumId\n AS \nres4\n,\n\n\n       \nt1\n.\nMediaTypeId\n AS \nres5\n,\n\n\n       \nt1\n.\nGenreId\n AS \nres6\n,\n\n\n       \nt1\n.\nComposer\n AS \nres7\n,\n\n\n       \nt1\n.\nMilliseconds\n AS \nres8\n,\n\n\n       \nt1\n.\nBytes\n AS \nres9\n,\n\n\n       \nt1\n.\nUnitPrice\n AS \nres10\n,\n\n\n       \nt0\n.\nres2\n AS \nres11\n,\n\n\n       \nt0\n.\nres3\n AS \nres12\n\n\nFROM\n\n\n  (SELECT \nt0\n.\nGenreId\n AS \nres0\n,\n\n\n          \nt0\n.\nName\n AS \nres1\n,\n\n\n          COUNT(DISTINCT \nt1\n.\nUnitPrice\n) AS \nres2\n,\n\n\n          (SUM(ALL \nt1\n.\nMilliseconds\n)) / (?) AS \nres3\n\n\n   FROM \nGenre\n AS \nt0\n\n\n   INNER JOIN \nTrack\n AS \nt1\n\n\n   GROUP BY \nt0\n.\nGenreId\n,\n\n\n            \nt0\n.\nName\n) AS \nt0\n\n\nINNER JOIN \nTrack\n AS \nt1\n ON (\nt1\n.\nGenreId\n)=(\nt0\n.\nres0\n)\n\n\nWHERE (\nt0\n.\nres3\n)\n=(?)\n\n\n\n\n        \n\n    \n        \n\n            \nSELECT\n \nt0\n.\nres0\n \nAS\n \nres0\n,\n\n       \nt0\n.\nres1\n \nAS\n \nres1\n,\n\n       \nt1\n.\nTrackId\n \nAS\n \nres2\n,\n\n       \nt1\n.\nName\n \nAS\n \nres3\n,\n\n       \nt1\n.\nAlbumId\n \nAS\n \nres4\n,\n\n       \nt1\n.\nMediaTypeId\n \nAS\n \nres5\n,\n\n       \nt1\n.\nGenreId\n \nAS\n \nres6\n,\n\n       \nt1\n.\nComposer\n \nAS\n \nres7\n,\n\n       \nt1\n.\nMilliseconds\n \nAS\n \nres8\n,\n\n       \nt1\n.\nBytes\n \nAS\n \nres9\n,\n\n       \nt1\n.\nUnitPrice\n \nAS\n \nres10\n,\n\n       \nt0\n.\nres2\n \nAS\n \nres11\n,\n\n       \nt0\n.\nres3\n \nAS\n \nres12\n\n\nFROM\n\n  \n(\nSELECT\n \nt0\n.\nGenreId\n \nAS\n \nres0\n,\n\n          \nt0\n.\nName\n \nAS\n \nres1\n,\n\n          \nCOUNT\n(\nDISTINCT\n \nt1\n.\nUnitPrice\n)\n \nAS\n \nres2\n,\n\n          \n(\nSUM\n(\nALL\n \nt1\n.\nMilliseconds\n))\n \n/\n \n(\n1000\n)\n \nAS\n \nres3\n\n   \nFROM\n \nGenre\n \nAS\n \nt0\n\n   \nINNER\n \nJOIN\n \nTrack\n \nAS\n \nt1\n\n   \nGROUP\n \nBY\n \nt0\n.\nGenreId\n,\n\n            \nt0\n.\nName\n)\n \nAS\n \nt0\n\n\nINNER\n \nJOIN\n \nTrack\n \nAS\n \nt1\n \nON\n \n(\nt1\n.\nGenreId\n)\n \n=\n \n(\nt0\n.\nres0\n)\n\n\nWHERE\n \n(\nt0\n.\nres3\n)\n \n=\n \n(\n300000\n)\n\n\n\n\n        \n\n    \n        \n\n    \n                \n\n                    \n\n\nThe position of \nfilter_\n changes the code generated. Above, the \nfilter_\n\nproduced a \nWHERE\n clause on the outermost \nSELECT\n. If instead, we put the\n\nfilter_\n clause right outside the \naggregate_\n, beam will produce a \nHAVING\n clause instead.\n\n\n\n                \n\n                    \n        \n\n            \n        \n\n            \nHaskell\n\n        \n\n    \n        \n\n            \nSqlite3\n\n        \n\n    \n        \n\n            \nPostgres\n\n        \n\n    \n        \n\n    \n        \n\n            \n        \n\n            \n-- Only return results for genres whose total track length is over 5 minutes\n\n\ndo\n \n(\ngenre\n,\n \npriceCnt\n,\n \ntrackLength\n)\n \n-\n\n            \nfilter_\n \n(\n\\\n(\ngenre\n,\n \ndistinctPriceCount\n,\n \ntotalTrackLength\n)\n \n-\n \ntotalTrackLength\n \n=.\n \n300000\n)\n \n$\n\n            \naggregate_\n \n(\n\\\n(\ngenre\n,\n \ntrack\n)\n \n-\n\n                          \n(\n \ngroup_\n \ngenre\n\n                          \n,\n \nas_\n \n@\nInt\n \n$\n \ncountOver_\n \ndistinctInGroup_\n \n(\ntrackUnitPrice\n \ntrack\n)\n\n                          \n,\n \nsumOver_\n \nallInGroupExplicitly_\n \n(\ntrackMilliseconds\n \ntrack\n)\n \n`\ndiv_\n`\n \n1000\n \n))\n \n$\n\n            \n((,)\n \n$\n \nall_\n \n(\ngenre\n \nchinookDb\n)\n \n*\n \nall_\n \n(\ntrack\n \nchinookDb\n))\n\n   \ntrack\n \n-\n \njoin_\n \n(\ntrack\n \nchinookDb\n)\n \n(\n\\\ntrack\n \n-\n \ntrackGenreId\n \ntrack\n \n==.\n \njust_\n \n(\npk\n \ngenre\n))\n\n   \npure\n \n(\ngenre\n,\n \ntrack\n,\n \npriceCnt\n,\n \ntrackLength\n)\n\n\n\n\n        \n\n    \n        \n\n            \nSELECT \nt0\n.\nres0\n AS \nres0\n,\n\n\n       \nt0\n.\nres1\n AS \nres1\n,\n\n\n       \nt1\n.\nTrackId\n AS \nres2\n,\n\n\n       \nt1\n.\nName\n AS \nres3\n,\n\n\n       \nt1\n.\nAlbumId\n AS \nres4\n,\n\n\n       \nt1\n.\nMediaTypeId\n AS \nres5\n,\n\n\n       \nt1\n.\nGenreId\n AS \nres6\n,\n\n\n       \nt1\n.\nComposer\n AS \nres7\n,\n\n\n       \nt1\n.\nMilliseconds\n AS \nres8\n,\n\n\n       \nt1\n.\nBytes\n AS \nres9\n,\n\n\n       \nt1\n.\nUnitPrice\n AS \nres10\n,\n\n\n       \nt0\n.\nres2\n AS \nres11\n,\n\n\n       \nt0\n.\nres3\n AS \nres12\n\n\nFROM\n\n\n  (SELECT \nt0\n.\nGenreId\n AS \nres0\n,\n\n\n          \nt0\n.\nName\n AS \nres1\n,\n\n\n          COUNT(DISTINCT \nt1\n.\nUnitPrice\n) AS \nres2\n,\n\n\n          (SUM(ALL \nt1\n.\nMilliseconds\n)) / (?) AS \nres3\n\n\n   FROM \nGenre\n AS \nt0\n\n\n   INNER JOIN \nTrack\n AS \nt1\n\n\n   GROUP BY \nt0\n.\nGenreId\n,\n\n\n            \nt0\n.\nName\n\n\n   HAVING ((SUM(ALL \nt1\n.\nMilliseconds\n)) / (?))\n=(?)) AS \nt0\n\n\nINNER JOIN \nTrack\n AS \nt1\n ON (\nt1\n.\nGenreId\n)=(\nt0\n.\nres0\n)\n\n\n\n\n        \n\n    \n        \n\n            \nSELECT\n \nt0\n.\nres0\n \nAS\n \nres0\n,\n\n       \nt0\n.\nres1\n \nAS\n \nres1\n,\n\n       \nt1\n.\nTrackId\n \nAS\n \nres2\n,\n\n       \nt1\n.\nName\n \nAS\n \nres3\n,\n\n       \nt1\n.\nAlbumId\n \nAS\n \nres4\n,\n\n       \nt1\n.\nMediaTypeId\n \nAS\n \nres5\n,\n\n       \nt1\n.\nGenreId\n \nAS\n \nres6\n,\n\n       \nt1\n.\nComposer\n \nAS\n \nres7\n,\n\n       \nt1\n.\nMilliseconds\n \nAS\n \nres8\n,\n\n       \nt1\n.\nBytes\n \nAS\n \nres9\n,\n\n       \nt1\n.\nUnitPrice\n \nAS\n \nres10\n,\n\n       \nt0\n.\nres2\n \nAS\n \nres11\n,\n\n       \nt0\n.\nres3\n \nAS\n \nres12\n\n\nFROM\n\n  \n(\nSELECT\n \nt0\n.\nGenreId\n \nAS\n \nres0\n,\n\n          \nt0\n.\nName\n \nAS\n \nres1\n,\n\n          \nCOUNT\n(\nDISTINCT\n \nt1\n.\nUnitPrice\n)\n \nAS\n \nres2\n,\n\n          \n(\nSUM\n(\nALL\n \nt1\n.\nMilliseconds\n))\n \n/\n \n(\n1000\n)\n \nAS\n \nres3\n\n   \nFROM\n \nGenre\n \nAS\n \nt0\n\n   \nINNER\n \nJOIN\n \nTrack\n \nAS\n \nt1\n\n   \nGROUP\n \nBY\n \nt0\n.\nGenreId\n,\n\n            \nt0\n.\nName\n\n   \nHAVING\n \n((\nSUM\n(\nALL\n \nt1\n.\nMilliseconds\n))\n \n/\n \n(\n1000\n))\n \n=\n \n(\n300000\n))\n \nAS\n \nt0\n\n\nINNER\n \nJOIN\n \nTrack\n \nAS\n \nt1\n \nON\n \n(\nt1\n.\nGenreId\n)\n \n=\n \n(\nt0\n.\nres0\n)\n\n\n\n\n        \n\n    \n        \n\n    \n                \n\n                    \n\n\nDue to the monadic structure, putting the filtered aggregate as the second\nclause in the JOIN causes the HAVING to be floated out, because the compiler\ncan't prove that the conditional expression only depends on the results of the\naggregate.\n\n\n\n                \n\n                    \n        \n\n            \n        \n\n            \nHaskell\n\n        \n\n    \n        \n\n            \nSqlite3\n\n        \n\n    \n        \n\n            \nPostgres\n\n        \n\n    \n        \n\n    \n        \n\n            \n        \n\n            \n-- Only return results for genres whose total track length is over 5 minutes\n\n\ndo\n \ntrack_\n \n-\n \nall_\n \n(\ntrack\n \nchinookDb\n)\n\n   \n(\ngenre\n,\n \npriceCnt\n,\n \ntrackLength\n)\n \n-\n\n            \nfilter_\n \n(\n\\\n(\ngenre\n,\n \ndistinctPriceCount\n,\n \ntotalTrackLength\n)\n \n-\n \ntotalTrackLength\n \n=.\n \n300000\n)\n \n$\n\n            \naggregate_\n \n(\n\\\n(\ngenre\n,\n \ntrack\n)\n \n-\n\n                          \n(\n \ngroup_\n \ngenre\n\n                          \n,\n \nas_\n \n@\nInt\n \n$\n \ncountOver_\n \ndistinctInGroup_\n \n(\ntrackUnitPrice\n \ntrack\n)\n\n                          \n,\n \nsumOver_\n \nallInGroupExplicitly_\n \n(\ntrackMilliseconds\n \ntrack\n)\n \n`\ndiv_\n`\n \n1000\n \n))\n \n$\n\n            \n((,)\n \n$\n \nall_\n \n(\ngenre\n \nchinookDb\n)\n \n*\n \nall_\n \n(\ntrack\n \nchinookDb\n))\n \n   \nguard_\n \n(\ntrackGenreId\n \ntrack_\n \n==.\n \njust_\n \n(\npk\n \ngenre\n))\n\n   \npure\n \n(\ngenre\n,\n \ntrack_\n,\n \npriceCnt\n,\n \ntrackLength\n)\n\n\n\n\n        \n\n    \n        \n\n            \nSELECT \nt1\n.\nres0\n AS \nres0\n,\n\n\n       \nt1\n.\nres1\n AS \nres1\n,\n\n\n       \nt0\n.\nTrackId\n AS \nres2\n,\n\n\n       \nt0\n.\nName\n AS \nres3\n,\n\n\n       \nt0\n.\nAlbumId\n AS \nres4\n,\n\n\n       \nt0\n.\nMediaTypeId\n AS \nres5\n,\n\n\n       \nt0\n.\nGenreId\n AS \nres6\n,\n\n\n       \nt0\n.\nComposer\n AS \nres7\n,\n\n\n       \nt0\n.\nMilliseconds\n AS \nres8\n,\n\n\n       \nt0\n.\nBytes\n AS \nres9\n,\n\n\n       \nt0\n.\nUnitPrice\n AS \nres10\n,\n\n\n       \nt1\n.\nres2\n AS \nres11\n,\n\n\n       \nt1\n.\nres3\n AS \nres12\n\n\nFROM \nTrack\n AS \nt0\n\n\nINNER JOIN\n\n\n  (SELECT \nt0\n.\nGenreId\n AS \nres0\n,\n\n\n          \nt0\n.\nName\n AS \nres1\n,\n\n\n          COUNT(DISTINCT \nt1\n.\nUnitPrice\n) AS \nres2\n,\n\n\n          (SUM(ALL \nt1\n.\nMilliseconds\n)) / (?) AS \nres3\n\n\n   FROM \nGenre\n AS \nt0\n\n\n   INNER JOIN \nTrack\n AS \nt1\n\n\n   GROUP BY \nt0\n.\nGenreId\n,\n\n\n            \nt0\n.\nName\n) AS \nt1\n\n\nWHERE ((\nt1\n.\nres3\n)\n=(?))\n\n\n  AND ((\nt0\n.\nGenreId\n)=(\nt1\n.\nres0\n))\n\n\n\n\n        \n\n    \n        \n\n            \nSELECT\n \nt1\n.\nres0\n \nAS\n \nres0\n,\n\n       \nt1\n.\nres1\n \nAS\n \nres1\n,\n\n       \nt0\n.\nTrackId\n \nAS\n \nres2\n,\n\n       \nt0\n.\nName\n \nAS\n \nres3\n,\n\n       \nt0\n.\nAlbumId\n \nAS\n \nres4\n,\n\n       \nt0\n.\nMediaTypeId\n \nAS\n \nres5\n,\n\n       \nt0\n.\nGenreId\n \nAS\n \nres6\n,\n\n       \nt0\n.\nComposer\n \nAS\n \nres7\n,\n\n       \nt0\n.\nMilliseconds\n \nAS\n \nres8\n,\n\n       \nt0\n.\nBytes\n \nAS\n \nres9\n,\n\n       \nt0\n.\nUnitPrice\n \nAS\n \nres10\n,\n\n       \nt1\n.\nres2\n \nAS\n \nres11\n,\n\n       \nt1\n.\nres3\n \nAS\n \nres12\n\n\nFROM\n \nTrack\n \nAS\n \nt0\n\n\nINNER\n \nJOIN\n\n  \n(\nSELECT\n \nt0\n.\nGenreId\n \nAS\n \nres0\n,\n\n          \nt0\n.\nName\n \nAS\n \nres1\n,\n\n          \nCOUNT\n(\nDISTINCT\n \nt1\n.\nUnitPrice\n)\n \nAS\n \nres2\n,\n\n          \n(\nSUM\n(\nALL\n \nt1\n.\nMilliseconds\n))\n \n/\n \n(\n1000\n)\n \nAS\n \nres3\n\n   \nFROM\n \nGenre\n \nAS\n \nt0\n\n   \nINNER\n \nJOIN\n \nTrack\n \nAS\n \nt1\n\n   \nGROUP\n \nBY\n \nt0\n.\nGenreId\n,\n\n            \nt0\n.\nName\n)\n \nAS\n \nt1\n\n\nWHERE\n \n((\nt1\n.\nres3\n)\n \n=\n \n(\n300000\n))\n\n  \nAND\n \n((\nt0\n.\nGenreId\n)\n \n=\n \n(\nt1\n.\nres0\n))\n\n\n\n\n        \n\n    \n        \n\n    \n                \n\n                    \n\n\nYou can prove to the compiler that the \nfilter_\n should generate a having by\nusing the \nsubselect_\n combinator.\n\n\n\n                \n\n                    \n        \n\n            \n        \n\n            \nHaskell\n\n        \n\n    \n        \n\n            \nSqlite3\n\n        \n\n    \n        \n\n            \nPostgres\n\n        \n\n    \n        \n\n    \n        \n\n            \n        \n\n            \n-- Only return results for genres whose total track length is over 5 minutes\n\n\ndo\n \ntrack_\n \n-\n \nall_\n \n(\ntrack\n \nchinookDb\n)\n\n   \n(\ngenre\n,\n \npriceCnt\n,\n \ntrackLength\n)\n \n-\n\n            \nsubselect_\n \n$\n\n            \nfilter_\n \n(\n\\\n(\ngenre\n,\n \ndistinctPriceCount\n,\n \ntotalTrackLength\n)\n \n-\n \ntotalTrackLength\n \n=.\n \n300000\n)\n \n$\n\n            \naggregate_\n \n(\n\\\n(\ngenre\n,\n \ntrack\n)\n \n-\n\n                          \n(\n \ngroup_\n \ngenre\n\n                          \n,\n \nas_\n \n@\nInt\n \n$\n \ncountOver_\n \ndistinctInGroup_\n \n(\ntrackUnitPrice\n \ntrack\n)\n\n                          \n,\n \nsumOver_\n \nallInGroupExplicitly_\n \n(\ntrackMilliseconds\n \ntrack\n)\n \n`\ndiv_\n`\n \n1000\n \n))\n \n$\n\n            \n((,)\n \n$\n \nall_\n \n(\ngenre\n \nchinookDb\n)\n \n*\n \nall_\n \n(\ntrack\n \nchinookDb\n))\n \n   \nguard_\n \n(\ntrackGenreId\n \ntrack_\n \n==.\n \njust_\n \n(\npk\n \ngenre\n))\n\n   \npure\n \n(\ngenre\n,\n \ntrack_\n,\n \npriceCnt\n,\n \ntrackLength\n)\n\n\n\n\n        \n\n    \n        \n\n            \nSELECT \nt1\n.\nres0\n AS \nres0\n,\n\n\n       \nt1\n.\nres1\n AS \nres1\n,\n\n\n       \nt0\n.\nTrackId\n AS \nres2\n,\n\n\n       \nt0\n.\nName\n AS \nres3\n,\n\n\n       \nt0\n.\nAlbumId\n AS \nres4\n,\n\n\n       \nt0\n.\nMediaTypeId\n AS \nres5\n,\n\n\n       \nt0\n.\nGenreId\n AS \nres6\n,\n\n\n       \nt0\n.\nComposer\n AS \nres7\n,\n\n\n       \nt0\n.\nMilliseconds\n AS \nres8\n,\n\n\n       \nt0\n.\nBytes\n AS \nres9\n,\n\n\n       \nt0\n.\nUnitPrice\n AS \nres10\n,\n\n\n       \nt1\n.\nres2\n AS \nres11\n,\n\n\n       \nt1\n.\nres3\n AS \nres12\n\n\nFROM \nTrack\n AS \nt0\n\n\nINNER JOIN\n\n\n  (SELECT \nt0\n.\nres0\n AS \nres0\n,\n\n\n          \nt0\n.\nres1\n AS \nres1\n,\n\n\n          \nt0\n.\nres2\n AS \nres2\n,\n\n\n          \nt0\n.\nres3\n AS \nres3\n\n\n   FROM\n\n\n     (SELECT \nt0\n.\nGenreId\n AS \nres0\n,\n\n\n             \nt0\n.\nName\n AS \nres1\n,\n\n\n             COUNT(DISTINCT \nt1\n.\nUnitPrice\n) AS \nres2\n,\n\n\n             (SUM(ALL \nt1\n.\nMilliseconds\n)) / (?) AS \nres3\n\n\n      FROM \nGenre\n AS \nt0\n\n\n      INNER JOIN \nTrack\n AS \nt1\n\n\n      GROUP BY \nt0\n.\nGenreId\n,\n\n\n               \nt0\n.\nName\n\n\n      HAVING ((SUM(ALL \nt1\n.\nMilliseconds\n)) / (?))\n=(?)) AS \nt0\n) AS \nt1\n\n\nWHERE (\nt0\n.\nGenreId\n)=(\nt1\n.\nres0\n)\n\n\n\n\n        \n\n    \n        \n\n            \nSELECT\n \nt1\n.\nres0\n \nAS\n \nres0\n,\n\n       \nt1\n.\nres1\n \nAS\n \nres1\n,\n\n       \nt0\n.\nTrackId\n \nAS\n \nres2\n,\n\n       \nt0\n.\nName\n \nAS\n \nres3\n,\n\n       \nt0\n.\nAlbumId\n \nAS\n \nres4\n,\n\n       \nt0\n.\nMediaTypeId\n \nAS\n \nres5\n,\n\n       \nt0\n.\nGenreId\n \nAS\n \nres6\n,\n\n       \nt0\n.\nComposer\n \nAS\n \nres7\n,\n\n       \nt0\n.\nMilliseconds\n \nAS\n \nres8\n,\n\n       \nt0\n.\nBytes\n \nAS\n \nres9\n,\n\n       \nt0\n.\nUnitPrice\n \nAS\n \nres10\n,\n\n       \nt1\n.\nres2\n \nAS\n \nres11\n,\n\n       \nt1\n.\nres3\n \nAS\n \nres12\n\n\nFROM\n \nTrack\n \nAS\n \nt0\n\n\nINNER\n \nJOIN\n\n  \n(\nSELECT\n \nt0\n.\nres0\n \nAS\n \nres0\n,\n\n          \nt0\n.\nres1\n \nAS\n \nres1\n,\n\n          \nt0\n.\nres2\n \nAS\n \nres2\n,\n\n          \nt0\n.\nres3\n \nAS\n \nres3\n\n   \nFROM\n\n     \n(\nSELECT\n \nt0\n.\nGenreId\n \nAS\n \nres0\n,\n\n             \nt0\n.\nName\n \nAS\n \nres1\n,\n\n             \nCOUNT\n(\nDISTINCT\n \nt1\n.\nUnitPrice\n)\n \nAS\n \nres2\n,\n\n             \n(\nSUM\n(\nALL\n \nt1\n.\nMilliseconds\n))\n \n/\n \n(\n1000\n)\n \nAS\n \nres3\n\n      \nFROM\n \nGenre\n \nAS\n \nt0\n\n      \nINNER\n \nJOIN\n \nTrack\n \nAS\n \nt1\n\n      \nGROUP\n \nBY\n \nt0\n.\nGenreId\n,\n\n               \nt0\n.\nName\n\n      \nHAVING\n \n((\nSUM\n(\nALL\n \nt1\n.\nMilliseconds\n))\n \n/\n \n(\n1000\n))\n \n=\n \n(\n300000\n))\n \nAS\n \nt0\n)\n \nAS\n \nt1\n\n\nWHERE\n \n(\nt0\n.\nGenreId\n)\n \n=\n \n(\nt1\n.\nres0\n)", 
            "title": "Aggregates"
        }, 
        {
            "location": "/user-guide/queries/aggregates/#simple-aggregate-usage", 
            "text": "Suppose we wanted to count the number of genres in our database.  \n                 \n                    \n         \n            \n         \n             Haskell \n         \n    \n         \n             Sqlite3 \n         \n    \n         \n             Postgres \n         \n    \n         \n    \n         \n            \n         \n             aggregate_   ( \\ _   -   countAll_ )   ( all_   ( genre   chinookDb ))  \n\n         \n    \n         \n             SELECT COUNT(*) AS  res0  FROM  Genre  AS  t0  \n\n         \n    \n         \n             SELECT   COUNT ( * )   AS   res0  FROM   Genre   AS   t0", 
            "title": "Simple aggregate usage"
        }, 
        {
            "location": "/user-guide/queries/aggregates/#adding-a-group-by-clause", 
            "text": "Above, SQL used the default grouping, which puts all rows in one group. We can\nalso specify columns and expressions to group by. For example, if we wanted to\ncount the number of tracks for each genre, we can use the  group_  function to\ngroup by the genre.  \n                 \n                    \n         \n            \n         \n             Haskell \n         \n    \n         \n             Sqlite3 \n         \n    \n         \n             Postgres \n         \n    \n         \n    \n         \n            \n         \n             aggregate_   ( \\ ( genre ,   track )   -   ( group_   genre ,   as_   @ Int   $   count_   ( trackId   track ))) \n            ((,)   $   all_   ( genre   chinookDb )   *   all_   ( track   chinookDb ))  \n\n         \n    \n         \n             SELECT  t0 . GenreId  AS  res0 ,          t0 . Name  AS  res1 ,         COUNT( t1 . TrackId ) AS  res2  FROM  Genre  AS  t0  INNER JOIN  Track  AS  t1  GROUP BY  t0 . GenreId ,            t0 . Name  \n\n         \n    \n         \n             SELECT   t0 . GenreId   AS   res0 , \n        t0 . Name   AS   res1 , \n        COUNT ( t1 . TrackId )   AS   res2  FROM   Genre   AS   t0  INNER   JOIN   Track   AS   t1  GROUP   BY   t0 . GenreId , \n          t0 . Name  \n\n         \n    \n         \n    \n                 \n                       Tip  count_  can return any  Integral  type. Adding the explicit  as_ @Int  above \nprevents an ambiguous type error.", 
            "title": "Adding a GROUP BY clause"
        }, 
        {
            "location": "/user-guide/queries/aggregates/#sql-compatibility", 
            "text": "Above, we demonstrated the use of  count_  and  countAll_  which map to the\nappropriate SQL aggregates. Beam supports all of the other standard SQL92\naggregates.  In general, SQL aggregates are named similarly in beam and SQL. As usual, the\naggregate function in beam is suffixed by an underscore. For example,  sum_ \ncorresponds to the SQL aggregate  SUM .  SQL also allows you to specify set quantifiers for each aggregate. Beam supports\nthese as well. By convention, versions of aggregates that take in an optional\nset quantifier are suffixed by  Over . For example  SUM(DISTINCT x)  can be\nwritten  sumOver_ distinctInGroup_ x . The universally quantified version of\neach aggregate is obtained by using the  allInGroup_  quantifier. Thus,  sum_ ==\nsumOver_ allInGroup_ . Because  ALL  is the default set quantifier, beam does\nnot typically generate it in queries. If, for some reason, you would like beam\nto be explicit about it, you can use the  allInGroupExplicitly_  quantifier.  \n                 \n                    \n         \n            \n         \n             Haskell \n         \n    \n         \n             Sqlite3 \n         \n    \n         \n             Postgres \n         \n    \n         \n    \n         \n            \n         \n             aggregate_   ( \\ ( genre ,   track )   - \n               (   group_   genre \n               ,   as_   @ Int   $   countOver_   distinctInGroup_   ( trackUnitPrice   track ) \n               ,   sumOver_   allInGroupExplicitly_   ( trackMilliseconds   track )   ` div_ `   1000   ))   $ \n            ((,)   $   all_   ( genre   chinookDb )   *   all_   ( track   chinookDb ))  \n\n         \n    \n         \n             SELECT  t0 . GenreId  AS  res0 ,          t0 . Name  AS  res1 ,         COUNT(DISTINCT  t1 . UnitPrice ) AS  res2 ,         (SUM(ALL  t1 . Milliseconds )) / (?) AS  res3  FROM  Genre  AS  t0  INNER JOIN  Track  AS  t1  GROUP BY  t0 . GenreId ,            t0 . Name  \n\n         \n    \n         \n             SELECT   t0 . GenreId   AS   res0 , \n        t0 . Name   AS   res1 , \n        COUNT ( DISTINCT   t1 . UnitPrice )   AS   res2 , \n        ( SUM ( ALL   t1 . Milliseconds ))   /   ( 1000 )   AS   res3  FROM   Genre   AS   t0  INNER   JOIN   Track   AS   t1  GROUP   BY   t0 . GenreId , \n          t0 . Name  \n\n         \n    \n         \n    \n                 \n                      The  beam-core  library supports the standard SQL aggregation functions.\nIndividual backends are likely to support the full range of aggregates available\non that backend (if not, please send a bug report).     SQL Aggregate  Relevant standard  Unquantified beam function  Quantified beam function      SUM  SQL92  sum_  sumOver_    MIN  SQL92  min_  minOver_    MAX  SQL92  max_  maxOver_    AVG  SQL92  avg_  avgOver_    COUNT(x)  SQL92  count_  countOver_    COUNT(*)  SQL92  countAll_  N/A    EVERY(x)  SQL99  every_  everyOver_    ANY(x)/SOME(x)  SQL99  any_ ,  some_  anyOver_ ,  someOver_", 
            "title": "SQL compatibility"
        }, 
        {
            "location": "/user-guide/queries/aggregates/#the-having-clause", 
            "text": "SQL allows users to specify a  HAVING  condition to filter results based on the\ncomputed result of an aggregate. Beam fully supports  HAVIVG  clauses, but does\nnot use any special syntax. Simply use  filter_  or  guard_  as usual, and beam\nwill add a  HAVING  clause if it forms legal SQL. Otherwise, beam will create a\nsubselect and add a  WHERE  clause. Either way, this is transparent to the user.  \n                 \n                    \n         \n            \n         \n             Haskell \n         \n    \n         \n             Sqlite3 \n         \n    \n         \n             Postgres \n         \n    \n         \n    \n         \n            \n         \n             -- Only return results for genres whose total track length is over 5 minutes  filter_   ( \\ ( genre ,   distinctPriceCount ,   totalTrackLength )   -   totalTrackLength   =.   300000 )   $  aggregate_   ( \\ ( genre ,   track )   - \n               (   group_   genre \n               ,   as_   @ Int   $   countOver_   distinctInGroup_   ( trackUnitPrice   track ) \n               ,   sumOver_   allInGroupExplicitly_   ( trackMilliseconds   track )   ` div_ `   1000   ))   $ \n            ((,)   $   all_   ( genre   chinookDb )   *   all_   ( track   chinookDb ))  \n\n         \n    \n         \n             SELECT  t0 . GenreId  AS  res0 ,          t0 . Name  AS  res1 ,         COUNT(DISTINCT  t1 . UnitPrice ) AS  res2 ,         (SUM(ALL  t1 . Milliseconds )) / (?) AS  res3  FROM  Genre  AS  t0  INNER JOIN  Track  AS  t1  GROUP BY  t0 . GenreId ,            t0 . Name  HAVING ((SUM(ALL  t1 . Milliseconds )) / (?)) =(?)  \n\n         \n    \n         \n             SELECT   t0 . GenreId   AS   res0 , \n        t0 . Name   AS   res1 , \n        COUNT ( DISTINCT   t1 . UnitPrice )   AS   res2 , \n        ( SUM ( ALL   t1 . Milliseconds ))   /   ( 1000 )   AS   res3  FROM   Genre   AS   t0  INNER   JOIN   Track   AS   t1  GROUP   BY   t0 . GenreId , \n          t0 . Name  HAVING   (( SUM ( ALL   t1 . Milliseconds ))   /   ( 1000 ))   =   ( 300000 )  \n\n         \n    \n         \n    \n                 \n                      Beam will also handle the  filter_  correctly in the presence of more\ncomplicated queries. For example, we can now join our aggregate on genres back\nover tracks.  \n                 \n                    \n         \n            \n         \n             Haskell \n         \n    \n         \n             Sqlite3 \n         \n    \n         \n             Postgres \n         \n    \n         \n    \n         \n            \n         \n             -- Only return results for genres whose total track length is over 5 minutes  filter_   ( \\ ( genre ,   track ,   distinctPriceCount ,   totalTrackLength )   -   totalTrackLength   =.   300000 )   $  do   ( genre ,   priceCnt ,   trackLength )   - \n             aggregate_   ( \\ ( genre ,   track )   - \n                           (   group_   genre \n                           ,   as_   @ Int   $   countOver_   distinctInGroup_   ( trackUnitPrice   track ) \n                           ,   sumOver_   allInGroupExplicitly_   ( trackMilliseconds   track )   ` div_ `   1000   ))   $ \n             ((,)   $   all_   ( genre   chinookDb )   *   all_   ( track   chinookDb )) \n    track   -   join_   ( track   chinookDb )   ( \\ track   -   trackGenreId   track   ==.   just_   ( pk   genre )) \n    pure   ( genre ,   track ,   priceCnt ,   trackLength )  \n\n         \n    \n         \n             SELECT  t0 . res0  AS  res0 ,          t0 . res1  AS  res1 ,          t1 . TrackId  AS  res2 ,          t1 . Name  AS  res3 ,          t1 . AlbumId  AS  res4 ,          t1 . MediaTypeId  AS  res5 ,          t1 . GenreId  AS  res6 ,          t1 . Composer  AS  res7 ,          t1 . Milliseconds  AS  res8 ,          t1 . Bytes  AS  res9 ,          t1 . UnitPrice  AS  res10 ,          t0 . res2  AS  res11 ,          t0 . res3  AS  res12  FROM    (SELECT  t0 . GenreId  AS  res0 ,             t0 . Name  AS  res1 ,            COUNT(DISTINCT  t1 . UnitPrice ) AS  res2 ,            (SUM(ALL  t1 . Milliseconds )) / (?) AS  res3     FROM  Genre  AS  t0     INNER JOIN  Track  AS  t1     GROUP BY  t0 . GenreId ,               t0 . Name ) AS  t0  INNER JOIN  Track  AS  t1  ON ( t1 . GenreId )=( t0 . res0 )  WHERE ( t0 . res3 ) =(?)  \n\n         \n    \n         \n             SELECT   t0 . res0   AS   res0 , \n        t0 . res1   AS   res1 , \n        t1 . TrackId   AS   res2 , \n        t1 . Name   AS   res3 , \n        t1 . AlbumId   AS   res4 , \n        t1 . MediaTypeId   AS   res5 , \n        t1 . GenreId   AS   res6 , \n        t1 . Composer   AS   res7 , \n        t1 . Milliseconds   AS   res8 , \n        t1 . Bytes   AS   res9 , \n        t1 . UnitPrice   AS   res10 , \n        t0 . res2   AS   res11 , \n        t0 . res3   AS   res12  FROM \n   ( SELECT   t0 . GenreId   AS   res0 , \n           t0 . Name   AS   res1 , \n           COUNT ( DISTINCT   t1 . UnitPrice )   AS   res2 , \n           ( SUM ( ALL   t1 . Milliseconds ))   /   ( 1000 )   AS   res3 \n    FROM   Genre   AS   t0 \n    INNER   JOIN   Track   AS   t1 \n    GROUP   BY   t0 . GenreId , \n             t0 . Name )   AS   t0  INNER   JOIN   Track   AS   t1   ON   ( t1 . GenreId )   =   ( t0 . res0 )  WHERE   ( t0 . res3 )   =   ( 300000 )  \n\n         \n    \n         \n    \n                 \n                      The position of  filter_  changes the code generated. Above, the  filter_ \nproduced a  WHERE  clause on the outermost  SELECT . If instead, we put the filter_  clause right outside the  aggregate_ , beam will produce a  HAVING  clause instead.  \n                 \n                    \n         \n            \n         \n             Haskell \n         \n    \n         \n             Sqlite3 \n         \n    \n         \n             Postgres \n         \n    \n         \n    \n         \n            \n         \n             -- Only return results for genres whose total track length is over 5 minutes  do   ( genre ,   priceCnt ,   trackLength )   - \n             filter_   ( \\ ( genre ,   distinctPriceCount ,   totalTrackLength )   -   totalTrackLength   =.   300000 )   $ \n             aggregate_   ( \\ ( genre ,   track )   - \n                           (   group_   genre \n                           ,   as_   @ Int   $   countOver_   distinctInGroup_   ( trackUnitPrice   track ) \n                           ,   sumOver_   allInGroupExplicitly_   ( trackMilliseconds   track )   ` div_ `   1000   ))   $ \n             ((,)   $   all_   ( genre   chinookDb )   *   all_   ( track   chinookDb )) \n    track   -   join_   ( track   chinookDb )   ( \\ track   -   trackGenreId   track   ==.   just_   ( pk   genre )) \n    pure   ( genre ,   track ,   priceCnt ,   trackLength )  \n\n         \n    \n         \n             SELECT  t0 . res0  AS  res0 ,          t0 . res1  AS  res1 ,          t1 . TrackId  AS  res2 ,          t1 . Name  AS  res3 ,          t1 . AlbumId  AS  res4 ,          t1 . MediaTypeId  AS  res5 ,          t1 . GenreId  AS  res6 ,          t1 . Composer  AS  res7 ,          t1 . Milliseconds  AS  res8 ,          t1 . Bytes  AS  res9 ,          t1 . UnitPrice  AS  res10 ,          t0 . res2  AS  res11 ,          t0 . res3  AS  res12  FROM    (SELECT  t0 . GenreId  AS  res0 ,             t0 . Name  AS  res1 ,            COUNT(DISTINCT  t1 . UnitPrice ) AS  res2 ,            (SUM(ALL  t1 . Milliseconds )) / (?) AS  res3     FROM  Genre  AS  t0     INNER JOIN  Track  AS  t1     GROUP BY  t0 . GenreId ,               t0 . Name     HAVING ((SUM(ALL  t1 . Milliseconds )) / (?)) =(?)) AS  t0  INNER JOIN  Track  AS  t1  ON ( t1 . GenreId )=( t0 . res0 )  \n\n         \n    \n         \n             SELECT   t0 . res0   AS   res0 , \n        t0 . res1   AS   res1 , \n        t1 . TrackId   AS   res2 , \n        t1 . Name   AS   res3 , \n        t1 . AlbumId   AS   res4 , \n        t1 . MediaTypeId   AS   res5 , \n        t1 . GenreId   AS   res6 , \n        t1 . Composer   AS   res7 , \n        t1 . Milliseconds   AS   res8 , \n        t1 . Bytes   AS   res9 , \n        t1 . UnitPrice   AS   res10 , \n        t0 . res2   AS   res11 , \n        t0 . res3   AS   res12  FROM \n   ( SELECT   t0 . GenreId   AS   res0 , \n           t0 . Name   AS   res1 , \n           COUNT ( DISTINCT   t1 . UnitPrice )   AS   res2 , \n           ( SUM ( ALL   t1 . Milliseconds ))   /   ( 1000 )   AS   res3 \n    FROM   Genre   AS   t0 \n    INNER   JOIN   Track   AS   t1 \n    GROUP   BY   t0 . GenreId , \n             t0 . Name \n    HAVING   (( SUM ( ALL   t1 . Milliseconds ))   /   ( 1000 ))   =   ( 300000 ))   AS   t0  INNER   JOIN   Track   AS   t1   ON   ( t1 . GenreId )   =   ( t0 . res0 )  \n\n         \n    \n         \n    \n                 \n                      Due to the monadic structure, putting the filtered aggregate as the second\nclause in the JOIN causes the HAVING to be floated out, because the compiler\ncan't prove that the conditional expression only depends on the results of the\naggregate.  \n                 \n                    \n         \n            \n         \n             Haskell \n         \n    \n         \n             Sqlite3 \n         \n    \n         \n             Postgres \n         \n    \n         \n    \n         \n            \n         \n             -- Only return results for genres whose total track length is over 5 minutes  do   track_   -   all_   ( track   chinookDb ) \n    ( genre ,   priceCnt ,   trackLength )   - \n             filter_   ( \\ ( genre ,   distinctPriceCount ,   totalTrackLength )   -   totalTrackLength   =.   300000 )   $ \n             aggregate_   ( \\ ( genre ,   track )   - \n                           (   group_   genre \n                           ,   as_   @ Int   $   countOver_   distinctInGroup_   ( trackUnitPrice   track ) \n                           ,   sumOver_   allInGroupExplicitly_   ( trackMilliseconds   track )   ` div_ `   1000   ))   $ \n             ((,)   $   all_   ( genre   chinookDb )   *   all_   ( track   chinookDb ))  \n    guard_   ( trackGenreId   track_   ==.   just_   ( pk   genre )) \n    pure   ( genre ,   track_ ,   priceCnt ,   trackLength )  \n\n         \n    \n         \n             SELECT  t1 . res0  AS  res0 ,          t1 . res1  AS  res1 ,          t0 . TrackId  AS  res2 ,          t0 . Name  AS  res3 ,          t0 . AlbumId  AS  res4 ,          t0 . MediaTypeId  AS  res5 ,          t0 . GenreId  AS  res6 ,          t0 . Composer  AS  res7 ,          t0 . Milliseconds  AS  res8 ,          t0 . Bytes  AS  res9 ,          t0 . UnitPrice  AS  res10 ,          t1 . res2  AS  res11 ,          t1 . res3  AS  res12  FROM  Track  AS  t0  INNER JOIN    (SELECT  t0 . GenreId  AS  res0 ,             t0 . Name  AS  res1 ,            COUNT(DISTINCT  t1 . UnitPrice ) AS  res2 ,            (SUM(ALL  t1 . Milliseconds )) / (?) AS  res3     FROM  Genre  AS  t0     INNER JOIN  Track  AS  t1     GROUP BY  t0 . GenreId ,               t0 . Name ) AS  t1  WHERE (( t1 . res3 ) =(?))    AND (( t0 . GenreId )=( t1 . res0 ))  \n\n         \n    \n         \n             SELECT   t1 . res0   AS   res0 , \n        t1 . res1   AS   res1 , \n        t0 . TrackId   AS   res2 , \n        t0 . Name   AS   res3 , \n        t0 . AlbumId   AS   res4 , \n        t0 . MediaTypeId   AS   res5 , \n        t0 . GenreId   AS   res6 , \n        t0 . Composer   AS   res7 , \n        t0 . Milliseconds   AS   res8 , \n        t0 . Bytes   AS   res9 , \n        t0 . UnitPrice   AS   res10 , \n        t1 . res2   AS   res11 , \n        t1 . res3   AS   res12  FROM   Track   AS   t0  INNER   JOIN \n   ( SELECT   t0 . GenreId   AS   res0 , \n           t0 . Name   AS   res1 , \n           COUNT ( DISTINCT   t1 . UnitPrice )   AS   res2 , \n           ( SUM ( ALL   t1 . Milliseconds ))   /   ( 1000 )   AS   res3 \n    FROM   Genre   AS   t0 \n    INNER   JOIN   Track   AS   t1 \n    GROUP   BY   t0 . GenreId , \n             t0 . Name )   AS   t1  WHERE   (( t1 . res3 )   =   ( 300000 )) \n   AND   (( t0 . GenreId )   =   ( t1 . res0 ))  \n\n         \n    \n         \n    \n                 \n                      You can prove to the compiler that the  filter_  should generate a having by\nusing the  subselect_  combinator.  \n                 \n                    \n         \n            \n         \n             Haskell \n         \n    \n         \n             Sqlite3 \n         \n    \n         \n             Postgres \n         \n    \n         \n    \n         \n            \n         \n             -- Only return results for genres whose total track length is over 5 minutes  do   track_   -   all_   ( track   chinookDb ) \n    ( genre ,   priceCnt ,   trackLength )   - \n             subselect_   $ \n             filter_   ( \\ ( genre ,   distinctPriceCount ,   totalTrackLength )   -   totalTrackLength   =.   300000 )   $ \n             aggregate_   ( \\ ( genre ,   track )   - \n                           (   group_   genre \n                           ,   as_   @ Int   $   countOver_   distinctInGroup_   ( trackUnitPrice   track ) \n                           ,   sumOver_   allInGroupExplicitly_   ( trackMilliseconds   track )   ` div_ `   1000   ))   $ \n             ((,)   $   all_   ( genre   chinookDb )   *   all_   ( track   chinookDb ))  \n    guard_   ( trackGenreId   track_   ==.   just_   ( pk   genre )) \n    pure   ( genre ,   track_ ,   priceCnt ,   trackLength )  \n\n         \n    \n         \n             SELECT  t1 . res0  AS  res0 ,          t1 . res1  AS  res1 ,          t0 . TrackId  AS  res2 ,          t0 . Name  AS  res3 ,          t0 . AlbumId  AS  res4 ,          t0 . MediaTypeId  AS  res5 ,          t0 . GenreId  AS  res6 ,          t0 . Composer  AS  res7 ,          t0 . Milliseconds  AS  res8 ,          t0 . Bytes  AS  res9 ,          t0 . UnitPrice  AS  res10 ,          t1 . res2  AS  res11 ,          t1 . res3  AS  res12  FROM  Track  AS  t0  INNER JOIN    (SELECT  t0 . res0  AS  res0 ,             t0 . res1  AS  res1 ,             t0 . res2  AS  res2 ,             t0 . res3  AS  res3     FROM       (SELECT  t0 . GenreId  AS  res0 ,                t0 . Name  AS  res1 ,               COUNT(DISTINCT  t1 . UnitPrice ) AS  res2 ,               (SUM(ALL  t1 . Milliseconds )) / (?) AS  res3        FROM  Genre  AS  t0        INNER JOIN  Track  AS  t1        GROUP BY  t0 . GenreId ,                  t0 . Name        HAVING ((SUM(ALL  t1 . Milliseconds )) / (?)) =(?)) AS  t0 ) AS  t1  WHERE ( t0 . GenreId )=( t1 . res0 )  \n\n         \n    \n         \n             SELECT   t1 . res0   AS   res0 , \n        t1 . res1   AS   res1 , \n        t0 . TrackId   AS   res2 , \n        t0 . Name   AS   res3 , \n        t0 . AlbumId   AS   res4 , \n        t0 . MediaTypeId   AS   res5 , \n        t0 . GenreId   AS   res6 , \n        t0 . Composer   AS   res7 , \n        t0 . Milliseconds   AS   res8 , \n        t0 . Bytes   AS   res9 , \n        t0 . UnitPrice   AS   res10 , \n        t1 . res2   AS   res11 , \n        t1 . res3   AS   res12  FROM   Track   AS   t0  INNER   JOIN \n   ( SELECT   t0 . res0   AS   res0 , \n           t0 . res1   AS   res1 , \n           t0 . res2   AS   res2 , \n           t0 . res3   AS   res3 \n    FROM \n      ( SELECT   t0 . GenreId   AS   res0 , \n              t0 . Name   AS   res1 , \n              COUNT ( DISTINCT   t1 . UnitPrice )   AS   res2 , \n              ( SUM ( ALL   t1 . Milliseconds ))   /   ( 1000 )   AS   res3 \n       FROM   Genre   AS   t0 \n       INNER   JOIN   Track   AS   t1 \n       GROUP   BY   t0 . GenreId , \n                t0 . Name \n       HAVING   (( SUM ( ALL   t1 . Milliseconds ))   /   ( 1000 ))   =   ( 300000 ))   AS   t0 )   AS   t1  WHERE   ( t0 . GenreId )   =   ( t1 . res0 )", 
            "title": "The HAVING clause"
        }, 
        {
            "location": "/user-guide/queries/combining-queries/", 
            "text": "SQL lets you combine the results of multiple \nSELECT\n statements using the\n\nUNION\n, \nINTERSECT\n, and \nEXCEPT\n clauses.\n\n\nSQL Set operations\n\n\nThe SQL Set operations are provided as the \nunion_\n, \nintersect_\n, and \nexcept_\n\nfunctions. SQL also allows an optional \nALL\n clause to be specified with each of\nthese. Beam implements these as \nunionAll_\n, \nintersectAll_\n, and \nexceptAll_\n\nrespectively. Each combinator takes two queries as arguments. The results of\nboth queries will be combined accordingly. The returned type is the same as the\ntype of both query arguments, which must be the same.\n\n\nFor example, suppose we wanted the first and last names of both customers and\nemployees.\n\n\n\n                \n\n                    \n        \n\n            \n        \n\n            \nHaskell\n\n        \n\n    \n        \n\n            \nSqlite3\n\n        \n\n    \n        \n\n            \nPostgres\n\n        \n\n    \n        \n\n    \n        \n\n            \n        \n\n            \nlet\n \ncustomerNames\n \n=\n\n      \nfmap\n \n(\n\\\nc\n \n-\n \n(\ncustomerFirstName\n \nc\n,\n \ncustomerLastName\n \nc\n))\n\n           \n(\nall_\n \n(\ncustomer\n \nchinookDb\n))\n\n    \nemployeeNames\n \n=\n\n      \nfmap\n \n(\n\\\ne\n \n-\n \n(\nemployeeFirstName\n \ne\n,\n \nemployeeLastName\n \ne\n))\n\n           \n(\nall_\n \n(\nemployee\n \nchinookDb\n))\n\n\nin\n \nunion_\n \ncustomerNames\n \nemployeeNames\n\n\n\n\n        \n\n    \n        \n\n            \nSELECT \nt0\n.\nFirstName\n AS \nres0\n,\n\n\n       \nt0\n.\nLastName\n AS \nres1\n\n\nFROM \nCustomer\n AS \nt0\n\n\nUNION\n\n\nSELECT \nt0\n.\nFirstName\n AS \nres0\n,\n\n\n       \nt0\n.\nLastName\n AS \nres1\n\n\nFROM \nEmployee\n AS \nt0\n\n\n\n\n        \n\n    \n        \n\n            \n  \n(\nSELECT\n \nt0\n.\nFirstName\n \nAS\n \nres0\n,\n\n          \nt0\n.\nLastName\n \nAS\n \nres1\n\n   \nFROM\n \nCustomer\n \nAS\n \nt0\n)\n\n\nUNION\n\n  \n(\nSELECT\n \nt0\n.\nFirstName\n \nAS\n \nres0\n,\n\n          \nt0\n.\nLastName\n \nAS\n \nres1\n\n   \nFROM\n \nEmployee\n \nAS\n \nt0\n)\n\n\n\n\n        \n\n    \n        \n\n    \n                \n\n                    \n\n\nCombining arbitrary set expressions\n\n\nSuppose we wanted all employee and customer first names that were also customer\nlast names but not also employee last names. We could use \nUNION\n to combine the\nresults of a query over the first names of employees and customers, and an\n\nEXCEPT\n to get all customer last names that were not employee ones. Finally, an\n\nINTERSECT\n would give us the result we want. The beam query language allows\nthis and many popular backends do as well, but standard SQL makes it difficult\nto express. Beam has decided to go with the most common implement solution,\nwhich is to allow such nesting. This simplifies the API design.\n\n\nOn backends which allow such nesting (like Postgres), the query is translated\ndirectly. On backends that do not (like SQLite), an appropriate subselect is\ngenerated.\n\n\n\n                \n\n                    \n        \n\n            \n        \n\n            \nHaskell\n\n        \n\n    \n        \n\n            \nSqlite3\n\n        \n\n    \n        \n\n            \nPostgres\n\n        \n\n    \n        \n\n    \n        \n\n            \n        \n\n            \nlet\n \ncustomerFirstNames\n \n=\n\n      \nfmap\n \ncustomerFirstName\n\n           \n(\nall_\n \n(\ncustomer\n \nchinookDb\n))\n\n    \nemployeeFirstNames\n \n=\n\n      \nfmap\n \nemployeeFirstName\n\n           \n(\nall_\n \n(\nemployee\n \nchinookDb\n))\n\n    \ncustomerLastNames\n \n=\n\n      \nfmap\n \ncustomerLastName\n\n           \n(\nall_\n \n(\ncustomer\n \nchinookDb\n))\n\n    \nemployeeLastNames\n \n=\n\n      \nfmap\n \nemployeeLastName\n\n           \n(\nall_\n \n(\nemployee\n \nchinookDb\n))\n\n\nin\n \n(\ncustomerFirstNames\n \n`\nunion_\n`\nemployeeFirstNames\n)\n \n`\nintersect_\n`\n\n   \n(\ncustomerLastNames\n \n`\nexcept_\n`\n \nemployeeLastNames\n)\n\n\n\n\n        \n\n    \n        \n\n            \nSELECT \nt0\n.\nres0\n AS \nres0\n\n\nFROM\n\n\n  (SELECT \nt0\n.\nFirstName\n AS \nres0\n\n\n   FROM \nCustomer\n AS \nt0\n\n\n   UNION SELECT \nt0\n.\nFirstName\n AS \nres0\n\n\n   FROM \nEmployee\n AS \nt0\n) AS \nt0\n INTERSECT\n\n\nSELECT \nt0\n.\nres0\n AS \nres0\n\n\nFROM\n\n\n  (SELECT \nt0\n.\nLastName\n AS \nres0\n\n\n   FROM \nCustomer\n AS \nt0\n\n\n   EXCEPT SELECT \nt0\n.\nLastName\n AS \nres0\n\n\n   FROM \nEmployee\n AS \nt0\n) AS \nt0\n\n\n\n\n        \n\n    \n        \n\n            \n(\n\n   \n(\nSELECT\n \nt0\n.\nFirstName\n \nAS\n \nres0\n\n    \nFROM\n \nCustomer\n \nAS\n \nt0\n)\n\n \nUNION\n\n   \n(\nSELECT\n \nt0\n.\nFirstName\n \nAS\n \nres0\n\n    \nFROM\n \nEmployee\n \nAS\n \nt0\n))\n \nINTERSECT\n \n(\n\n                                           \n(\nSELECT\n \nt0\n.\nLastName\n \nAS\n \nres0\n\n                                            \nFROM\n \nCustomer\n \nAS\n \nt0\n)\n\n                                         \nEXCEPT\n\n                                           \n(\nSELECT\n \nt0\n.\nLastName\n \nAS\n \nres0\n\n                                            \nFROM\n \nEmployee\n \nAS\n \nt0\n))\n\n\n\n\n        \n\n    \n        \n\n    \n                \n\n                    \n\n\nLIMIT\n/\nOFFSET\n and set operations\n\n\nThe \nLIMIT\n and \nOFFSET\n clauses generated by \nlimit_\n and \noffset_\n apply to\nthe entire result of the set operation. Beam will correctly generate the query\nyou specify, placing the \nLIMIT\n and \nOFFSET\n at the appropriate point. If\nnecessary, it will also generate a sub select to preserve the meaning of the\nquery.\n\n\nFor example, to get the second ten full names in common.\n\n\n\n                \n\n                    \n        \n\n            \n        \n\n            \nHaskell\n\n        \n\n    \n        \n\n            \nSqlite3\n\n        \n\n    \n        \n\n            \nPostgres\n\n        \n\n    \n        \n\n    \n        \n\n            \n        \n\n            \nlet\n \ncustomerNames\n \n=\n\n      \nfmap\n \n(\n\\\nc\n \n-\n \n(\ncustomerFirstName\n \nc\n,\n \ncustomerLastName\n \nc\n))\n\n           \n(\nall_\n \n(\ncustomer\n \nchinookDb\n))\n\n    \nemployeeNames\n \n=\n\n      \nfmap\n \n(\n\\\ne\n \n-\n \n(\nemployeeFirstName\n \ne\n,\n \nemployeeLastName\n \ne\n))\n\n           \n(\nall_\n \n(\nemployee\n \nchinookDb\n))\n\n\nin\n \nlimit_\n \n10\n \n(\nunion_\n \ncustomerNames\n \nemployeeNames\n)\n\n\n\n\n        \n\n    \n        \n\n            \nSELECT \nt0\n.\nFirstName\n AS \nres0\n,\n\n\n       \nt0\n.\nLastName\n AS \nres1\n\n\nFROM \nCustomer\n AS \nt0\n\n\nUNION\n\n\nSELECT \nt0\n.\nFirstName\n AS \nres0\n,\n\n\n       \nt0\n.\nLastName\n AS \nres1\n\n\nFROM \nEmployee\n AS \nt0\n\n\nLIMIT 10\n\n\n\n\n        \n\n    \n        \n\n            \n  \n(\nSELECT\n \nt0\n.\nFirstName\n \nAS\n \nres0\n,\n\n          \nt0\n.\nLastName\n \nAS\n \nres1\n\n   \nFROM\n \nCustomer\n \nAS\n \nt0\n)\n\n\nUNION\n\n  \n(\nSELECT\n \nt0\n.\nFirstName\n \nAS\n \nres0\n,\n\n          \nt0\n.\nLastName\n \nAS\n \nres1\n\n   \nFROM\n \nEmployee\n \nAS\n \nt0\n)\n\n\nLIMIT\n \n10\n\n\n\n\n        \n\n    \n        \n\n    \n                \n\n                    \n\n\nIf we only wanted the union of the first 10 names of each.\n\n\n\n                \n\n                    \n        \n\n            \n        \n\n            \nHaskell\n\n        \n\n    \n        \n\n            \nSqlite3\n\n        \n\n    \n        \n\n            \nPostgres\n\n        \n\n    \n        \n\n    \n        \n\n            \n        \n\n            \nlet\n \ncustomerNames\n \n=\n\n      \nfmap\n \n(\n\\\nc\n \n-\n \n(\ncustomerFirstName\n \nc\n,\n \ncustomerLastName\n \nc\n))\n\n           \n(\nall_\n \n(\ncustomer\n \nchinookDb\n))\n\n    \nemployeeNames\n \n=\n\n      \nfmap\n \n(\n\\\ne\n \n-\n \n(\nemployeeFirstName\n \ne\n,\n \nemployeeLastName\n \ne\n))\n\n           \n(\nall_\n \n(\nemployee\n \nchinookDb\n))\n\n\nin\n \nunion_\n \n(\nlimit_\n \n10\n \ncustomerNames\n)\n \n(\nlimit_\n \n10\n \nemployeeNames\n)\n\n\n\n\n        \n\n    \n        \n\n            \nSELECT \nt0\n.\nres0\n AS \nres0\n,\n\n\n       \nt0\n.\nres1\n AS \nres1\n\n\nFROM\n\n\n  (SELECT \nt0\n.\nFirstName\n AS \nres0\n,\n\n\n          \nt0\n.\nLastName\n AS \nres1\n\n\n   FROM \nCustomer\n AS \nt0\n\n\n   LIMIT 10) AS \nt0\n\n\nUNION\n\n\nSELECT \nt0\n.\nres0\n AS \nres0\n,\n\n\n       \nt0\n.\nres1\n AS \nres1\n\n\nFROM\n\n\n  (SELECT \nt0\n.\nFirstName\n AS \nres0\n,\n\n\n          \nt0\n.\nLastName\n AS \nres1\n\n\n   FROM \nEmployee\n AS \nt0\n\n\n   LIMIT 10) AS \nt0\n\n\n\n\n        \n\n    \n        \n\n            \n  \n(\nSELECT\n \nt0\n.\nres0\n \nAS\n \nres0\n,\n\n          \nt0\n.\nres1\n \nAS\n \nres1\n\n   \nFROM\n\n     \n(\nSELECT\n \nt0\n.\nFirstName\n \nAS\n \nres0\n,\n\n             \nt0\n.\nLastName\n \nAS\n \nres1\n\n      \nFROM\n \nCustomer\n \nAS\n \nt0\n\n      \nLIMIT\n \n10\n)\n \nAS\n \nt0\n)\n\n\nUNION\n\n  \n(\nSELECT\n \nt0\n.\nres0\n \nAS\n \nres0\n,\n\n          \nt0\n.\nres1\n \nAS\n \nres1\n\n   \nFROM\n\n     \n(\nSELECT\n \nt0\n.\nFirstName\n \nAS\n \nres0\n,\n\n             \nt0\n.\nLastName\n \nAS\n \nres1\n\n      \nFROM\n \nEmployee\n \nAS\n \nt0\n\n      \nLIMIT\n \n10\n)\n \nAS\n \nt0\n)", 
            "title": "Combining queries"
        }, 
        {
            "location": "/user-guide/queries/combining-queries/#sql-set-operations", 
            "text": "The SQL Set operations are provided as the  union_ ,  intersect_ , and  except_ \nfunctions. SQL also allows an optional  ALL  clause to be specified with each of\nthese. Beam implements these as  unionAll_ ,  intersectAll_ , and  exceptAll_ \nrespectively. Each combinator takes two queries as arguments. The results of\nboth queries will be combined accordingly. The returned type is the same as the\ntype of both query arguments, which must be the same.  For example, suppose we wanted the first and last names of both customers and\nemployees.  \n                 \n                    \n         \n            \n         \n             Haskell \n         \n    \n         \n             Sqlite3 \n         \n    \n         \n             Postgres \n         \n    \n         \n    \n         \n            \n         \n             let   customerNames   = \n       fmap   ( \\ c   -   ( customerFirstName   c ,   customerLastName   c )) \n            ( all_   ( customer   chinookDb )) \n     employeeNames   = \n       fmap   ( \\ e   -   ( employeeFirstName   e ,   employeeLastName   e )) \n            ( all_   ( employee   chinookDb ))  in   union_   customerNames   employeeNames  \n\n         \n    \n         \n             SELECT  t0 . FirstName  AS  res0 ,          t0 . LastName  AS  res1  FROM  Customer  AS  t0  UNION  SELECT  t0 . FirstName  AS  res0 ,          t0 . LastName  AS  res1  FROM  Employee  AS  t0  \n\n         \n    \n         \n                ( SELECT   t0 . FirstName   AS   res0 , \n           t0 . LastName   AS   res1 \n    FROM   Customer   AS   t0 )  UNION \n   ( SELECT   t0 . FirstName   AS   res0 , \n           t0 . LastName   AS   res1 \n    FROM   Employee   AS   t0 )", 
            "title": "SQL Set operations"
        }, 
        {
            "location": "/user-guide/queries/combining-queries/#combining-arbitrary-set-expressions", 
            "text": "Suppose we wanted all employee and customer first names that were also customer\nlast names but not also employee last names. We could use  UNION  to combine the\nresults of a query over the first names of employees and customers, and an EXCEPT  to get all customer last names that were not employee ones. Finally, an INTERSECT  would give us the result we want. The beam query language allows\nthis and many popular backends do as well, but standard SQL makes it difficult\nto express. Beam has decided to go with the most common implement solution,\nwhich is to allow such nesting. This simplifies the API design.  On backends which allow such nesting (like Postgres), the query is translated\ndirectly. On backends that do not (like SQLite), an appropriate subselect is\ngenerated.  \n                 \n                    \n         \n            \n         \n             Haskell \n         \n    \n         \n             Sqlite3 \n         \n    \n         \n             Postgres \n         \n    \n         \n    \n         \n            \n         \n             let   customerFirstNames   = \n       fmap   customerFirstName \n            ( all_   ( customer   chinookDb )) \n     employeeFirstNames   = \n       fmap   employeeFirstName \n            ( all_   ( employee   chinookDb )) \n     customerLastNames   = \n       fmap   customerLastName \n            ( all_   ( customer   chinookDb )) \n     employeeLastNames   = \n       fmap   employeeLastName \n            ( all_   ( employee   chinookDb ))  in   ( customerFirstNames   ` union_ ` employeeFirstNames )   ` intersect_ ` \n    ( customerLastNames   ` except_ `   employeeLastNames )  \n\n         \n    \n         \n             SELECT  t0 . res0  AS  res0  FROM    (SELECT  t0 . FirstName  AS  res0     FROM  Customer  AS  t0     UNION SELECT  t0 . FirstName  AS  res0     FROM  Employee  AS  t0 ) AS  t0  INTERSECT  SELECT  t0 . res0  AS  res0  FROM    (SELECT  t0 . LastName  AS  res0     FROM  Customer  AS  t0     EXCEPT SELECT  t0 . LastName  AS  res0     FROM  Employee  AS  t0 ) AS  t0  \n\n         \n    \n         \n             ( \n    ( SELECT   t0 . FirstName   AS   res0 \n     FROM   Customer   AS   t0 ) \n  UNION \n    ( SELECT   t0 . FirstName   AS   res0 \n     FROM   Employee   AS   t0 ))   INTERSECT   ( \n                                            ( SELECT   t0 . LastName   AS   res0 \n                                             FROM   Customer   AS   t0 ) \n                                          EXCEPT \n                                            ( SELECT   t0 . LastName   AS   res0 \n                                             FROM   Employee   AS   t0 ))", 
            "title": "Combining arbitrary set expressions"
        }, 
        {
            "location": "/user-guide/queries/combining-queries/#limitoffset-and-set-operations", 
            "text": "The  LIMIT  and  OFFSET  clauses generated by  limit_  and  offset_  apply to\nthe entire result of the set operation. Beam will correctly generate the query\nyou specify, placing the  LIMIT  and  OFFSET  at the appropriate point. If\nnecessary, it will also generate a sub select to preserve the meaning of the\nquery.  For example, to get the second ten full names in common.  \n                 \n                    \n         \n            \n         \n             Haskell \n         \n    \n         \n             Sqlite3 \n         \n    \n         \n             Postgres \n         \n    \n         \n    \n         \n            \n         \n             let   customerNames   = \n       fmap   ( \\ c   -   ( customerFirstName   c ,   customerLastName   c )) \n            ( all_   ( customer   chinookDb )) \n     employeeNames   = \n       fmap   ( \\ e   -   ( employeeFirstName   e ,   employeeLastName   e )) \n            ( all_   ( employee   chinookDb ))  in   limit_   10   ( union_   customerNames   employeeNames )  \n\n         \n    \n         \n             SELECT  t0 . FirstName  AS  res0 ,          t0 . LastName  AS  res1  FROM  Customer  AS  t0  UNION  SELECT  t0 . FirstName  AS  res0 ,          t0 . LastName  AS  res1  FROM  Employee  AS  t0  LIMIT 10  \n\n         \n    \n         \n                ( SELECT   t0 . FirstName   AS   res0 , \n           t0 . LastName   AS   res1 \n    FROM   Customer   AS   t0 )  UNION \n   ( SELECT   t0 . FirstName   AS   res0 , \n           t0 . LastName   AS   res1 \n    FROM   Employee   AS   t0 )  LIMIT   10  \n\n         \n    \n         \n    \n                 \n                      If we only wanted the union of the first 10 names of each.  \n                 \n                    \n         \n            \n         \n             Haskell \n         \n    \n         \n             Sqlite3 \n         \n    \n         \n             Postgres \n         \n    \n         \n    \n         \n            \n         \n             let   customerNames   = \n       fmap   ( \\ c   -   ( customerFirstName   c ,   customerLastName   c )) \n            ( all_   ( customer   chinookDb )) \n     employeeNames   = \n       fmap   ( \\ e   -   ( employeeFirstName   e ,   employeeLastName   e )) \n            ( all_   ( employee   chinookDb ))  in   union_   ( limit_   10   customerNames )   ( limit_   10   employeeNames )  \n\n         \n    \n         \n             SELECT  t0 . res0  AS  res0 ,          t0 . res1  AS  res1  FROM    (SELECT  t0 . FirstName  AS  res0 ,             t0 . LastName  AS  res1     FROM  Customer  AS  t0     LIMIT 10) AS  t0  UNION  SELECT  t0 . res0  AS  res0 ,          t0 . res1  AS  res1  FROM    (SELECT  t0 . FirstName  AS  res0 ,             t0 . LastName  AS  res1     FROM  Employee  AS  t0     LIMIT 10) AS  t0  \n\n         \n    \n         \n                ( SELECT   t0 . res0   AS   res0 , \n           t0 . res1   AS   res1 \n    FROM \n      ( SELECT   t0 . FirstName   AS   res0 , \n              t0 . LastName   AS   res1 \n       FROM   Customer   AS   t0 \n       LIMIT   10 )   AS   t0 )  UNION \n   ( SELECT   t0 . res0   AS   res0 , \n           t0 . res1   AS   res1 \n    FROM \n      ( SELECT   t0 . FirstName   AS   res0 , \n              t0 . LastName   AS   res1 \n       FROM   Employee   AS   t0 \n       LIMIT   10 )   AS   t0 )", 
            "title": "LIMIT/OFFSET and set operations"
        }, 
        {
            "location": "/user-guide/queries/window-functions/", 
            "text": "Window functions allow you to calculate aggregates over portions of your result\nset. They are defined in SQL2003. Some databases use the alternative\nnomenclature \nanalytic functions\n. They are expressed in SQL with the \nOVER\n\nclause. The\n\nPostgres documentation\n\noffers a good overview of window functions.\n\n\nThe \nwithWindow_\n function\n\n\nWhen you want to add windows to a query, use the \nwithWindow_\n function to\nintroduce your frames, and compute the projection. You may notice that this is a\ndeparture from SQL syntax, where you can define window expressions inline. Beam\nseeks to be type-safe. Queries with window functions follow slightly different\nrules. Wrapping such a query with a special function allows beam to enforce\nthese rules.\n\n\nFor example, to get each invoice along with the average invoice total by each\ncustomer, use \nwithWindow_\n as follows.\n\n\n\n                \n\n                    \n        \n\n            \n        \n\n            \nHaskell\n\n        \n\n    \n        \n\n            \nPostgres\n\n        \n\n    \n        \n\n    \n        \n\n            \n        \n\n            \nwithWindow_\n \n(\n\\\ni\n \n-\n \nframe_\n \n(\npartitionBy_\n \n(\ninvoiceCustomer\n \ni\n))\n \nnoOrder_\n \nnoBounds_\n)\n\n            \n(\n\\\ni\n \nw\n \n-\n \n(\ni\n,\n \navg_\n \n(\ninvoiceTotal\n \ni\n)\n \n`\nover_\n`\n \nw\n))\n\n            \n(\nall_\n \n(\ninvoice\n \nchinookDb\n))\n\n\n\n\n        \n\n    \n        \n\n            \nSELECT\n \nt0\n.\nInvoiceId\n \nAS\n \nres0\n,\n\n       \nt0\n.\nCustomerId\n \nAS\n \nres1\n,\n\n       \nt0\n.\nInvoiceDate\n \nAS\n \nres2\n,\n\n       \nt0\n.\nBillingAddress\n \nAS\n \nres3\n,\n\n       \nt0\n.\nBillingCity\n \nAS\n \nres4\n,\n\n       \nt0\n.\nBillingState\n \nAS\n \nres5\n,\n\n       \nt0\n.\nBillingCountry\n \nAS\n \nres6\n,\n\n       \nt0\n.\nBillingPostalCode\n \nAS\n \nres7\n,\n\n       \nt0\n.\nTotal\n \nAS\n \nres8\n,\n\n       \nAVG\n(\nt0\n.\nTotal\n)\n \nOVER\n \n(\nPARTITION\n \nBY\n \nt0\n.\nCustomerId\n)\n \nAS\n \nres9\n\n\nFROM\n \nInvoice\n \nAS\n \nt0\n\n\n\n\n        \n\n    \n        \n\n    \n                \n\n                    \n\n\nOr to get each invoice along with the ranking of each invoice by total per\ncustomer \nand\n the overall ranking,\n\n\n\n                \n\n                    \n        \n\n            \n        \n\n            \nHaskell\n\n        \n\n    \n        \n\n            \nPostgres\n\n        \n\n    \n        \n\n    \n        \n\n            \n        \n\n            \nwithWindow_\n \n(\n\\\ni\n \n-\n \n(\n \nframe_\n \nnoPartition_\n \n(\norderPartitionBy_\n \n(\nasc_\n \n(\ninvoiceTotal\n \ni\n)))\n \nnoBounds_\n\n                   \n,\n \nframe_\n \n(\npartitionBy_\n \n(\ninvoiceCustomer\n \ni\n))\n \n(\norderPartitionBy_\n \n(\nasc_\n \n(\ninvoiceTotal\n \ni\n)))\n \nnoBounds_\n \n))\n\n            \n(\n\\\ni\n \n(\nallInvoices\n,\n \ncustomerInvoices\n)\n \n-\n \n(\ni\n,\n \nrank_\n \n`\nover_\n`\n \nallInvoices\n,\n \nrank_\n \n`\nover_\n`\n \ncustomerInvoices\n))\n\n            \n(\nall_\n \n(\ninvoice\n \nchinookDb\n))\n\n\n\n\n        \n\n    \n        \n\n            \nSELECT\n \nt0\n.\nInvoiceId\n \nAS\n \nres0\n,\n\n       \nt0\n.\nCustomerId\n \nAS\n \nres1\n,\n\n       \nt0\n.\nInvoiceDate\n \nAS\n \nres2\n,\n\n       \nt0\n.\nBillingAddress\n \nAS\n \nres3\n,\n\n       \nt0\n.\nBillingCity\n \nAS\n \nres4\n,\n\n       \nt0\n.\nBillingState\n \nAS\n \nres5\n,\n\n       \nt0\n.\nBillingCountry\n \nAS\n \nres6\n,\n\n       \nt0\n.\nBillingPostalCode\n \nAS\n \nres7\n,\n\n       \nt0\n.\nTotal\n \nAS\n \nres8\n,\n\n       \nRANK\n()\n \nOVER\n \n(\n\n                    \nORDER\n \nBY\n \nt0\n.\nTotal\n \nASC\n)\n \nAS\n \nres9\n,\n\n       \nRANK\n()\n \nOVER\n \n(\nPARTITION\n \nBY\n \nt0\n.\nCustomerId\n\n                    \nORDER\n \nBY\n \nt0\n.\nTotal\n \nASC\n)\n \nAS\n \nres10\n\n\nFROM\n \nInvoice\n \nAS\n \nt0\n\n\n\n\n        \n\n    \n        \n\n    \n                \n\n                    \n\n\n\n\nNote\n\n\nrank_\n is only available in backends that implement the optional SQL2003\nT611 feature \"Elementary OLAP operations\". Beam syntaxes that implement this\nfunctionality implement the\n\nIsSql2003ExpressionElementaryOLAPOperationsSyntax\n type class.\n\n\n\n\nNotice that aggregates over the result of the window expression work as you'd\nexpect. Beam automatically generates a subquery once a query has been windowed.\nFor example, to get the sum of the totals of the invoices, by rank.\n\n\n\n                \n\n                    \n        \n\n            \n        \n\n            \nHaskell\n\n        \n\n    \n        \n\n            \nPostgres\n\n        \n\n    \n        \n\n    \n        \n\n            \n        \n\n            \norderBy_\n \n(\n\\\n(\nrank\n,\n \n_\n)\n \n-\n \nasc_\n \nrank\n)\n \n$\n\n\naggregate_\n \n(\n\\\n(\ni\n,\n \nrank\n)\n \n-\n \n(\ngroup_\n \nrank\n,\n \nsum_\n \n$\n \ninvoiceTotal\n \ni\n))\n \n$\n\n\nwithWindow_\n \n(\n\\\ni\n \n-\n \nframe_\n \n(\npartitionBy_\n \n(\ninvoiceCustomer\n \ni\n))\n \n(\norderPartitionBy_\n \n(\nasc_\n \n(\ninvoiceTotal\n \ni\n)))\n \nnoBounds_\n)\n\n            \n(\n\\\ni\n \nw\n \n-\n \n(\ni\n,\n \nrank_\n \n`\nover_\n`\n \nw\n))\n\n            \n(\nall_\n \n(\ninvoice\n \nchinookDb\n))\n\n\n\n\n        \n\n    \n        \n\n            \nSELECT\n \nt0\n.\nres9\n \nAS\n \nres0\n,\n\n       \nSUM\n(\nt0\n.\nres8\n)\n \nAS\n \nres1\n\n\nFROM\n\n  \n(\nSELECT\n \nt0\n.\nInvoiceId\n \nAS\n \nres0\n,\n\n          \nt0\n.\nCustomerId\n \nAS\n \nres1\n,\n\n          \nt0\n.\nInvoiceDate\n \nAS\n \nres2\n,\n\n          \nt0\n.\nBillingAddress\n \nAS\n \nres3\n,\n\n          \nt0\n.\nBillingCity\n \nAS\n \nres4\n,\n\n          \nt0\n.\nBillingState\n \nAS\n \nres5\n,\n\n          \nt0\n.\nBillingCountry\n \nAS\n \nres6\n,\n\n          \nt0\n.\nBillingPostalCode\n \nAS\n \nres7\n,\n\n          \nt0\n.\nTotal\n \nAS\n \nres8\n,\n\n          \nRANK\n()\n \nOVER\n \n(\nPARTITION\n \nBY\n \nt0\n.\nCustomerId\n\n                       \nORDER\n \nBY\n \nt0\n.\nTotal\n \nASC\n)\n \nAS\n \nres9\n\n   \nFROM\n \nInvoice\n \nAS\n \nt0\n)\n \nAS\n \nt0\n\n\nGROUP\n \nBY\n \nt0\n.\nres9\n\n\nORDER\n \nBY\n \nt0\n.\nres9\n \nASC\n\n\n\n\n        \n\n    \n        \n\n    \n                \n\n                    \n\n\nFrame syntax\n\n\nThe \nframe_\n function takes a partition, ordering, and bounds parameter, all of\nwhich are optional. To specify no partition, use \nnoPartition_\n. For no\nordering, use \nnoOrder_\n. For no bounds, use \nnoBounds_\n.\n\n\nTo specify a partition, use \npartitionBy_\n with an expression or a tuple of\nexpressions. To specify an ordering use \norderPartitionBy_\n with an ordering\nexpression or a tuple of ordering expressions. Ordering expressions are scalar\nexpressions passed to either \nasc_\n or \ndesc_\n. Finally, to specify bounds, use\n\nbounds_\n or \nfromBound_\n. \nfromBound_\n starts the window at the specified\nposition, which can be \nunbounded_\n (the default) to include all rows seen thus\nfar. \nbounds_\n lets you specify an optional ending bound, which can be \nNothing\n\n(the default), \nJust unbounded_\n (the semantic default, but producing an\nexplicit bound syntactically), or \nJust (nrows_ x)\n, where \nx\n is an integer\nexpression, specifying the number of rows before or after to include in the\ncalculation.\n\n\nThe following query illustrates some of these features. Along with each invoice, it returns\n\n\n\n\nThe average total of all invoices, given by the frame with no partition, ordering, and bounds.\n\n\nThe average total of all invoices, by customer.\n\n\nThe rank of each invoice over all the rows, when ordered by total.\n\n\nThe average of the totals of the invoices starting at the two immediately\n  preceding and ending with the two immediately succeeding invoices, when\n  ordered by date.\n\n\n\n\n\n                \n\n                    \n        \n\n            \n        \n\n            \nHaskell\n\n        \n\n    \n        \n\n            \nPostgres\n\n        \n\n    \n        \n\n    \n        \n\n            \n        \n\n            \nwithWindow_\n \n(\n\\\ni\n \n-\n \n(\n \nframe_\n \nnoPartition_\n \nnoOrder_\n \nnoBounds_\n\n                   \n,\n \nframe_\n \n(\npartitionBy_\n \n(\ninvoiceCustomer\n \ni\n))\n \nnoOrder_\n \nnoBounds_\n\n                   \n,\n \nframe_\n \nnoPartition_\n \n(\norderPartitionBy_\n \n(\nasc_\n \n(\ninvoiceTotal\n \ni\n)))\n \nnoBounds_\n\n                   \n,\n \nframe_\n \nnoPartition_\n \n(\norderPartitionBy_\n \n(\nasc_\n \n(\ninvoiceDate\n \ni\n)))\n \n(\nbounds_\n \n(\nnrows_\n \n2\n)\n \n(\nJust\n \n(\nnrows_\n \n2\n)))))\n\n            \n(\n\\\ni\n \n(\nallRows_\n,\n \nsameCustomer_\n,\n \ntotals_\n,\n \nfourInvoicesAround_\n)\n \n-\n\n                 \n(\n \ni\n\n                 \n,\n \navg_\n \n(\ninvoiceTotal\n \ni\n)\n \n`\nover_\n`\n \nallRows_\n\n                 \n,\n \navg_\n \n(\ninvoiceTotal\n \ni\n)\n \n`\nover_\n`\n \nsameCustomer_\n\n                 \n,\n \nrank_\n \n`\nover_\n`\n \ntotals_\n\n                 \n,\n \navg_\n \n(\ninvoiceTotal\n \ni\n)\n \n`\nover_\n`\n \nfourInvoicesAround_\n \n))\n\n            \n(\nall_\n \n(\ninvoice\n \nchinookDb\n))\n\n\n\n\n        \n\n    \n        \n\n            \nSELECT\n \nt0\n.\nInvoiceId\n \nAS\n \nres0\n,\n\n       \nt0\n.\nCustomerId\n \nAS\n \nres1\n,\n\n       \nt0\n.\nInvoiceDate\n \nAS\n \nres2\n,\n\n       \nt0\n.\nBillingAddress\n \nAS\n \nres3\n,\n\n       \nt0\n.\nBillingCity\n \nAS\n \nres4\n,\n\n       \nt0\n.\nBillingState\n \nAS\n \nres5\n,\n\n       \nt0\n.\nBillingCountry\n \nAS\n \nres6\n,\n\n       \nt0\n.\nBillingPostalCode\n \nAS\n \nres7\n,\n\n       \nt0\n.\nTotal\n \nAS\n \nres8\n,\n\n       \nAVG\n(\nt0\n.\nTotal\n)\n \nOVER\n \n()\n \nAS\n \nres9\n,\n\n       \nAVG\n(\nt0\n.\nTotal\n)\n \nOVER\n \n(\nPARTITION\n \nBY\n \nt0\n.\nCustomerId\n)\n \nAS\n \nres10\n,\n\n       \nRANK\n()\n \nOVER\n \n(\n\n                    \nORDER\n \nBY\n \nt0\n.\nTotal\n \nASC\n)\n \nAS\n \nres11\n,\n\n       \nAVG\n(\nt0\n.\nTotal\n)\n \nOVER\n \n(\n\n                               \nORDER\n \nBY\n \nt0\n.\nInvoiceDate\n \nASC\n \nROWS\n \nBETWEEN\n \n2\n \nPRECEDING\n \nAND\n \n2\n \nFOLLOWING\n)\n \nAS\n \nres12\n\n\nFROM\n \nInvoice\n \nAS\n \nt0", 
            "title": "Window functions"
        }, 
        {
            "location": "/user-guide/queries/window-functions/#the-withwindow_-function", 
            "text": "When you want to add windows to a query, use the  withWindow_  function to\nintroduce your frames, and compute the projection. You may notice that this is a\ndeparture from SQL syntax, where you can define window expressions inline. Beam\nseeks to be type-safe. Queries with window functions follow slightly different\nrules. Wrapping such a query with a special function allows beam to enforce\nthese rules.  For example, to get each invoice along with the average invoice total by each\ncustomer, use  withWindow_  as follows.  \n                 \n                    \n         \n            \n         \n             Haskell \n         \n    \n         \n             Postgres \n         \n    \n         \n    \n         \n            \n         \n             withWindow_   ( \\ i   -   frame_   ( partitionBy_   ( invoiceCustomer   i ))   noOrder_   noBounds_ ) \n             ( \\ i   w   -   ( i ,   avg_   ( invoiceTotal   i )   ` over_ `   w )) \n             ( all_   ( invoice   chinookDb ))  \n\n         \n    \n         \n             SELECT   t0 . InvoiceId   AS   res0 , \n        t0 . CustomerId   AS   res1 , \n        t0 . InvoiceDate   AS   res2 , \n        t0 . BillingAddress   AS   res3 , \n        t0 . BillingCity   AS   res4 , \n        t0 . BillingState   AS   res5 , \n        t0 . BillingCountry   AS   res6 , \n        t0 . BillingPostalCode   AS   res7 , \n        t0 . Total   AS   res8 , \n        AVG ( t0 . Total )   OVER   ( PARTITION   BY   t0 . CustomerId )   AS   res9  FROM   Invoice   AS   t0  \n\n         \n    \n         \n    \n                 \n                      Or to get each invoice along with the ranking of each invoice by total per\ncustomer  and  the overall ranking,  \n                 \n                    \n         \n            \n         \n             Haskell \n         \n    \n         \n             Postgres \n         \n    \n         \n    \n         \n            \n         \n             withWindow_   ( \\ i   -   (   frame_   noPartition_   ( orderPartitionBy_   ( asc_   ( invoiceTotal   i )))   noBounds_ \n                    ,   frame_   ( partitionBy_   ( invoiceCustomer   i ))   ( orderPartitionBy_   ( asc_   ( invoiceTotal   i )))   noBounds_   )) \n             ( \\ i   ( allInvoices ,   customerInvoices )   -   ( i ,   rank_   ` over_ `   allInvoices ,   rank_   ` over_ `   customerInvoices )) \n             ( all_   ( invoice   chinookDb ))  \n\n         \n    \n         \n             SELECT   t0 . InvoiceId   AS   res0 , \n        t0 . CustomerId   AS   res1 , \n        t0 . InvoiceDate   AS   res2 , \n        t0 . BillingAddress   AS   res3 , \n        t0 . BillingCity   AS   res4 , \n        t0 . BillingState   AS   res5 , \n        t0 . BillingCountry   AS   res6 , \n        t0 . BillingPostalCode   AS   res7 , \n        t0 . Total   AS   res8 , \n        RANK ()   OVER   ( \n                     ORDER   BY   t0 . Total   ASC )   AS   res9 , \n        RANK ()   OVER   ( PARTITION   BY   t0 . CustomerId \n                     ORDER   BY   t0 . Total   ASC )   AS   res10  FROM   Invoice   AS   t0  \n\n         \n    \n         \n    \n                 \n                       Note  rank_  is only available in backends that implement the optional SQL2003\nT611 feature \"Elementary OLAP operations\". Beam syntaxes that implement this\nfunctionality implement the IsSql2003ExpressionElementaryOLAPOperationsSyntax  type class.   Notice that aggregates over the result of the window expression work as you'd\nexpect. Beam automatically generates a subquery once a query has been windowed.\nFor example, to get the sum of the totals of the invoices, by rank.  \n                 \n                    \n         \n            \n         \n             Haskell \n         \n    \n         \n             Postgres \n         \n    \n         \n    \n         \n            \n         \n             orderBy_   ( \\ ( rank ,   _ )   -   asc_   rank )   $  aggregate_   ( \\ ( i ,   rank )   -   ( group_   rank ,   sum_   $   invoiceTotal   i ))   $  withWindow_   ( \\ i   -   frame_   ( partitionBy_   ( invoiceCustomer   i ))   ( orderPartitionBy_   ( asc_   ( invoiceTotal   i )))   noBounds_ ) \n             ( \\ i   w   -   ( i ,   rank_   ` over_ `   w )) \n             ( all_   ( invoice   chinookDb ))  \n\n         \n    \n         \n             SELECT   t0 . res9   AS   res0 , \n        SUM ( t0 . res8 )   AS   res1  FROM \n   ( SELECT   t0 . InvoiceId   AS   res0 , \n           t0 . CustomerId   AS   res1 , \n           t0 . InvoiceDate   AS   res2 , \n           t0 . BillingAddress   AS   res3 , \n           t0 . BillingCity   AS   res4 , \n           t0 . BillingState   AS   res5 , \n           t0 . BillingCountry   AS   res6 , \n           t0 . BillingPostalCode   AS   res7 , \n           t0 . Total   AS   res8 , \n           RANK ()   OVER   ( PARTITION   BY   t0 . CustomerId \n                        ORDER   BY   t0 . Total   ASC )   AS   res9 \n    FROM   Invoice   AS   t0 )   AS   t0  GROUP   BY   t0 . res9  ORDER   BY   t0 . res9   ASC", 
            "title": "The withWindow_ function"
        }, 
        {
            "location": "/user-guide/queries/window-functions/#frame-syntax", 
            "text": "The  frame_  function takes a partition, ordering, and bounds parameter, all of\nwhich are optional. To specify no partition, use  noPartition_ . For no\nordering, use  noOrder_ . For no bounds, use  noBounds_ .  To specify a partition, use  partitionBy_  with an expression or a tuple of\nexpressions. To specify an ordering use  orderPartitionBy_  with an ordering\nexpression or a tuple of ordering expressions. Ordering expressions are scalar\nexpressions passed to either  asc_  or  desc_ . Finally, to specify bounds, use bounds_  or  fromBound_ .  fromBound_  starts the window at the specified\nposition, which can be  unbounded_  (the default) to include all rows seen thus\nfar.  bounds_  lets you specify an optional ending bound, which can be  Nothing \n(the default),  Just unbounded_  (the semantic default, but producing an\nexplicit bound syntactically), or  Just (nrows_ x) , where  x  is an integer\nexpression, specifying the number of rows before or after to include in the\ncalculation.  The following query illustrates some of these features. Along with each invoice, it returns   The average total of all invoices, given by the frame with no partition, ordering, and bounds.  The average total of all invoices, by customer.  The rank of each invoice over all the rows, when ordered by total.  The average of the totals of the invoices starting at the two immediately\n  preceding and ending with the two immediately succeeding invoices, when\n  ordered by date.   \n                 \n                    \n         \n            \n         \n             Haskell \n         \n    \n         \n             Postgres \n         \n    \n         \n    \n         \n            \n         \n             withWindow_   ( \\ i   -   (   frame_   noPartition_   noOrder_   noBounds_ \n                    ,   frame_   ( partitionBy_   ( invoiceCustomer   i ))   noOrder_   noBounds_ \n                    ,   frame_   noPartition_   ( orderPartitionBy_   ( asc_   ( invoiceTotal   i )))   noBounds_ \n                    ,   frame_   noPartition_   ( orderPartitionBy_   ( asc_   ( invoiceDate   i )))   ( bounds_   ( nrows_   2 )   ( Just   ( nrows_   2 ))))) \n             ( \\ i   ( allRows_ ,   sameCustomer_ ,   totals_ ,   fourInvoicesAround_ )   - \n                  (   i \n                  ,   avg_   ( invoiceTotal   i )   ` over_ `   allRows_ \n                  ,   avg_   ( invoiceTotal   i )   ` over_ `   sameCustomer_ \n                  ,   rank_   ` over_ `   totals_ \n                  ,   avg_   ( invoiceTotal   i )   ` over_ `   fourInvoicesAround_   )) \n             ( all_   ( invoice   chinookDb ))  \n\n         \n    \n         \n             SELECT   t0 . InvoiceId   AS   res0 , \n        t0 . CustomerId   AS   res1 , \n        t0 . InvoiceDate   AS   res2 , \n        t0 . BillingAddress   AS   res3 , \n        t0 . BillingCity   AS   res4 , \n        t0 . BillingState   AS   res5 , \n        t0 . BillingCountry   AS   res6 , \n        t0 . BillingPostalCode   AS   res7 , \n        t0 . Total   AS   res8 , \n        AVG ( t0 . Total )   OVER   ()   AS   res9 , \n        AVG ( t0 . Total )   OVER   ( PARTITION   BY   t0 . CustomerId )   AS   res10 , \n        RANK ()   OVER   ( \n                     ORDER   BY   t0 . Total   ASC )   AS   res11 , \n        AVG ( t0 . Total )   OVER   ( \n                                ORDER   BY   t0 . InvoiceDate   ASC   ROWS   BETWEEN   2   PRECEDING   AND   2   FOLLOWING )   AS   res12  FROM   Invoice   AS   t0", 
            "title": "Frame syntax"
        }, 
        {
            "location": "/user-guide/queries/advanced-features/", 
            "text": "This page documents other advanced features that beam supports across backends\nthat support them.\n\n\nSQL2003 T611: Elementary OLAP operations\n\n\nThis optional SQL2003 feature allows attaching arbitrary \nFILTER (WHERE ..)\n\nclauses to aggregates. During querying only rows matching the given expression\nare included in computing the aggregate. This can often be simulated in other\ndatabases by an appropriate \nCASE\n expression, but beam will not do this\ntranslation.\n\n\n\n                \n\n                    \n        \n\n            \n        \n\n            \nHaskell\n\n        \n\n    \n        \n\n            \nPostgres\n\n        \n\n    \n        \n\n    \n        \n\n            \n        \n\n            \naggregate_\n \n(\n\\\ni\n \n-\n \n(\ngroup_\n \n(\ninvoiceCustomer\n \ni\n),\n \nas_\n \n@\nInt\n \n$\n \ncountAll_\n \n`\nfilterWhere_\n`\n \n(\ninvoiceTotal\n \ni\n \n.\n \n500\n),\n \nas_\n \n@\nInt\n \n$\n \ncountAll_\n \n`\nfilterWhere_\n`\n \n(\ninvoiceTotal\n \ni\n \n.\n \n100\n)))\n \n$\n\n\nall_\n \n(\ninvoice\n \nchinookDb\n)\n\n\n\n\n        \n\n    \n        \n\n            \nSELECT\n \nt0\n.\nCustomerId\n \nAS\n \nres0\n,\n\n       \nCOUNT\n(\n*\n)\n \nFILTER\n \n(\n\n                        \nWHERE\n \n(\nt0\n.\nTotal\n)\n \n \n(\n500.0\n))\n \nAS\n \nres1\n,\n\n       \nCOUNT\n(\n*\n)\n \nFILTER\n \n(\n\n                        \nWHERE\n \n(\nt0\n.\nTotal\n)\n \n \n(\n100.0\n))\n \nAS\n \nres2\n\n\nFROM\n \nInvoice\n \nAS\n \nt0\n\n\nGROUP\n \nBY\n \nt0\n.\nCustomerId\n\n\n\n\n        \n\n    \n        \n\n    \n                \n\n                    \n\n\nThese combine as you'd expect with window functions. For example, to return each\ninvoice along with the average total of all invoices by the same customer where\nthe invoice was billed to an address in Los Angeles,\n\n\n\n                \n\n                    \n        \n\n            \n        \n\n            \nHaskell\n\n        \n\n    \n        \n\n            \nPostgres\n\n        \n\n    \n        \n\n    \n        \n\n            \n        \n\n            \nwithWindow_\n \n(\n\\\ni\n \n-\n \nframe_\n \n(\npartitionBy_\n \n(\ninvoiceCustomer\n \ni\n))\n \nnoOrder_\n \nnoBounds_\n)\n\n            \n(\n\\\ni\n \nw\n \n-\n \n(\ni\n,\n \navg_\n \n(\ninvoiceTotal\n \ni\n)\n \n`\nfilterWhere_\n`\n \n(\naddressCity\n \n(\ninvoiceBillingAddress\n \ni\n)\n \n==.\n \njust_\n \nLos Angeles\n)\n \n`\nover_\n`\n \nw\n))\n\n            \n(\nall_\n \n(\ninvoice\n \nchinookDb\n))\n\n\n\n\n        \n\n    \n        \n\n            \nSELECT\n \nt0\n.\nInvoiceId\n \nAS\n \nres0\n,\n\n       \nt0\n.\nCustomerId\n \nAS\n \nres1\n,\n\n       \nt0\n.\nInvoiceDate\n \nAS\n \nres2\n,\n\n       \nt0\n.\nBillingAddress\n \nAS\n \nres3\n,\n\n       \nt0\n.\nBillingCity\n \nAS\n \nres4\n,\n\n       \nt0\n.\nBillingState\n \nAS\n \nres5\n,\n\n       \nt0\n.\nBillingCountry\n \nAS\n \nres6\n,\n\n       \nt0\n.\nBillingPostalCode\n \nAS\n \nres7\n,\n\n       \nt0\n.\nTotal\n \nAS\n \nres8\n,\n\n       \nAVG\n(\nt0\n.\nTotal\n)\n \nFILTER\n \n(\n\n                                 \nWHERE\n \n(\nt0\n.\nBillingCity\n)\n \n=\n \n(\nLos Angeles\n))\n \nOVER\n \n(\nPARTITION\n \nBY\n \nt0\n.\nCustomerId\n)\n \nAS\n \nres9\n\n\nFROM\n \nInvoice\n \nAS\n \nt0\n\n\n\n\n        \n\n    \n        \n\n    \n                \n\n                    \n\n\n\n\nDanger\n\n\nFILTER (WHERE ..)\n must be applied directly to a SQL aggregate function,\nbut this isn't enforced at compile time. This may be fixed in a later\nversion of beam.\n\n\n\n\nThis extension also provides various window functions for SQL. The only one beam\ncurrently implements is \nRANK()\n via the \nrank_\n function. Contributions are\nappreciated!\n\n\nSQL2003 T612: Advanced OLAP operations\n\n\nThis provides both the \nPERCENT_RANK()\n and \nCUME_DIST()\n functions as\n\npercentRank_\n and \ncumeDist_\n respectively.", 
            "title": "Advanced features"
        }, 
        {
            "location": "/user-guide/queries/advanced-features/#sql2003-t611-elementary-olap-operations", 
            "text": "This optional SQL2003 feature allows attaching arbitrary  FILTER (WHERE ..) \nclauses to aggregates. During querying only rows matching the given expression\nare included in computing the aggregate. This can often be simulated in other\ndatabases by an appropriate  CASE  expression, but beam will not do this\ntranslation.  \n                 \n                    \n         \n            \n         \n             Haskell \n         \n    \n         \n             Postgres \n         \n    \n         \n    \n         \n            \n         \n             aggregate_   ( \\ i   -   ( group_   ( invoiceCustomer   i ),   as_   @ Int   $   countAll_   ` filterWhere_ `   ( invoiceTotal   i   .   500 ),   as_   @ Int   $   countAll_   ` filterWhere_ `   ( invoiceTotal   i   .   100 )))   $  all_   ( invoice   chinookDb )  \n\n         \n    \n         \n             SELECT   t0 . CustomerId   AS   res0 , \n        COUNT ( * )   FILTER   ( \n                         WHERE   ( t0 . Total )     ( 500.0 ))   AS   res1 , \n        COUNT ( * )   FILTER   ( \n                         WHERE   ( t0 . Total )     ( 100.0 ))   AS   res2  FROM   Invoice   AS   t0  GROUP   BY   t0 . CustomerId  \n\n         \n    \n         \n    \n                 \n                      These combine as you'd expect with window functions. For example, to return each\ninvoice along with the average total of all invoices by the same customer where\nthe invoice was billed to an address in Los Angeles,  \n                 \n                    \n         \n            \n         \n             Haskell \n         \n    \n         \n             Postgres \n         \n    \n         \n    \n         \n            \n         \n             withWindow_   ( \\ i   -   frame_   ( partitionBy_   ( invoiceCustomer   i ))   noOrder_   noBounds_ ) \n             ( \\ i   w   -   ( i ,   avg_   ( invoiceTotal   i )   ` filterWhere_ `   ( addressCity   ( invoiceBillingAddress   i )   ==.   just_   Los Angeles )   ` over_ `   w )) \n             ( all_   ( invoice   chinookDb ))  \n\n         \n    \n         \n             SELECT   t0 . InvoiceId   AS   res0 , \n        t0 . CustomerId   AS   res1 , \n        t0 . InvoiceDate   AS   res2 , \n        t0 . BillingAddress   AS   res3 , \n        t0 . BillingCity   AS   res4 , \n        t0 . BillingState   AS   res5 , \n        t0 . BillingCountry   AS   res6 , \n        t0 . BillingPostalCode   AS   res7 , \n        t0 . Total   AS   res8 , \n        AVG ( t0 . Total )   FILTER   ( \n                                  WHERE   ( t0 . BillingCity )   =   ( Los Angeles ))   OVER   ( PARTITION   BY   t0 . CustomerId )   AS   res9  FROM   Invoice   AS   t0  \n\n         \n    \n         \n    \n                 \n                       Danger  FILTER (WHERE ..)  must be applied directly to a SQL aggregate function,\nbut this isn't enforced at compile time. This may be fixed in a later\nversion of beam.   This extension also provides various window functions for SQL. The only one beam\ncurrently implements is  RANK()  via the  rank_  function. Contributions are\nappreciated!", 
            "title": "SQL2003 T611: Elementary OLAP operations"
        }, 
        {
            "location": "/user-guide/queries/advanced-features/#sql2003-t612-advanced-olap-operations", 
            "text": "This provides both the  PERCENT_RANK()  and  CUME_DIST()  functions as percentRank_  and  cumeDist_  respectively.", 
            "title": "SQL2003 T612: Advanced OLAP operations"
        }, 
        {
            "location": "/schema-guide/migrations/", 
            "text": "Warning\n\n\nThe \nbeam-migrations\n package is still a WIP. The following manual represents\nboth planned and implemented features.\n\n\n\n\nIn the User Guide we saw how to declare a schema for an already created database\nand use it to perform queries. Beam can also manage a database schema based on\nHaskell datatypes you feed it.\n\n\nThe Beam Migrations Framework is meant to be a robust, modular, and opinionated\nway of managing schema changes. It is an optional part of beam provided in the\n\nbeam-migrate\n package.\n\n\nInstall the migrations framework and tool by running\n\n\n$ cabal install beam-migrate\n\n# or\n\n$ stack install beam-migrate\n\n\n\n\n\nThis installs the \nbeam-migrate\n library as well as a CLI tool (named\n\nbeam-migrate\n as well) which automates common tasks.\n\n\nIf you use \nstack\n make sure you always use \nstack exec -- beam-migrate\n instead\nof the typical \nbeam-migrate\n command in order to have the package path\nautomatically and correctly set for you.\n\n\nBasic concepts\n\n\nIn the user guide, we saw how we can use \ndefaultDbSettings\n to generate default\nmetadata that can be used to access the database. This default metadata is\nenough to query, but not enough for \nbeam-migrate\n. Thus, \nbeam-migrate\n offers\nthe \ndefaultMigratableDbSettings\n function, which annotates the database schema\nwith additional information. Whereas \ndefaultDbSettings\n yields a value of type\n\nDatabaseSettings be db\n, \ndefaultMigratableDbSettings\n yields a value of type\n\nCheckedDatabaseSettings be db\n. You can recover a \nDatabaseSettings be db\n from\na \nCheckedDatabaseSettings be db\n value by applying the \nunCheckDatabase\n\nfunction.\n\n\nThe \nCheckedDatabaseSettings\n value contains the original \nDatabaseSettings\n\nalong with a series of \npredicates\n. Each \npredicate\n describes one aspect of\nthe database. As far as \nbeam-migrate\n is concerned, each database schema is\nfully specified by the set of predicates that apply to it. \nbeam-migrate\n calls\nthis the \nchecked type\n of the database.\n\n\nFor example, a database schema that consists of one table named \ntable\n with no\nfields is represented uniquely by the \nchecked type\n of \n[TableExistsPredicate\n\"table\"]\n. If you add a field \nfield1\n of type \nINT\n to the table, then the\nchecked type becomes \n[TableExistsPredicate \"table\", TableHasColumn \"table\"\n\"field1\" intType]\n.\n\n\n\n\nNote\n\n\nThe types are a bit more complicated than what they appear. In particular, a\npredicate can be of any type that satisfies the \nDatabasePredicate\n type\nclass. The predicates can be stored in a list because they are wrapped in\nthe \nSomeDatabasePredicate\n GADT that holds the type class instance as well.\n\n\n\n\nAutomatic migration generation\n\n\nGiven two \nCheckedDatabaseSettings\n values, \nbeam-migrate\n can generate a set of\nSQL steps that will transform one schema to another. The generation of such\nsteps is an exceedingly difficult problem in general. \nbeam-migrate\n can\nautomatically handle most common cases, but it will not always succeed. In this\ncase, it can present to you a list of steps it thinks are best as well as what\nremains to be solved.\n\n\nThe migration generation is implemented as a proof search in linear logic\n1\n. In\nparticular, \nbeam-migrate\n views a migration as a linear logic proof of the form\n\na \u22b8 b\n, where \na\n is the set of predicates of the original schema and \nb\n is\nthe set of predicates in the target schema. \nbeam-migrate\n ships with a set of\ndefault proof steps. Backends can add to these steps for backend-specific\npredicates.\n\n\n\n\nNote\n\n\nAt this time, Haskell does not allow the expression of linear programs (this\nwill change with the introduction of linear types). Thus, migrations written\nin Haskell are not checked by GHC for linear-ness, but \nbeam-migrate\n will\nvalidate such migrations at run-time to the best of its ability.\n\n\n\n\nThe migration prover may not be able to find a migration in a sufficiently short\nperiod of time. \nbeam-migrate\n's algorithm is designed to terminate, but this\nmay take a while. Additionally, the prover will not automatically generate steps\nfor some migrations. For example, \nbeam-migrate\n will never rename a table\nwithout explicit instructions.\n\n\nFor these cases, the \nbeam-migrate\n command line interface offers an\n\ninteractive\n mode. Here, it presents both database types for the user's\ninspection, as well as a list of steps \nbeam-migrate\n thinks it can take based\non these types. The user can choose to let \nbeam-migrate\n guess the next step,\nor the user can select a step that \nbeam-migrate\n offers as the next step, or\nthe user can enter his/her own step and select which predicates are consumed and\ngenerated.\n\n\nAdvantages of checked migrations\n\n\nUnlike other database migration frameworks, the checking process allows\n\nbeam-migrate\n to be sure that the migration you specify will result in the\ndatabase type you want. Also, checked migrations allow the programmer to verify\nthat the database they are accessing indeed matches what their schema expects.\n\n\nUsage modes\n\n\nbeam-migrate\n can be used as a library or a command-line tool in \nmanaged\n or\n\nunmanaged\n mode.\n\n\nThe \nbeam-migrate\n library\n\n\nThe \nbeam-migrate\n library provides syntax definitions for common SQL DDL tasks.\nIt also provides types for expressing migrations as transformations of one or\nmore schemas to another. The library exports types for database backends to hook\ninto automated migration tools (including the \nbeam-migrate\n tool). Finally, it\nalso implements the migration solver. Its use is described in the next section.\n\n\nThe \nbeam-migrate\n tool\n\n\nIn \nunmanaged\n mode, the \nbeam-migrate\n tool offers a set of convenience\nfunctionality for generating migrations, checking databases, etc based off of a\nschema. It is useful for performing one-off tasks (such as generating a\nmigration script) that you don't want to recompile your project for.\n\n\nManaged\n mode extends the functionality of \nunmanaged\n mode with tools for\nmanaging sets of schemas and migrations between them. In this mode,\n\nbeam-migrate\n can be used to update a production or development database\nschema. This mode supports schema branching and version-control integration but\nforces you to adopt \nbeam-migrate\n's conventions.\n\n\n\n\nTip\n\n\nIf you're worried about a \nbeam-migrate\n dependency in a production\napplication, \nbeam-migrate\n can be used to freeze a particular checked\ndatabase settings object. This means you get a \nDatabaseSettings\n value\nwhich can be used in an application with only a \nbeam-core\n dependency.\n\n\n\n\n\n\n\n\n\n\n\n\nLinear logic is a type of logic first described by Jean-Yves Gerard. In\nparticular, it constrains the weakening and strengthening rules of classical\nlogic. Intuitively, you can think of it as forcing the rule that each\nassumption is used exactly once to produce the result. Read\nmore \non Wikipedia\n.", 
            "title": "The Migrations Framework"
        }, 
        {
            "location": "/schema-guide/migrations/#basic-concepts", 
            "text": "In the user guide, we saw how we can use  defaultDbSettings  to generate default\nmetadata that can be used to access the database. This default metadata is\nenough to query, but not enough for  beam-migrate . Thus,  beam-migrate  offers\nthe  defaultMigratableDbSettings  function, which annotates the database schema\nwith additional information. Whereas  defaultDbSettings  yields a value of type DatabaseSettings be db ,  defaultMigratableDbSettings  yields a value of type CheckedDatabaseSettings be db . You can recover a  DatabaseSettings be db  from\na  CheckedDatabaseSettings be db  value by applying the  unCheckDatabase \nfunction.  The  CheckedDatabaseSettings  value contains the original  DatabaseSettings \nalong with a series of  predicates . Each  predicate  describes one aspect of\nthe database. As far as  beam-migrate  is concerned, each database schema is\nfully specified by the set of predicates that apply to it.  beam-migrate  calls\nthis the  checked type  of the database.  For example, a database schema that consists of one table named  table  with no\nfields is represented uniquely by the  checked type  of  [TableExistsPredicate\n\"table\"] . If you add a field  field1  of type  INT  to the table, then the\nchecked type becomes  [TableExistsPredicate \"table\", TableHasColumn \"table\"\n\"field1\" intType] .   Note  The types are a bit more complicated than what they appear. In particular, a\npredicate can be of any type that satisfies the  DatabasePredicate  type\nclass. The predicates can be stored in a list because they are wrapped in\nthe  SomeDatabasePredicate  GADT that holds the type class instance as well.", 
            "title": "Basic concepts"
        }, 
        {
            "location": "/schema-guide/migrations/#automatic-migration-generation", 
            "text": "Given two  CheckedDatabaseSettings  values,  beam-migrate  can generate a set of\nSQL steps that will transform one schema to another. The generation of such\nsteps is an exceedingly difficult problem in general.  beam-migrate  can\nautomatically handle most common cases, but it will not always succeed. In this\ncase, it can present to you a list of steps it thinks are best as well as what\nremains to be solved.  The migration generation is implemented as a proof search in linear logic 1 . In\nparticular,  beam-migrate  views a migration as a linear logic proof of the form a \u22b8 b , where  a  is the set of predicates of the original schema and  b  is\nthe set of predicates in the target schema.  beam-migrate  ships with a set of\ndefault proof steps. Backends can add to these steps for backend-specific\npredicates.   Note  At this time, Haskell does not allow the expression of linear programs (this\nwill change with the introduction of linear types). Thus, migrations written\nin Haskell are not checked by GHC for linear-ness, but  beam-migrate  will\nvalidate such migrations at run-time to the best of its ability.   The migration prover may not be able to find a migration in a sufficiently short\nperiod of time.  beam-migrate 's algorithm is designed to terminate, but this\nmay take a while. Additionally, the prover will not automatically generate steps\nfor some migrations. For example,  beam-migrate  will never rename a table\nwithout explicit instructions.  For these cases, the  beam-migrate  command line interface offers an interactive  mode. Here, it presents both database types for the user's\ninspection, as well as a list of steps  beam-migrate  thinks it can take based\non these types. The user can choose to let  beam-migrate  guess the next step,\nor the user can select a step that  beam-migrate  offers as the next step, or\nthe user can enter his/her own step and select which predicates are consumed and\ngenerated.", 
            "title": "Automatic migration generation"
        }, 
        {
            "location": "/schema-guide/migrations/#advantages-of-checked-migrations", 
            "text": "Unlike other database migration frameworks, the checking process allows beam-migrate  to be sure that the migration you specify will result in the\ndatabase type you want. Also, checked migrations allow the programmer to verify\nthat the database they are accessing indeed matches what their schema expects.", 
            "title": "Advantages of checked migrations"
        }, 
        {
            "location": "/schema-guide/migrations/#usage-modes", 
            "text": "beam-migrate  can be used as a library or a command-line tool in  managed  or unmanaged  mode.", 
            "title": "Usage modes"
        }, 
        {
            "location": "/schema-guide/migrations/#the-beam-migrate-library", 
            "text": "The  beam-migrate  library provides syntax definitions for common SQL DDL tasks.\nIt also provides types for expressing migrations as transformations of one or\nmore schemas to another. The library exports types for database backends to hook\ninto automated migration tools (including the  beam-migrate  tool). Finally, it\nalso implements the migration solver. Its use is described in the next section.", 
            "title": "The beam-migrate library"
        }, 
        {
            "location": "/schema-guide/migrations/#the-beam-migrate-tool", 
            "text": "In  unmanaged  mode, the  beam-migrate  tool offers a set of convenience\nfunctionality for generating migrations, checking databases, etc based off of a\nschema. It is useful for performing one-off tasks (such as generating a\nmigration script) that you don't want to recompile your project for.  Managed  mode extends the functionality of  unmanaged  mode with tools for\nmanaging sets of schemas and migrations between them. In this mode, beam-migrate  can be used to update a production or development database\nschema. This mode supports schema branching and version-control integration but\nforces you to adopt  beam-migrate 's conventions.   Tip  If you're worried about a  beam-migrate  dependency in a production\napplication,  beam-migrate  can be used to freeze a particular checked\ndatabase settings object. This means you get a  DatabaseSettings  value\nwhich can be used in an application with only a  beam-core  dependency.       Linear logic is a type of logic first described by Jean-Yves Gerard. In\nparticular, it constrains the weakening and strengthening rules of classical\nlogic. Intuitively, you can think of it as forcing the rule that each\nassumption is used exactly once to produce the result. Read\nmore  on Wikipedia .", 
            "title": "The beam-migrate tool"
        }, 
        {
            "location": "/schema-guide/tool/", 
            "text": "Tool", 
            "title": "The beam-migrate tool"
        }, 
        {
            "location": "/schema-guide/supported/", 
            "text": "supported", 
            "title": "Supported migrations"
        }, 
        {
            "location": "/user-guide/backends/beam-postgres/", 
            "text": "The \nbeam-postgres\n backend is the most feature complete SQL backend for beam.\nThe Postgres RDBMS supports most of the standards beam follows, so you can\nusually expect most queries to simply work. Additionally, \nbeam-postgres\n is\npart of the standard Beam distribution, and so upgrades are applied\nperiodically, and new functions are added to achieve feature-parity with the\nlatest Postgres stable\n\n\nPostgres-specific data types\n\n\nPostgres has several data types not available from \nbeam-core\n. The\n\nbeam-postgres\n library provides several types and functions to make working\nwith these easier.\n\n\nThe \ntsvector\n and \ntsquery\n types\n\n\nThe \ntsvector\n and \ntsquery\n types form the basis of full-text search in\nPostgres.", 
            "title": "beam-postgres"
        }, 
        {
            "location": "/user-guide/backends/beam-postgres/#postgres-specific-data-types", 
            "text": "Postgres has several data types not available from  beam-core . The beam-postgres  library provides several types and functions to make working\nwith these easier.", 
            "title": "Postgres-specific data types"
        }, 
        {
            "location": "/user-guide/backends/beam-postgres/#the-tsvector-and-tsquery-types", 
            "text": "The  tsvector  and  tsquery  types form the basis of full-text search in\nPostgres.", 
            "title": "The tsvector and tsquery types"
        }, 
        {
            "location": "/user-guide/backends/beam-sqlite/", 
            "text": "SQLite is a lightweight RDBMS meant for embedding in larger applications.\nBecause it is not designed to be full-featured, not all Beam queries will work\nwith SQLite. The module \nDatabase.Beam.SQLite.Checked\n provides many symbols\nusually imported from the \nDatabase.Beam\n module that enforce extra checks on\nqueries to assure compliance with SQLite. Use this module in code that is SQLite\nspecific for maximal compile-time safety. Note that this module should be\nimported instead of \nDatabase.Beam\n to avoid name clashes.\n\n\nCompatibility\n\n\nSQLite is compatible enough with Beam's query syntax, that adapting to its\nquirks is pretty straightforwards. The main special case for SQLite is its\nhandling of nested set operations. On most backends, beam can output these\ndirectly, but SQLite requires us to generate subqueries.", 
            "title": "beam-sqlite"
        }, 
        {
            "location": "/user-guide/backends/beam-sqlite/#compatibility", 
            "text": "SQLite is compatible enough with Beam's query syntax, that adapting to its\nquirks is pretty straightforwards. The main special case for SQLite is its\nhandling of nested set operations. On most backends, beam can output these\ndirectly, but SQLite requires us to generate subqueries.", 
            "title": "Compatibility"
        }, 
        {
            "location": "/user-guide/custom-backends/", 
            "text": "Writing a custom backend", 
            "title": "Writing a Custom Backend"
        }, 
        {
            "location": "/reference/models/", 
            "text": "A beam model is any single-constructer Haskell record type parameterized by a\ntype of kind \n* -\n *\n. The model must have an instance of \nGeneric\n, \nBeamable\n,\nand \nTable\n. \nGeneric\n can be derived using the \nDeriveGeneric\n extension of\nGHC. \nBeamable\n must be given an empty instance declaration (\ninstance Beamable\nTbl\n for a table of type \nTbl\n). \nTable\n is discussed next.\n\n\nEach field in the record type must either be a sub-table (another parameterized\ntype with a \nBeamable\n instance) or an explicit column. A column is specified\nusing the \nColumnar\n type family applied to the type's parameter and the\nunderlying Haskell type of the field.\n\n\nThe \nTable\n type class\n\n\nTable\n is a type class that must be instantiated for all types that you would\nlike to use as a table. It has one associated \ndata\n instance and one function.\n\n\nYou must create a type to represent the primary key of the table. The primary\nkey of a table \nTbl\n is the associated data type \nPrimaryKey Tbl\n. Like \nTbl\n,\nit takes one type parameter of kind \n* -\n *\n. It must have only one constructor\nwhich can hold all fields in the primary key. The constructor need not be a\nrecord constructor (although it can be).\n\n\nYou must also write a function \nprimaryKey\n that takes an instance of \nTbl\n\n(parameterized over any functor \nf\n) and returns the associated \nPrimaryKey\n\ntype. It is sometimes easiest to use the \nApplicative\n instance for \nr -\n to\nwrite this function. For example, if \ntblField1\n and \ntblField2\n are part of the\nprimary key, you can write\n\n\ninstance\n \nTable\n \nTbl\n \nwhere\n\n  \ndata\n \nPrimaryKey\n \nTbl\n \nf\n \n=\n \nTblKey\n \n(\nColumnar\n \nf\n \n..\n)\n \n(\nColumnar\n \nf\n \n..\n)\n\n  \nprimaryKey\n \nt\n \n=\n \nTblKey\n \n(\ntblField1\n \nt\n)\n \n(\ntblField2\n \nt\n)\n\n\n\n\n\n\nmore simply as\n\n\ninstance\n \nTable\n \nTbl\n \nwhere\n\n  \ndata\n \nPrimaryKey\n \nTbl\n \nf\n \n=\n \nTblKey\n \n(\nColumnar\n \nf\n \n..\n)\n \n(\nColumnar\n \nf\n \n..\n)\n\n  \nprimaryKey\n \n=\n \nTblKey\n \n$\n \ntblField1\n \n*\n \ntblField2\n\n\n\n\n\n\nThe \nIdentity\n trick\n\n\nBeam table types are commonly prefixed by a \nT\n to indicate the name of the\ngeneric table type. Usually, a type synonym named by leaving out the \nT\n is\ndefined by applying the table to \nIdentity\n. Recall each field in the table is\neither another table or an application of \nColumnar\n to the type parameter. When\nthe type is parameterized by \nIdentity\n, every column is also parameterized by\n\nIdentity\n.\n\n\nColumnar\n is a type family defined such that \nColumnar Identity x ~ x\n. Thus,\nwhen parameterized over \nIdentity\n, every field in the table type takes on the\nunderlying Haskell type.\n\n\nSuppose you have a table type \nModelT\n and a type synonym \ntype Model = ModelT\nIdentity\n. Notice that deriving \nShow\n, \nEq\n, and other standard Haskell type\nclasses won't generally work for \nModelT\n. However, you can use the standalone\nderiving mechanism to derive these instances for \nModel\n.\n\n\ndata\n \nModelT\n \nf\n \n=\n \nModel\n \n{\n \n..\n \n}\n \nderiving\n \nGeneric\n\n\ninstance\n \nBeamable\n \nModelT\n\n\n\n-- deriving instance Show (ModelT f) -- Won\nt work because GHC won\nt get the constraints right\n\n\n\ntype\n \nModel\n \n=\n \nModelT\n \nIdentity\n\n\nderiving\n \ninstance\n \nShow\n \nModel\n\n\nderiving\n \ninstance\n \nEq\n \nModel\n\n\nderiving\n \ninstance\n \nOrd\n \nModel\n\n\n\n\n\n\nAllowed data types\n\n\nAny data type can be used within a \nColumnar\n. Beam does no checking that a\nfield can be used against a particular database when the data type is defined.\nInstead, type errors will occur when the table is being used as a query. For\nexample, the following is allowed, even though many backends will not work with\narray data types.\n\n\nimport\n \nqualified\n \nData.Vector\n \nas\n \nV\n\n\n\ndata\n \nArrayTable\n \nf\n\n    \n=\n \nArrayTable\n\n    \n{\n \narrayTablePoints\n \n::\n \nColumnar\n \nf\n \n(\nV\n.\nVector\n \nInt32\n)\n\n    \n}\n \nderiving\n \nGeneric\n\n\n\n\n\n\nYou can construct values of type \nArrayTable Identity\n and even write queries\nover it (relying on type inference to get the constraints right). However, if\nyou attempt to solve the constraints over a database that doesn't support\ncolumns of type \nV.Vector Int32\n, GHC will throw an error. Thus, it's important\nto understand the limits of your backend when deciding which types to use. In\ngeneral, numeric, floating-point, and text types are well supported.\n\n\nMaybe\n types\n\n\nOptional fields (those that allow a SQL \nNULL\n) can usually be given a \nMaybe\n\ntype. However, you cannot use \nMaybe\n around an embedded table (you will be\nunable to instantiate \nBeamable\n).\n\n\nBeam offers a way around this. Instead of embedding the table applied to the\ntype parameter \nf\n, apply it to \nNullable f\n. \nColumnar (Nullable f) a ~ Maybe\n(Columnar f a)\n for all \na\n. Thus, this will make every column in the embedded\ntable take on the corresponding \nMaybe\n type.\n\n\n\n\nWarning\n\n\nNullable\n will nest \nMaybe\ns. That is \nColumnar (Nullable f) (Maybe a) ~\nMaybe (Maybe a)\n. This is bad from a SQL perspective, since SQL has no\nconcept of a nested optional type. Beam treats a \nNothing\n at any 'layer' of\nthe \nMaybe\n stack as a corresponding SQL \nNULL\n. When marshalling data back,\na SQL \nNULL\n is read in as a top-level \nNothing\n.\n\n\nThe reasons for this misfeature is basically code simplicity. Fixing this is\na top priority of future versions of beam.\n\n\n\n\nColumn tags\n\n\nAbove, we saw that applying \nIdentity\n to a table type results in a type whose\ncolumns are the underlying Haskell type. Beam uses other column tags for\nquerying and describing databases. Below is a table of common column tags and\ntheir meaning.\n\n\nConverting between tags\n\n\nSuppose you have a \nBeamable\n type paramaterized over a tag \nf\n and needed one\nparameterized over a tag \ng\n. Given a function \nconv :: forall a. Columnar f a\n-\n Columnar g a\n, you can use \nchangeBeamRep\n to convert between the tables.\n\n\nThere is one caveat however -- since \nColumnar\n is a type family, the type of\n\nconv\n is actually ambiguous. We need a way to carry the type of \nf\n, \ng\n, and\n\na\n into the code. For this reason, \nconv\n must actually be written over the\n\nColumnar'\n(notice the tick) \nnewtype\n. \nColumnar'\n is a newtype defined as such\n\n\nnewtype\n \nColumnar\n \nf\n \na\n \n=\n \nColumnar\n \n(\nColumnar\n \nf\n \na\n)\n\n\n\n\n\n\nNotice that, unlinke \nColumnar\n (a non-injective type family), \nColumnar'\n is a\nfull type. The type of \nconv' :: forall a. Columnar' f a -\n Columnar' g a\n is\nnow unambiguous. You can easily use \nconv\n to implement \nconv'\n:\n\n\nconv\n \n(\nColumnar\n \na\n)\n \n=\n \nColumnar\n \n(\nconv\n \na\n)\n\n\n\n\n\n\nYou will often need to write explicit type signatures in order to get the\ncompiler to accept your code.", 
            "title": "Models"
        }, 
        {
            "location": "/reference/models/#the-table-type-class", 
            "text": "Table  is a type class that must be instantiated for all types that you would\nlike to use as a table. It has one associated  data  instance and one function.  You must create a type to represent the primary key of the table. The primary\nkey of a table  Tbl  is the associated data type  PrimaryKey Tbl . Like  Tbl ,\nit takes one type parameter of kind  * -  * . It must have only one constructor\nwhich can hold all fields in the primary key. The constructor need not be a\nrecord constructor (although it can be).  You must also write a function  primaryKey  that takes an instance of  Tbl \n(parameterized over any functor  f ) and returns the associated  PrimaryKey \ntype. It is sometimes easiest to use the  Applicative  instance for  r -  to\nwrite this function. For example, if  tblField1  and  tblField2  are part of the\nprimary key, you can write  instance   Table   Tbl   where \n   data   PrimaryKey   Tbl   f   =   TblKey   ( Columnar   f   .. )   ( Columnar   f   .. ) \n   primaryKey   t   =   TblKey   ( tblField1   t )   ( tblField2   t )   more simply as  instance   Table   Tbl   where \n   data   PrimaryKey   Tbl   f   =   TblKey   ( Columnar   f   .. )   ( Columnar   f   .. ) \n   primaryKey   =   TblKey   $   tblField1   *   tblField2", 
            "title": "The Table type class"
        }, 
        {
            "location": "/reference/models/#the-identity-trick", 
            "text": "Beam table types are commonly prefixed by a  T  to indicate the name of the\ngeneric table type. Usually, a type synonym named by leaving out the  T  is\ndefined by applying the table to  Identity . Recall each field in the table is\neither another table or an application of  Columnar  to the type parameter. When\nthe type is parameterized by  Identity , every column is also parameterized by Identity .  Columnar  is a type family defined such that  Columnar Identity x ~ x . Thus,\nwhen parameterized over  Identity , every field in the table type takes on the\nunderlying Haskell type.  Suppose you have a table type  ModelT  and a type synonym  type Model = ModelT\nIdentity . Notice that deriving  Show ,  Eq , and other standard Haskell type\nclasses won't generally work for  ModelT . However, you can use the standalone\nderiving mechanism to derive these instances for  Model .  data   ModelT   f   =   Model   {   ..   }   deriving   Generic  instance   Beamable   ModelT  -- deriving instance Show (ModelT f) -- Won t work because GHC won t get the constraints right  type   Model   =   ModelT   Identity  deriving   instance   Show   Model  deriving   instance   Eq   Model  deriving   instance   Ord   Model", 
            "title": "The Identity trick"
        }, 
        {
            "location": "/reference/models/#allowed-data-types", 
            "text": "Any data type can be used within a  Columnar . Beam does no checking that a\nfield can be used against a particular database when the data type is defined.\nInstead, type errors will occur when the table is being used as a query. For\nexample, the following is allowed, even though many backends will not work with\narray data types.  import   qualified   Data.Vector   as   V  data   ArrayTable   f \n     =   ArrayTable \n     {   arrayTablePoints   ::   Columnar   f   ( V . Vector   Int32 ) \n     }   deriving   Generic   You can construct values of type  ArrayTable Identity  and even write queries\nover it (relying on type inference to get the constraints right). However, if\nyou attempt to solve the constraints over a database that doesn't support\ncolumns of type  V.Vector Int32 , GHC will throw an error. Thus, it's important\nto understand the limits of your backend when deciding which types to use. In\ngeneral, numeric, floating-point, and text types are well supported.", 
            "title": "Allowed data types"
        }, 
        {
            "location": "/reference/models/#maybe-types", 
            "text": "Optional fields (those that allow a SQL  NULL ) can usually be given a  Maybe \ntype. However, you cannot use  Maybe  around an embedded table (you will be\nunable to instantiate  Beamable ).  Beam offers a way around this. Instead of embedding the table applied to the\ntype parameter  f , apply it to  Nullable f .  Columnar (Nullable f) a ~ Maybe\n(Columnar f a)  for all  a . Thus, this will make every column in the embedded\ntable take on the corresponding  Maybe  type.   Warning  Nullable  will nest  Maybe s. That is  Columnar (Nullable f) (Maybe a) ~\nMaybe (Maybe a) . This is bad from a SQL perspective, since SQL has no\nconcept of a nested optional type. Beam treats a  Nothing  at any 'layer' of\nthe  Maybe  stack as a corresponding SQL  NULL . When marshalling data back,\na SQL  NULL  is read in as a top-level  Nothing .  The reasons for this misfeature is basically code simplicity. Fixing this is\na top priority of future versions of beam.", 
            "title": "Maybe types"
        }, 
        {
            "location": "/reference/models/#column-tags", 
            "text": "Above, we saw that applying  Identity  to a table type results in a type whose\ncolumns are the underlying Haskell type. Beam uses other column tags for\nquerying and describing databases. Below is a table of common column tags and\ntheir meaning.", 
            "title": "Column tags"
        }, 
        {
            "location": "/reference/models/#converting-between-tags", 
            "text": "Suppose you have a  Beamable  type paramaterized over a tag  f  and needed one\nparameterized over a tag  g . Given a function  conv :: forall a. Columnar f a\n-  Columnar g a , you can use  changeBeamRep  to convert between the tables.  There is one caveat however -- since  Columnar  is a type family, the type of conv  is actually ambiguous. We need a way to carry the type of  f ,  g , and a  into the code. For this reason,  conv  must actually be written over the Columnar' (notice the tick)  newtype .  Columnar'  is a newtype defined as such  newtype   Columnar   f   a   =   Columnar   ( Columnar   f   a )   Notice that, unlinke  Columnar  (a non-injective type family),  Columnar'  is a\nfull type. The type of  conv' :: forall a. Columnar' f a -  Columnar' g a  is\nnow unambiguous. You can easily use  conv  to implement  conv' :  conv   ( Columnar   a )   =   Columnar   ( conv   a )   You will often need to write explicit type signatures in order to get the\ncompiler to accept your code.", 
            "title": "Converting between tags"
        }, 
        {
            "location": "/reference/expression/", 
            "text": "Typing\n\n\nThe type of all SQL-level expressions is \nQGenExpr\n. See the \nquery tutorial\n for more information.\n\n\nIn many cases, you'd like to type the SQL-level result of an expression without\nhaving to give explicit types for the other \nQGenExpr\n parameters. You can do\nthis with the \nas_\n combinator and \n-XTypeApplications\n.\n\n\nThe following code types the literal 1 as a \nDouble\n.\n\n\nas_\n \n@\nDouble\n \n1\n\n\n\n\n\n\nThis is rarely needed, but there are a few cases where the beam types are too\ngeneral for the compiler to meaningfully infer types.\n\n\nLiterals\n\n\n\n\nInteger literals\n can be constructed using \nfromIntegral\n in the \nNum\n\n  typeclass. This means you can also just use a Haskell integer literal as a\n  \nQGenExpr\n in any context.\n\n\nRational literals\n can be constructed via \nfromRational\n in \nRational\n.\n  Regular Haskell rational literals will be automatically converted to\n  \nQGenExprs\n.\n\n\nText literals\n can be constructed via \nfromString\n in \nIsString\n. Again,\n  Haskell string constants will automaticall be converted to \nQGenExprs\n,\n  although you may have to provide an explicit type, as different backends\n  support different text types natively.\n\n\nAll other literals\n can be constructed using the \nval_\n function in\n  \nSqlValable\n. This requires that there is an implementation of\n  \nHasSqlValueSyntax (Sql92ExpressionValueSyntax syntax) x\n for the type \nx\n in\n  the appropriate \nsyntax\n for the \nQGenExpr\n. For example, to construct a value\n  of type \nVector Int\n in the \nbeam-postgres\n backend.\n\n\n\n\nval_\n \n(\nV\n.\nfromList\n \n[\n1\n,\n \n2\n,\n \n3\n \n::\n \nInt\n])\n\n\n\n\n\n\n\n\nExplicit tables\n can be brought to the SQL value level by using \nval_\n as\n  well. For example, if you have an \nAddressT Identity\n named \na\n, \nval_ a ::\n  AddressT (QGenExpr context expr s)\n.\n\n\n\n\nArithmetic\n\n\nArithmetic operations that are part of the \nFractional\n and \nNum\n classes can be\nused directly. For example, if \na\n and \nb\n are \nQGenExpr\ns of the same type,\nthen \na + b\n is a \nQGenExpr\n of the same type.\n\n\nBecause of the \ntoInteger\n class method in \nIntegral\n, \nQGenExpr\ns cannot\nimplement \nIntegral\n. Nevertheless, versions of \ndiv\n and \nmod\n are available as\n\ndiv_\n and \nmod_\n, respectively, having the corresponding type.\n\n\nCASE .. WHEN .. ELSE ..\n statements\n\n\nThe SQL \nCASE .. WHEN .. ELSE\n construct can be used to implement a multi-way\nif. The corresponding beam syntax is\n\n\nif_\n \n[\n \ncond1\n \n`\nthen_\n`\n \nresult1\n,\n \ncond2\n \n`\nthen_\n`\n \nresult2\n,\n \n...\n \n]\n \n(\nelse_\n \nelseResult\n)\n\n\n\n\n\n\nwhere \ncond\nn\n are \nQGenExpr\n of type \nBool\n, and \nresult1\n, \nresult2\n, and\n\nelseResult\n are \nQGenExprs\n of the same type.\n\n\nSQL Functions and operators\n\n\n\n\n\n\n\n\nSQL construct\n\n\nSQL standard\n\n\nBeam equivalent\n\n\nNotes\n\n\n\n\n\n\n\n\n\n\nEXISTS (x)\n\n\nSQL92\n\n\nexists_ x\n\n\nHere, \nx\n is any query (of type \nQ\n)\n\n\n\n\n\n\nUNIQUE (x)\n\n\nSQL92\n\n\nunique_ x\n\n\nSee note for \nEXISTS (x)\n\n\n\n\n\n\nDISTINCT (x)\n\n\nSQL99\n\n\ndistinct_ x\n\n\nSee note for \nEXISTS (x)\n\n\n\n\n\n\nSELECT .. FROM ...\n \n as an expression (subqueries)\n\n\nSQL92\n\n\nsubquery_ x\n\n\nx\n is an query (of type \nQ\n)\n\n\n\n\n\n\nCOALESCE(a, b, c, ...)\n\n\nSQL92\n\n\ncoalesce_ [a, b, c, ...]\n\n\na\n, \nb\n, and \nc\n must be of \ntype \nMaybe a\n.\nThe result has type \na\n\n\n\n\n\n\na BETWEEN b AND c\n\n\nSQL92\n\n\nbetween_ a b c\n\n\n\n\n\n\n\n\na LIKE b\n\n\nSQL92\n\n\na `like_` b\n\n\na\n and \nb\n should be string types\n\n\n\n\n\n\na SIMILAR TO b\n\n\nSQL99\n\n\na `similarTo_` b\n\n\nSee note for \nLIKE\n\n\n\n\n\n\nPOSITION(x IN y)\n\n\nSQL92\n\n\nposition_ x y\n\n\nx\n and \ny\n should be string types\n\n\n\n\n\n\nCHAR_LENGTH(x)\n\n\nSQL92\n\n\ncharLength_ x\n\n\n\n\n\n\n\n\nOCTET_LENGTH(x)\n\n\nSQL92\n\n\noctetLength_ x\n\n\n\n\n\n\n\n\nBIT_LENGTH(x)\n\n\nSQL92\n\n\nbitLength_ x\n\n\nx\n must be of the beam-specific \nSqlBitString\n type\n\n\n\n\n\n\nx IS TRUE\n / \nx IS NOT TRUE\n\n\nSQL92\n\n\nisTrue_ x\n / \nisNotTrue_ x\n\n\n\n\n\n\n\n\nx IS FALSE\n / \nx IS NOT FALSE\n\n\nSQL92\n\n\nisFalse_ x\n / \nisNotFalse_ x\n\n\n\n\n\n\n\n\nx IS UNKNOWN\n / \nx IS NOT UNKNOWN\n\n\nSQL92\n\n\nisUnknown_ x\n / \nisNotUnknown_ x\n\n\n\n\n\n\n\n\nNOT x\n\n\nSQL92\n\n\nnot_ x\n\n\n\n\n\n\n\n\n\n\nMy favorite operator / function isn't listed here!\n\n\nIf your favorite operator or function is not provided here, first ask yourself\nif it is part of any SQL standard. If it is not, then check the backend you are\nusing to see if it provides a corresponding construct. If the backend does not\nor if the function / operator you need is part of a SQL standard, please open an\nissue on GitHub. Alternatively, implement the construct yourself and send us a\npull request! See the section on \nadding your own functions", 
            "title": "Expressions"
        }, 
        {
            "location": "/reference/expression/#typing", 
            "text": "The type of all SQL-level expressions is  QGenExpr . See the  query tutorial  for more information.  In many cases, you'd like to type the SQL-level result of an expression without\nhaving to give explicit types for the other  QGenExpr  parameters. You can do\nthis with the  as_  combinator and  -XTypeApplications .  The following code types the literal 1 as a  Double .  as_   @ Double   1   This is rarely needed, but there are a few cases where the beam types are too\ngeneral for the compiler to meaningfully infer types.", 
            "title": "Typing"
        }, 
        {
            "location": "/reference/expression/#literals", 
            "text": "Integer literals  can be constructed using  fromIntegral  in the  Num \n  typeclass. This means you can also just use a Haskell integer literal as a\n   QGenExpr  in any context.  Rational literals  can be constructed via  fromRational  in  Rational .\n  Regular Haskell rational literals will be automatically converted to\n   QGenExprs .  Text literals  can be constructed via  fromString  in  IsString . Again,\n  Haskell string constants will automaticall be converted to  QGenExprs ,\n  although you may have to provide an explicit type, as different backends\n  support different text types natively.  All other literals  can be constructed using the  val_  function in\n   SqlValable . This requires that there is an implementation of\n   HasSqlValueSyntax (Sql92ExpressionValueSyntax syntax) x  for the type  x  in\n  the appropriate  syntax  for the  QGenExpr . For example, to construct a value\n  of type  Vector Int  in the  beam-postgres  backend.   val_   ( V . fromList   [ 1 ,   2 ,   3   ::   Int ])    Explicit tables  can be brought to the SQL value level by using  val_  as\n  well. For example, if you have an  AddressT Identity  named  a ,  val_ a ::\n  AddressT (QGenExpr context expr s) .", 
            "title": "Literals"
        }, 
        {
            "location": "/reference/expression/#arithmetic", 
            "text": "Arithmetic operations that are part of the  Fractional  and  Num  classes can be\nused directly. For example, if  a  and  b  are  QGenExpr s of the same type,\nthen  a + b  is a  QGenExpr  of the same type.  Because of the  toInteger  class method in  Integral ,  QGenExpr s cannot\nimplement  Integral . Nevertheless, versions of  div  and  mod  are available as div_  and  mod_ , respectively, having the corresponding type.", 
            "title": "Arithmetic"
        }, 
        {
            "location": "/reference/expression/#case-when-else-statements", 
            "text": "The SQL  CASE .. WHEN .. ELSE  construct can be used to implement a multi-way\nif. The corresponding beam syntax is  if_   [   cond1   ` then_ `   result1 ,   cond2   ` then_ `   result2 ,   ...   ]   ( else_   elseResult )   where  cond n  are  QGenExpr  of type  Bool , and  result1 ,  result2 , and elseResult  are  QGenExprs  of the same type.", 
            "title": "CASE .. WHEN .. ELSE .. statements"
        }, 
        {
            "location": "/reference/expression/#sql-functions-and-operators", 
            "text": "SQL construct  SQL standard  Beam equivalent  Notes      EXISTS (x)  SQL92  exists_ x  Here,  x  is any query (of type  Q )    UNIQUE (x)  SQL92  unique_ x  See note for  EXISTS (x)    DISTINCT (x)  SQL99  distinct_ x  See note for  EXISTS (x)    SELECT .. FROM ...    as an expression (subqueries)  SQL92  subquery_ x  x  is an query (of type  Q )    COALESCE(a, b, c, ...)  SQL92  coalesce_ [a, b, c, ...]  a ,  b , and  c  must be of  type  Maybe a . The result has type  a    a BETWEEN b AND c  SQL92  between_ a b c     a LIKE b  SQL92  a `like_` b  a  and  b  should be string types    a SIMILAR TO b  SQL99  a `similarTo_` b  See note for  LIKE    POSITION(x IN y)  SQL92  position_ x y  x  and  y  should be string types    CHAR_LENGTH(x)  SQL92  charLength_ x     OCTET_LENGTH(x)  SQL92  octetLength_ x     BIT_LENGTH(x)  SQL92  bitLength_ x  x  must be of the beam-specific  SqlBitString  type    x IS TRUE  /  x IS NOT TRUE  SQL92  isTrue_ x  /  isNotTrue_ x     x IS FALSE  /  x IS NOT FALSE  SQL92  isFalse_ x  /  isNotFalse_ x     x IS UNKNOWN  /  x IS NOT UNKNOWN  SQL92  isUnknown_ x  /  isNotUnknown_ x     NOT x  SQL92  not_ x", 
            "title": "SQL Functions and operators"
        }, 
        {
            "location": "/reference/expression/#my-favorite-operator-function-isnt-listed-here", 
            "text": "If your favorite operator or function is not provided here, first ask yourself\nif it is part of any SQL standard. If it is not, then check the backend you are\nusing to see if it provides a corresponding construct. If the backend does not\nor if the function / operator you need is part of a SQL standard, please open an\nissue on GitHub. Alternatively, implement the construct yourself and send us a\npull request! See the section on  adding your own functions", 
            "title": "My favorite operator / function isn't listed here!"
        }, 
        {
            "location": "/reference/queries/", 
            "text": "Literals", 
            "title": "Queries"
        }, 
        {
            "location": "/reference/queries/#literals", 
            "text": "", 
            "title": "Literals"
        }, 
        {
            "location": "/reference/extensibility/", 
            "text": "The \nbeam-core\n library and respective backends strive to expose the full power\nof each underlying database. If a particular feature is missing, please feel\nfree to file a bug report on the GitHub issue tracker.\n\n\nHowever, in the meantime, beam offers a few options to inject raw SQL into your\nqueries. Of course, beam cannot predict types of expressions and queries that\nwere not created with its combinators, so \ncaveat emptor\n.\n\n\nCustom expressions\n\n\nIf you'd like to write an expression that beam currently does not support, you\ncan use the \ncustomExpr_\n function. Your backend's syntax must implement the\n\nIsSqlCustomExpressionSyntax\n type class. \ncustomExpr_\n takes a function of\narity \nn\n and \nn\n arguments, which must all be \nQGenExpr\ns with the same thread\nparameter. The expressions may be from different contexts (i.e., you can pass an\naggregate and scalar into the same \ncustomExpr_\n).\n\n\nThe function supplied must return a \nByteString\n corresponding to the custom bit\nof SQL that implements your expression. The function's arguments are \nn\n\n\nByteString\ns corresponding to the expressions which will evaluate to the value\nof each of the arguments to \ncustomExpr_\n. The arguments are properly\nparenthesized and can be inserted whole into the final expression. You will\nlikely need to explicitly supply a result type using the \nas_\n function.\n\n\nFor example, below, we use \ncustomExpr_\n to access the \nregr_intercept\n and\n\nregr_slope\n functions in postgres.\n\n\n\n                \n\n                    \n        \n\n            \n        \n\n            \nHaskell\n\n        \n\n    \n        \n\n            \nPostgres\n\n        \n\n    \n        \n\n    \n        \n\n            \n        \n\n            \naggregate_\n \n(\n\\\nt\n \n-\n \n(\n \nas_\n \n@\nDouble\n \n@\nQAggregateContext\n \n$\n \ncustomExpr_\n \n(\n\\\nbytes\n \nms\n \n-\n \nregr_intercept(\n \n \nbytes\n \n \n, \n \n \nms\n \n \n)\n)\n \n(\ntrackBytes\n \nt\n)\n \n(\ntrackMilliseconds\n \nt\n)\n\n                  \n,\n \nas_\n \n@\nDouble\n \n@\nQAggregateContext\n \n$\n \ncustomExpr_\n \n(\n\\\nbytes\n \nms\n \n-\n \nregr_slope(\n \n \nbytes\n \n \n, \n \n \nms\n \n \n)\n)\n \n(\ntrackBytes\n \nt\n)\n \n(\ntrackMilliseconds\n \nt\n)\n \n))\n \n$\n\n\nall_\n \n(\ntrack\n \nchinookDb\n)\n\n\n\n\n        \n\n    \n        \n\n            \nSELECT\n \nregr_intercept\n((\nt0\n.\nBytes\n),\n \n(\nt0\n.\nMilliseconds\n))\n \nAS\n \nres0\n,\n\n       \nregr_slope\n((\nt0\n.\nBytes\n),\n \n(\nt0\n.\nMilliseconds\n))\n \nAS\n \nres1\n\n\nFROM\n \nTrack\n \nAS\n \nt0\n\n\n\n\n        \n\n    \n        \n\n    \n                \n\n                    \n\n\nOf course, this requires that the expression is easily expressible as a\n\nByteString\n.\n\n\nCustom queries\n\n\nSometimes you would like to drop down to raw SQL to write a query that will\nreturn an entire result. Beam supports this through the \ncustomQuery_\n function.\nLike \ncustomExpr_\n, this takes a function of \nn\n arity and \nn\n arguments, which\nmay be either \nQGenExpr\ns or \nQ\ns from the same thread, select syntax, etc. The\nfunction supplied to \ncustomQuery_\n must return a \nByteString\n and its arguments\nare \nByteString\ns corresponding to the given \nQ\n or \nQGenExpr\n parameter.", 
            "title": "Extending"
        }, 
        {
            "location": "/reference/extensibility/#custom-expressions", 
            "text": "If you'd like to write an expression that beam currently does not support, you\ncan use the  customExpr_  function. Your backend's syntax must implement the IsSqlCustomExpressionSyntax  type class.  customExpr_  takes a function of\narity  n  and  n  arguments, which must all be  QGenExpr s with the same thread\nparameter. The expressions may be from different contexts (i.e., you can pass an\naggregate and scalar into the same  customExpr_ ).  The function supplied must return a  ByteString  corresponding to the custom bit\nof SQL that implements your expression. The function's arguments are  n  ByteString s corresponding to the expressions which will evaluate to the value\nof each of the arguments to  customExpr_ . The arguments are properly\nparenthesized and can be inserted whole into the final expression. You will\nlikely need to explicitly supply a result type using the  as_  function.  For example, below, we use  customExpr_  to access the  regr_intercept  and regr_slope  functions in postgres.  \n                 \n                    \n         \n            \n         \n             Haskell \n         \n    \n         \n             Postgres \n         \n    \n         \n    \n         \n            \n         \n             aggregate_   ( \\ t   -   (   as_   @ Double   @ QAggregateContext   $   customExpr_   ( \\ bytes   ms   -   regr_intercept(     bytes     ,      ms     ) )   ( trackBytes   t )   ( trackMilliseconds   t ) \n                   ,   as_   @ Double   @ QAggregateContext   $   customExpr_   ( \\ bytes   ms   -   regr_slope(     bytes     ,      ms     ) )   ( trackBytes   t )   ( trackMilliseconds   t )   ))   $  all_   ( track   chinookDb )  \n\n         \n    \n         \n             SELECT   regr_intercept (( t0 . Bytes ),   ( t0 . Milliseconds ))   AS   res0 , \n        regr_slope (( t0 . Bytes ),   ( t0 . Milliseconds ))   AS   res1  FROM   Track   AS   t0  \n\n         \n    \n         \n    \n                 \n                      Of course, this requires that the expression is easily expressible as a ByteString .", 
            "title": "Custom expressions"
        }, 
        {
            "location": "/reference/extensibility/#custom-queries", 
            "text": "Sometimes you would like to drop down to raw SQL to write a query that will\nreturn an entire result. Beam supports this through the  customQuery_  function.\nLike  customExpr_ , this takes a function of  n  arity and  n  arguments, which\nmay be either  QGenExpr s or  Q s from the same thread, select syntax, etc. The\nfunction supplied to  customQuery_  must return a  ByteString  and its arguments\nare  ByteString s corresponding to the given  Q  or  QGenExpr  parameter.", 
            "title": "Custom queries"
        }, 
        {
            "location": "/about/compatibility/", 
            "text": "Beam strives to cover the full breadth of the relevant SQL\nstandards. In general, if there is something in a SQL standard that is\nnot implemented in a generic manner in \nbeam-core\n, feel free to file\nan issue requesting support. There are some features that beam\npurposefully omits because no major RDBMS implements them. For\nexample, database-level assertions are not supported in any of the\ndefault beam backends, and thus are not supported by \nbeam-core\n. If\nyou have a need for these features, feel free to file an issue. Be\nsure to motivate your use case with examples and a testing strategy.\n\n\nThe relevant SQL standards are SQL-92, SQL:1999, SQL:2003, SQL:2008,\nand SQL:2011. Because not all the standards are not publicly\naccessible, I've done my best to piece together features from various\ndocuments available online. I believe I've covered most of the common\ncases, but there may be pieces of functionality that are missing. File\nan issue if this is the case.\n\n\nThe table below summarizes the features defined in each SQL standard and beam's\nsupport for them. FULL means beam supports everything in that feature. NONE\nmeans that there is no support for that feature, and none planned. N/A means\nthat the feature only applies to RDBMSs, not the SQL language. WONTFIX means\nthat the feature has been considered and willfully ignored. UNKNOWN means not\nenough investigation has gone into the feature to make a determination. TODO\nmeans the feature has not been implemented yet, but an implementation is\nplanned.\n\n\n\n\nTip\n\n\nThe 'TODO' items are a great way to contribute to beam!\n\n\n\n\n\n\n\n\n\n\nFeature\n\n\nStatus\n\n\nNotes\n\n\n\n\n\n\n\n\n\n\nB011 Embedded Ada\n\n\nNONE\n\n\n\n\n\n\n\n\nB012 Embedded C\n\n\nNONE\n\n\n\n\n\n\n\n\nB013 Embedded COBOL\n\n\nNONE\n\n\n\n\n\n\n\n\nB014 Embedded FORTRAN\n\n\nNONE\n\n\n\n\n\n\n\n\nB015 Embedded MUMPS\n\n\nNONE\n\n\n\n\n\n\n\n\nB016 Embedded Pascal\n\n\nNONE\n\n\n\n\n\n\n\n\nB017 Embedded PL/I\n\n\nNONE\n\n\n\n\n\n\n\n\nB021 Direct SQL\n\n\nNONE\n\n\n\n\n\n\n\n\nB031 Basic dynamic SQL\n\n\nNONE\n\n\n\n\n\n\n\n\nB032 Extended dynamic SQL\n\n\nNONE\n\n\n\n\n\n\n\n\nB033 Untyped SQL-invoked function arguments\n\n\nNONE\n\n\n\n\n\n\n\n\nB034 Dynamic specification of cursor attributes\n\n\nNONE\n\n\n\n\n\n\n\n\nB035 Non-extended descriptor names\n\n\nNONE\n\n\n\n\n\n\n\n\nB051 Enhanced execution rights\n\n\nNONE\n\n\n\n\n\n\n\n\nB111 Module language Ada\n\n\nNONE\n\n\n\n\n\n\n\n\nB112 Module language C\n\n\nNONE\n\n\n\n\n\n\n\n\nB113 Module language COBOL\n\n\nNONE\n\n\n\n\n\n\n\n\nB114 Module language Fortran\n\n\nNONE\n\n\n\n\n\n\n\n\nB115 Module language MUMPS\n\n\nNONE\n\n\n\n\n\n\n\n\nB116 Module language Pascal\n\n\nNONE\n\n\n\n\n\n\n\n\nB117 Module language PL/I\n\n\nNONE\n\n\n\n\n\n\n\n\nB121 Routine language Ada\n\n\nNONE\n\n\n\n\n\n\n\n\nB122 Routine language C\n\n\nNONE\n\n\n\n\n\n\n\n\nB123 Routine language COBOL\n\n\nNONE\n\n\n\n\n\n\n\n\nB124 Routine language Fortran\n\n\nNONE\n\n\n\n\n\n\n\n\nB125 Routine language MUMPS\n\n\nNONE\n\n\n\n\n\n\n\n\nB126 Routine language Pascal\n\n\nNONE\n\n\n\n\n\n\n\n\nB127 Routine language PL/I\n\n\nNONE\n\n\n\n\n\n\n\n\nB128 Routine language SQL\n\n\nNONE\n\n\n\n\n\n\n\n\nB211 Module language Ada: VARCHAR and NUMERIC support\n\n\nNONE\n\n\n\n\n\n\n\n\nB221 Routine language Ada: VARCHAR and NUMERIC support\n\n\nNONE\n\n\n\n\n\n\n\n\nE011 - Numeric data types\n\n\n\n\n\n\n\n\n\n\nE011-01 INTEGER and SMALLINT data types\n\n\nFULL\n\n\nUse \nInt32\n for \nINTEGER\n, \nInt16\n for \nSMALLINT\n\n\n\n\n\n\nE011-02 REAL, DOUBLE PRECISION, FLOAT\n\n\nFULL\n\n\nUse \nDouble\n and \nFloat\n\n\n\n\n\n\nE011-03 DECIMAL and NUMERIC data types\n\n\nFULL\n\n\nUse \nScientific\n. You can provide the database precision using \nbeam-migrate\n\n\n\n\n\n\nE011-04 Arithmetic operators\n\n\nFULL\n\n\nUse the \nNum\n instance for \nQGenExpr\n\n\n\n\n\n\nE011-05 Numeric comparison\n\n\nFULL\n\n\nUse the \n.\n prefixed operators (i.e., \n==.\n, \n/=.\n, \n.\n, etc)\n\n\n\n\n\n\nE011-06 Implicit casting among numeric data types\n\n\nWONTFIX\n\n\nBeam never implicitly casts. Use \ncast_\n\n\n\n\n\n\nE021 Character string types\n\n\n\n\n\n\n\n\n\n\nE021-01 CHARACTER data type\n\n\nFULL\n\n\nUse \nText\n. Use \nbeam-migrate\n to specify width\n\n\n\n\n\n\nE021-02 CHARACTER VARYING data type\n\n\nFULL\n\n\nUse \nText\n. Use \nbeam-migrate\n to specify width.\n\n\n\n\n\n\nE021-03 Character literals\n\n\nFULL\n\n\nUse \nval_\n\n\n\n\n\n\nE021-04 CHARACTER_LENGTH function\n\n\nFULL\n\n\nUse \ncharLength_\n\n\n\n\n\n\nE021-05 OCTET_LENGTH function\n\n\nFULL\n\n\nUse \noctetLength_\n\n\n\n\n\n\nE021-06 SUBSTRING function\n\n\nTODO\n\n\n\n\n\n\n\n\nE021-07 Character concatenation\n\n\nTODO\n\n\nUse \n(\n#x007c;\n#0x007c).\n\n\n\n\n\n\nE021-08 UPPER and LOWER functions\n\n\nTODO\n\n\nUse \nupper_\n and \nlower_\n\n\n\n\n\n\nE021-09 TRIM function\n\n\nTODO\n\n\nUse \ntrim_\n\n\n\n\n\n\nE021-10 Implicit casting among string types\n\n\nWONTFIX\n\n\nBeam never implicitly casts. Use \ncast_\n\n\n\n\n\n\nE021-11 POSITION function\n\n\nFULL\n\n\nUse \nposition_\n\n\n\n\n\n\nE021-12 Character comparison\n\n\nFULL\n\n\nUse comparison operators (See E011-05)\n\n\n\n\n\n\nE031 Identifiers\n\n\n\n\n\n\n\n\n\n\nE031-01 Delimited identifiers\n\n\nTODO\n\n\nFind out more\n\n\n\n\n\n\nE021-02 Lower case identifiers\n\n\nTODO\n\n\n\n\n\n\n\n\nE021-03 Trailing underscore\n\n\nN/A\n\n\nBeam will use whatever column names you specify\n\n\n\n\n\n\nE051 Basic query specification\n\n\n\n\n\n\n\n\n\n\nE051-01 SELECT DISTINCT\n\n\nTODO\n\n\nUse \nselectDistinct_\n\n\n\n\n\n\nE051-02 GROUP BY clause\n\n\nFULL\n\n\nSee \naggregate_\n or read the \nsection on aggregates\n\n\n\n\n\n\nE051-04 GROUP BY can contain columns not in SELECT\n\n\nTODO\n\n\nUnsure how this applies to beam in particular\n\n\n\n\n\n\nE051-05 Select list items can be renamed\n\n\nN/A\n\n\nBeam uses this feature internally, the user never needs it\n\n\n\n\n\n\nE051-06 HAVING clause\n\n\nFULL\n\n\nguard_\n and \nfilter_\n are appropriately converted to \nHAVING\n when allowed\n\n\n\n\n\n\nE051-07 Qualified * in select list\n\n\nN/A\n\n\nBeam handles projections instead\n\n\n\n\n\n\nE051-08 Correlation names in FROM\n\n\nTODO\n\n\nUnsure how this applies to beam\n\n\n\n\n\n\nE051-09 Rename columns in the FROM clause\n\n\nNONE\n\n\nBeam doesn't need this\n\n\n\n\n\n\nE061 Basic predicates and search conditions\n\n\n\n\n\n\n\n\n\n\nE061-01 Comparison predicate\n\n\nFULL\n\n\nUse the comparison operators (see E011-05)\n\n\n\n\n\n\nE061-02 BETWEEN predicate\n\n\nFULL\n\n\nUse \nbetween_\n\n\n\n\n\n\nE061-03 IN predicate with list of values\n\n\nTODO\n\n\n\n\n\n\n\n\nE061-04 LIKE predicate\n\n\nFULL\n\n\nUse \nlike_\n\n\n\n\n\n\nE061-05 LIKE predicate ESCAPE clause\n\n\nTODO\n\n\nUnsure how this would apply\n\n\n\n\n\n\nE061-06 NULL predicate\n\n\nFULL\n\n\nUse \nisNull_\n and \nisNotNull_\n\n\n\n\n\n\nE061-07 Quantified comparison predicate\n\n\nPARTIAL\n\n\nSupported in syntaxes, not exposed in \nQGenExpr\n (TODO)\n\n\n\n\n\n\nE051-08 EXISTS predicate\n\n\nFULL\n\n\nUse \nexists_\n\n\n\n\n\n\nE061-09 Subqueries in comparison predicate\n\n\nFULL\n\n\nUse \nsubquery_\n as usual\n\n\n\n\n\n\nE061-11 Subqueries in IN predicate\n\n\nTODO\n\n\nWould be fixed by E061-03\n\n\n\n\n\n\nE061-12 Subqueries in quantified comparison predicate\n\n\nTODO\n\n\nWould be fixed by E061-07\n\n\n\n\n\n\nE061-13 Correlated subqueries\n\n\nFULL\n\n\nUse \nsubquery_\n\n\n\n\n\n\nE061-14 Search condition\n\n\nFULL\n\n\nConstruct \nQGenExprs\n with type \nBool\n\n\n\n\n\n\nE071 Basic query expressions\n\n\n\n\n\n\n\n\n\n\nE071-01 UNION DISTINCT table operator\n\n\nFULL\n\n\nUse \nunion_\n\n\n\n\n\n\nE071-02 UNION ALL table operator\n\n\nFULL\n\n\nUse \nunionAll_\n\n\n\n\n\n\nE071-03 EXCEPT DISTINCT table operator\n\n\nFULL\n\n\nUse \nexcept_\n\n\n\n\n\n\nE071-05 Columns combined via operators need not have same type\n\n\nWONTFIX\n\n\nBeam is strongly typed\n\n\n\n\n\n\nE071-06 Table operators in subqueries\n\n\nFULL\n\n\nSupported for backends that support it\n\n\n\n\n\n\nE081 Basic privileges\n\n\nNONE\n\n\nDatabase security is not beam's focus. \nbeam-migrate\n may expose this in the future\n\n\n\n\n\n\nE091 Set functions\n\n\n\n\n\n\n\n\n\n\nE091-01 AVG\n\n\nFULL\n\n\nUse \navg_\n or \navgOver_\n\n\n\n\n\n\nE091-02 COUNT\n\n\nFULL\n\n\nUse \ncountAll_\n, \ncountAllOver_\n, \ncount_\n, or \ncountOver_\n\n\n\n\n\n\nE091-03 MAX\n\n\nFULL\n\n\nUse \nmax_\n or \nmaxOver_\n\n\n\n\n\n\nE091-04 MIN\n\n\nFULL\n\n\nUse \nmin_\n or \nminOver_\n\n\n\n\n\n\nE091-05 SUM\n\n\nFULL\n\n\nUse \nsum_\n or \nsumOver_\n\n\n\n\n\n\nE091-06 ALL quantifier\n\n\nFULL\n\n\nUse the \n*Over_\n functions with the \nallInGroupExplicitly_\n quantifier\n\n\n\n\n\n\nE091-07 DISTINCT quantifier\n\n\nFULL\n\n\nUse the \n*Over_\n functions with the \ndistinctInGroup_\n quantifier\n\n\n\n\n\n\nE101 Basic data manipulation\n\n\n\n\n\n\n\n\n\n\nE101-01 INSERT statement\n\n\nFULL\n\n\nUse \ninsert\n and \nSqlInsert\n\n\n\n\n\n\nE101-03 Searched UPDATE\n\n\nFULL\n\n\nUse \nupdate\n and \nSqlUpdate\n\n\n\n\n\n\nE101-04 Searched DELETE\n\n\nFULL\n\n\nUse \ndelete\n and \nSqlDelete\n\n\n\n\n\n\nE111 Single row SELECT statement\n\n\nFULL\n\n\nUse \nselect\n as expected\n\n\n\n\n\n\nE121 Basic cursor support\n\n\nNONE\n\n\nUse the backends explicitly\n\n\n\n\n\n\nE131 Null value support\n\n\nPARTIAL\n\n\nUse \nMaybe\n column types, \nNullable\n, and the \njust_\n, \nnothing_\n, and \nmaybe_\n functions\n\n\n\n\n\n\nE141 Basic integrity constraints\n\n\n\n\nImplemented in \nbeam-migrate\n\n\n\n\n\n\nE141-01 NOT NULL constraints\n\n\nFULL\n\n\nUse \nnotNull_\n\n\n\n\n\n\nE141-02 UNIQUE constraints of NOT NULL columns\n\n\nTODO\n\n\n\n\n\n\n\n\nE141-03 PRIMARY KEY constraints\n\n\nFULL\n\n\nInstantiate \nTable\n with the correct \nPrimaryKey\n\n\n\n\n\n\nE141-04 Basic FOREIGN KEY constraints\n\n\nTODO\n\n\nYou can embed the \nPrimaryKey\n of the relation directly.\n\n\n\n\n\n\nE141-06 CHECK constraints\n\n\nTODO\n\n\n\n\n\n\n\n\nE141-07 Column defaults\n\n\nFULL\n\n\nUse \ndefault_\n from \nbeam-migrate\n\n\n\n\n\n\nE141-08 NOT NULL inferred on PRIMARY KEY\n\n\nN/A\n\n\n\n\n\n\n\n\nE141-10 Names in a foreign key can be specified in any order\n\n\nN/A\n\n\n\n\n\n\n\n\nE151 Transaction support\n\n\nNone\n\n\nUse the backend functions explicitly\n\n\n\n\n\n\nE152 SET TRANSACTION statement\n\n\nN/A\n\n\n\n\n\n\n\n\nE153 Updatable queries with subqueries\n\n\nTODO\n\n\n\n\n\n\n\n\nE161 SQL comments with double minus\n\n\nN/A\n\n\n\n\n\n\n\n\nE171 SQLSTATE support\n\n\nN/A\n\n\n\n\n\n\n\n\nE182 Host language binding\n\n\nN/A\n\n\n\n\n\n\n\n\nF031 Basic schema manipulation\n\n\n\n\n\n\n\n\n\n\nF031-01 CREATE TABLE for persistent base tables\n\n\nFULL\n\n\nUse \ncreateTable_\n in \nbeam-migrate\n\n\n\n\n\n\nF031-02 CREATE VIEW statement\n\n\nTODO\n\n\n\n\n\n\n\n\nF031-03 GRANT statement\n\n\nTODO\n\n\n\n\n\n\n\n\nF031-04 ALTER TABLE statement: ADD COLUMN clause\n\n\nTODO\n\n\n\n\n\n\n\n\nF031-13 DROP TABLE statement: RESTRICT clause\n\n\nTODO\n\n\n\n\n\n\n\n\nF031-16 DROP VIEW statement: RESTRICT clause\n\n\nTODO\n\n\n\n\n\n\n\n\nF031-19 REVOKE statement: RESTRICT clause\n\n\nNONE\n\n\nSee note for E081\n\n\n\n\n\n\nF032 CASCADE drop behavior\n\n\nTODO\n\n\nWould be in \nbeam-migrate\n\n\n\n\n\n\nF033 ALTER TABLE statement: DROP COLUMN clause\n\n\nTODO\n\n\n\n\n\n\n\n\nF034 Extended REVOKE statement\n\n\nNONE\n\n\n\n\n\n\n\n\nF041 Basic joined table\n\n\n\n\n\n\n\n\n\n\nF041-01 Inner join\n\n\nFULL\n\n\nUse the \nmonadic join interface\n\n\n\n\n\n\nF041-02 INNER keyword\n\n\nN/A\n\n\nNo semantic difference\n\n\n\n\n\n\nF041-03 LEFT OUTER JOIN\n\n\nFULL\n\n\nUse \nleftJoin_\n\n\n\n\n\n\nF041-04 RIGHT OUTER JOIN\n\n\nPARTIAL\n\n\nSupported in backend syntaxes, not exposed. Can always be written using LEFT OUTER JOIN\n\n\n\n\n\n\nF041-05 Outer joins can be nested\n\n\nTODO\n\n\nDepends on \nouterJoin_\n\n\n\n\n\n\nF041-07 The inner table in outer join can be used in inner join\n\n\nTODO\n\n\nHow does this apply to us?\n\n\n\n\n\n\nF041-08 All comparison operators in JOIN\n\n\nFULL\n\n\nArbitrary \nQGenExpr\ns are supported.\n\n\n\n\n\n\nF051 Basic date and time\n\n\n\n\n\n\n\n\n\n\nF051-01 DATE data type\n\n\nFULL\n\n\nUse \nDay\n from \nData.Time\n and \nval_\n\n\n\n\n\n\nF051-02 TIME data type\n\n\nFULL\n\n\nUse \nTimeOfDay\n from \nData.Time\n and \nval_\n\n\n\n\n\n\nF051-03 TIMESTAMP datatype\n\n\nFULL\n\n\nUse \nLocalTime\n from \nData.Time\n and \nval_\n. Precision can be specified in \nbeam-migrate\n\n\n\n\n\n\nF051-04 Comparison predicate on time types\n\n\nFULL\n\n\nUse comparison operatiors (See E011-05)\n\n\n\n\n\n\nF051-05 Explicit cast between date-time types and string\n\n\nTODO\n\n\n\n\n\n\n\n\nF051-06 CURRENT_DATE\n\n\nTODO\n\n\n\n\n\n\n\n\nF051-07 LOCALTIME\n\n\nTODO\n\n\n\n\n\n\n\n\nF051-08 LOCALTIMESTAMP\n\n\nTODO\n\n\n\n\n\n\n\n\nF081 UNION and EXCEPT in views\n\n\nTODO\n\n\nDepends on view support\n\n\n\n\n\n\nF111 Isolation levels other than SERIALIZABLE\n\n\nNONE\n\n\nUse backends\n\n\n\n\n\n\nF121 Basic diagnostics mangement\n\n\nNONE\n\n\nUse backends\n\n\n\n\n\n\nF122 Extended diagnostics management\n\n\nNONE\n\n\nUse backends\n\n\n\n\n\n\nF123 All diagnostics\n\n\nNONE\n\n\nUse backends\n\n\n\n\n\n\nF131 Grouped operations\n\n\nTODO\n\n\nDepends on grouped views\n\n\n\n\n\n\nF171 Multiple schemas per user\n\n\nN/A\n\n\nDepends on backend\n\n\n\n\n\n\nF191 Referential delete actions\n\n\nTODO\n\n\n\n\n\n\n\n\nF181 Multiple module support\n\n\nN/A\n\n\n\n\n\n\n\n\nF200 TRUNCATE TABLE statement\n\n\nTODO\n\n\nMay be added in the future\n\n\n\n\n\n\nF201 CAST function\n\n\nTODO\n\n\n\n\n\n\n\n\nF202 TRUNCATE TABLE: identity column restart option\n\n\nTODO\n\n\nDepends on F200\n\n\n\n\n\n\nF221 Explicit defaults\n\n\nFULL\n\n\nUse \nAuto Nothing\n when inserting\n\n\n\n\n\n\nF222 INSERT statement: DEFAULT VALUES clause\n\n\nTODO\n\n\n\n\n\n\n\n\nF251 Domain support\n\n\nTODO\n\n\nUse \nDomainType\n\n\n\n\n\n\nF261 CASE expression\n\n\n\n\n\n\n\n\n\n\nF261-01 Simple CASE\n\n\nTODO\n\n\nUse searched case (see F261-02)\n\n\n\n\n\n\nF261-02 Searched CASE\n\n\nFULL\n\n\nUse \nif_\n, \nthen_\n, and \nelse_\n\n\n\n\n\n\nF261-03 NULLIF\n\n\nFULL\n\n\nUse \nnullIf_\n\n\n\n\n\n\nF261-04 COALESCE\n\n\nFULL\n\n\nUse \ncoalesce_\n\n\n\n\n\n\nF262 Extended CASE expression\n\n\nPARTIAL\n\n\nBeam allows any value in a \nCASE\n expression\n\n\n\n\n\n\nF263 Comma-separater predicates in simple CASE expression\n\n\nTODO\n\n\n\n\n\n\n\n\nF271 Compound character literals\n\n\nPARTIAL\n\n\nBackends may do this automatically\n\n\n\n\n\n\nF281 LIKE enhancements\n\n\nPARTIAL\n\n\nSupported in backends that support this\n\n\n\n\n\n\nF291 UNIQUE predicate\n\n\nFULL\n\n\nUse \nunique_\n\n\n\n\n\n\nF301 CORRESPONDING in query expressions\n\n\nTODO\n\n\n\n\n\n\n\n\nF302 INTERSECT table operator\n\n\nFULL\n\n\nUse \nintersect_\n\n\n\n\n\n\nF302-01 INTERSECT DISTINCT table operator\n\n\nFULL\n\n\nUse \nintersect_\n\n\n\n\n\n\nF302-02 INTERSET ALL table operator\n\n\nFULL\n\n\nUse \nintersectAll_\n\n\n\n\n\n\nF304 EXCEPT ALL table operator\n\n\nFULL\n\n\nUse \nexceptAll_\n\n\n\n\n\n\nF311 Schema definition statement\n\n\nTODO\n\n\nWould be in \nbeam-migrate\n\n\n\n\n\n\nF312 MERGE statement\n\n\nTODO\n\n\n\n\n\n\n\n\nF313 Enhanced MERGE statement\n\n\nTODO\n\n\n\n\n\n\n\n\nF314 MERGE statement with DELETE branch\n\n\nTODO\n\n\n\n\n\n\n\n\nF321 User authorization\n\n\nN/A\n\n\n\n\n\n\n\n\nF361 Subprogram support\n\n\nN/A\n\n\n\n\n\n\n\n\nF381 Extended schema manipulation\n\n\nTODO\n\n\n\n\n\n\n\n\nF382 Alter column data type\n\n\nTODO\n\n\n\n\n\n\n\n\nF384 Drop identity property clause\n\n\nTODO\n\n\n\n\n\n\n\n\nF385 Drop column generation expression clause\n\n\nTODO\n\n\n\n\n\n\n\n\nF386 Set identity column generation clause\n\n\nTODO\n\n\n\n\n\n\n\n\nF391 Long identifiers\n\n\nPARTIAL\n\n\nSupported in backends that support it\n\n\n\n\n\n\nF392 Unicode escapes in identifiers\n\n\nTODO\n\n\nUnsure how this applies\n\n\n\n\n\n\nF393 Unicode escapes in literals\n\n\nTODO\n\n\nUnsure how this applies\n\n\n\n\n\n\nF394 Optional normal form specification\n\n\nN/A\n\n\n\n\n\n\n\n\nF401 Extended joined table\n\n\nPARTIAL\n\n\nFULL OUTER JOIN\n support planned with \nouterJoin_\n. \nCROSS JOIN\n is the same as \nINNER JOIN\n, and \nNATURAL JOIN\n has no meaning in beam.\n\n\n\n\n\n\nF402 Named column joins for LOBs, arrays, and multisets\n\n\nPARTIAL\n\n\nSupported in backends that support it\n\n\n\n\n\n\nF403 Partitioned join tables\n\n\nTODO\n\n\n\n\n\n\n\n\nF411 Time zone specification\n\n\nTODO\n\n\n\n\n\n\n\n\nF421 National character\n\n\nFULL\n\n\nSupported in \nbeam-migrate\n as a data type for \nText\n\n\n\n\n\n\nF431 Read-only scrollable cursors\n\n\nN/A\n\n\nUse the underlying backend\n\n\n\n\n\n\nF441 Extended set function support\n\n\nTODO\n\n\n\n\n\n\n\n\nF442 Mixed column references in set functions\n\n\nTODO\n\n\nUnsure how this would work with beam\n\n\n\n\n\n\nF451 Character set definition\n\n\nTODO\n\n\nLikely would go in \nbeam-migrate\n\n\n\n\n\n\nF461 Named character sets\n\n\nTODO\n\n\nSee F451\n\n\n\n\n\n\nF491 Constraint management\n\n\nTODO\n\n\n\n\n\n\n\n\nF492 Optional table constraint enforcement\n\n\nTODO\n\n\n\n\n\n\n\n\nF521 Assertions\n\n\nTODO\n\n\n\n\n\n\n\n\nF531 Temporary tables\n\n\nTODO\n\n\n\n\n\n\n\n\nF481 Expanded NULL predicate\n\n\nFULL\n\n\nSupported in backends that support it\n\n\n\n\n\n\nF555 Enhanced seconds precision\n\n\nTODO\n\n\n\n\n\n\n\n\nF561 Full value expressions\n\n\nTODO\n\n\n\n\n\n\n\n\nF571 Truth value tests\n\n\nTODO\n\n\n\n\n\n\n\n\nF591 Derived tables\n\n\nTODO\n\n\n\n\n\n\n\n\nF611 Indicator data types\n\n\nTODO\n\n\n\n\n\n\n\n\nF641 Row and table constructors\n\n\nPARTIAL\n\n\nUse \nrow_\n (TODO)\n\n\n\n\n\n\nF651 Catalog name qualifiers\n\n\nTODO\n\n\n\n\n\n\n\n\nF661 Simple tables\n\n\nTODO\n\n\n\n\n\n\n\n\nF671 Subqueries in CHECK constraints\n\n\nTODO\n\n\nPlanned with E141-06\n\n\n\n\n\n\nF672 Retrospective CHECK constraints\n\n\nTODO\n\n\nWould require temporal DB support\n\n\n\n\n\n\nF690 Collation support\n\n\nPARTIAL\n\n\nbeam-migrate\n supports some collation features\n\n\n\n\n\n\nF692 Enhanced collation support\n\n\nTODO\n\n\n\n\n\n\n\n\nF693 SQL-session and client module collations\n\n\nTODO\n\n\n\n\n\n\n\n\nF695 Translation support\n\n\nTODO\n\n\n\n\n\n\n\n\nF701 Referential update actions\n\n\nTODO\n\n\n\n\n\n\n\n\nF711 ALTER domain\n\n\nTODO\n\n\n\n\n\n\n\n\nF721 Deferrable constraints\n\n\nPARTIAL\n\n\nThe syntax exists in \nbeam-migrate\n\n\n\n\n\n\nF731 INSERT column privileges\n\n\nN/A\n\n\n\n\n\n\n\n\nF741 Referential MATCH type\n\n\nPARTIAL\n\n\nExists in the syntax in \nbeam-migrate\n, not exposed yet (TODO)\n\n\n\n\n\n\nF751 View CHECK enhancements\n\n\nTODO\n\n\n\n\n\n\n\n\nF761 Session management\n\n\nTODO\n\n\n\n\n\n\n\n\nF762 CURRENT_CATALOG\n\n\nTODO\n\n\n\n\n\n\n\n\nF763 CURRENT_SCHEMA\n\n\nTODO\n\n\n\n\n\n\n\n\nF812 Basic flagging\n\n\nN/A\n\n\n\n\n\n\n\n\nF841 LIKE_REGEX predicate\n\n\nTODO\n\n\nEasy\n\n\n\n\n\n\nF842 OCCURENCES_REGEX function\n\n\nTODO\n\n\nEasy\n\n\n\n\n\n\nF843 POSITION_REGEX function\n\n\nTODO\n\n\nEasy\n\n\n\n\n\n\nF844 SUBSTRING_REGEX function\n\n\nTODO\n\n\nEasy\n\n\n\n\n\n\nF845 TRANSLATE_REGEX function\n\n\nTODO\n\n\nEasy\n\n\n\n\n\n\nF846 Octet support in regular expression operators\n\n\nTODO\n\n\n\n\n\n\n\n\nF847 Nonconstant regular expression\n\n\nTODO\n\n\nEasy once regex support is added\n\n\n\n\n\n\nF850 Top-level \n in \n\n\nFULL\n\n\nUse \norderBy_\n as usual. Beam will do the right thing behind the scenes.\n\n\n\n\n\n\nF851 \n in subqueries\n\n\nFULL\n\n\nWorks in backends that support it\n\n\n\n\n\n\nF852 Top-level \n in views\n\n\nTODO\n\n\n\n\n\n\n\n\nF855 Nested \n in \n\n\nUNKNOWN\n\n\n\n\n\n\n\n\nF856 Nested \n in \n\n\nTODO\n\n\n\n\n\n\n\n\nF857 Top-level \n in \n\n\nTODO\n\n\n\n\n\n\n\n\nF858 \n in subqueries\n\n\nTODO\n\n\n\n\n\n\n\n\nF859 Top-level \n in subqueries\n\n\nTODO\n\n\n\n\n\n\n\n\n*\nF860 dynamic \n in \n\n\nTODO\n\n\n\n\n\n\n\n\n*\nF861 Top-level \n in \n\n\nTODO\n\n\n\n\n\n\n\n\nF862 \n in subqueries\n\n\nTODO\n\n\n\n\n\n\n\n\nF863 Nested \n in \n\n\nTODO\n\n\n\n\n\n\n\n\nF864 TOp-level \n in views\n\n\nTODO\n\n\n\n\n\n\n\n\nF865 dynamic \n in \n\n\nTODO\n\n\n\n\n\n\n\n\nF866 FETCH FIRST clause: PERCENT option\n\n\nTODO\n\n\n\n\n\n\n\n\nF867 FETCH FIRST clause: WITH TIES option\n\n\nTODO\n\n\n\n\n\n\n\n\nR010 Row pattern recognition: FROM clause\n\n\nTODO\n\n\n\n\n\n\n\n\nR020 Row pattern recognition: WINDOW clause\n\n\nTODO\n\n\n\n\n\n\n\n\nR030 Row pattern recognition: full aggregate support\n\n\nTODO\n\n\n\n\n\n\n\n\nS011 Distinct data types\n\n\nTODO\n\n\n\n\n\n\n\n\nS023 Basic structured types\n\n\nTODO\n\n\n\n\n\n\n\n\nS024 Enhanced structured types\n\n\nTODO\n\n\n\n\n\n\n\n\nS025 Final structured types\n\n\nTODO\n\n\n\n\n\n\n\n\nS026 Self-referencing structured types\n\n\nTODO\n\n\n\n\n\n\n\n\nS027 Create method by specific method name\n\n\nTODO\n\n\n\n\n\n\n\n\nS028 Permutable UDT options list\n\n\nTODO\n\n\n\n\n\n\n\n\nS041 Basic reference types\n\n\nTODO\n\n\n\n\n\n\n\n\nS043 Enhanced reference types\n\n\nTODO\n\n\n\n\n\n\n\n\nS051 Create table of type\n\n\nTODO\n\n\n\n\n\n\n\n\nS071 SQL paths in function and type name resolution\n\n\nN/A\n\n\nBeam qualifies everything anyway\n\n\n\n\n\n\nS081 Subtables\n\n\nPARTIAL\n\n\nYou can use them right now, but there's no support for their creation or management in \nbeam-migrate\n\n\n\n\n\n\nS091 Basic array support\n\n\nPARTIAL\n\n\nSupported via custom syntax in some backends (\nbeam-postgres\n for example)\n\n\n\n\n\n\nS092 Arrays of user-defined types\n\n\nTODO\n\n\nDepends on user-defined types\n\n\n\n\n\n\nS094 Arrays of reference types\n\n\nTODO\n\n\n\n\n\n\n\n\nS095 Array constructors by query\n\n\nTODO\n\n\n\n\n\n\n\n\nS096 Optional array bounds\n\n\nPARTIAL\n\n\nSupported in \nbeam-postgres\n\n\n\n\n\n\nS097 Array element assignment\n\n\nTODO\n\n\nNot yet, but should be easy enough in \nbeam-postgres\n\n\n\n\n\n\nS098 ARRAY_AGG\n\n\nTODO\n\n\nEasily added to \nbeam-postgres\n (Easy)\n\n\n\n\n\n\nS111 ONLY in query expressions\n\n\nTODO\n\n\n\n\n\n\n\n\nS151 Type predicate\n\n\nTODO\n\n\n\n\n\n\n\n\nS161 Subtype treatment\n\n\nTODO\n\n\n\n\n\n\n\n\nS162 Subtype treatment for references\n\n\nTODO\n\n\n\n\n\n\n\n\nS201 SQL-invoked routines on arrays\n\n\nTODO\n\n\nWould be subsumed by sql-routines (T-321)\n\n\n\n\n\n\nS202 SQL-invoked routines on multisets\n\n\nTODO\n\n\nWould be subsumed by sql-routines (T-321)\n\n\n\n\n\n\nS211 User-defined cast functions\n\n\nTODO\n\n\n\n\n\n\n\n\nS231 Structured type locators\n\n\nTODO\n\n\n\n\n\n\n\n\nS232 Array locators\n\n\nTODO\n\n\n\n\n\n\n\n\nS233 Multiset locators\n\n\nTODO\n\n\n\n\n\n\n\n\nS241 Transform functions\n\n\nTODO\n\n\n\n\n\n\n\n\nS242 Alter transform statement\n\n\nTODO\n\n\n\n\n\n\n\n\nS251 User-defined orderings\n\n\nTODO\n\n\n\n\n\n\n\n\nS261 Specific type method\n\n\nTODO\n\n\n\n\n\n\n\n\nS271 Basic multiset support\n\n\nTODO\n\n\n\n\n\n\n\n\nS272 Multisets of user-defined types\n\n\nTODO\n\n\n\n\n\n\n\n\nS274 Multisets reference types\n\n\nTODO\n\n\n\n\n\n\n\n\nS275 Advanced multiset support\n\n\nTODO\n\n\n\n\n\n\n\n\nS281 Nested collection types\n\n\nTODO\n\n\n\n\n\n\n\n\nS291 Unique constraint on entire row\n\n\nTODO\n\n\n\n\n\n\n\n\nS301 Enhanced UNNEST\n\n\nTODO\n\n\n\n\n\n\n\n\nS401 Distinct types based on array types\n\n\nTODO\n\n\n\n\n\n\n\n\nS402 Distinct types based on distinct types\n\n\nTODO\n\n\n\n\n\n\n\n\nS403 ARRAY_MAX_CARDINALITY\n\n\nTODO\n\n\n\n\n\n\n\n\nS404 TRIM_ARRAY\n\n\nTODO\n\n\n\n\n\n\n\n\nT021 BINARY and VARBINARY data types\n\n\nTODO\n\n\n\n\n\n\n\n\nT022 Advanced support for BINARY and VARBINARY data types\n\n\nTODO\n\n\n\n\n\n\n\n\nT023 Compound binary literals\n\n\nTODO\n\n\n\n\n\n\n\n\nT024 Spaces in binary literals\n\n\nTODO\n\n\n\n\n\n\n\n\nT031 Boolean data type\n\n\nTODO\n\n\n\n\n\n\n\n\nT041 Basic LOB data type support\n\n\nTODO\n\n\n\n\n\n\n\n\nT042 Extended LOB data type support\n\n\nTODO\n\n\n\n\n\n\n\n\nT043 Multiplier T\n\n\nTODO\n\n\n\n\n\n\n\n\nT044 Multiplier P\n\n\nTODO\n\n\n\n\n\n\n\n\nT051 Row types\n\n\nPARTIAL\n\n\n\n\n\n\n\n\nT061 UCS support\n\n\nTODO\n\n\n\n\n\n\n\n\nT071 BIGINT data type\n\n\nTODO\n\n\n\n\n\n\n\n\nT101 Enhanced nullability detection\n\n\nTODO\n\n\n\n\n\n\n\n\nT111 Updatable joins, unions,  and columns\n\n\nTODO\n\n\n\n\n\n\n\n\nT121 WITH (excluding recursive) in query expression\n\n\nTODO\n\n\n\n\n\n\n\n\nT122 WITH (excluding recursive) in subquery\n\n\nTODO\n\n\n\n\n\n\n\n\nT131 Recursive query\n\n\nTODO\n\n\n\n\n\n\n\n\nT132 Recursive query in subquery\n\n\nTODO\n\n\n\n\n\n\n\n\nT141 SIMILAR predicate\n\n\nFULL\n\n\n\n\n\n\n\n\nT151 DISTINCT predicate\n\n\nFULL\n\n\n\n\n\n\n\n\nT152 DISTINCT predicate with negation\n\n\nTODO\n\n\n\n\n\n\n\n\nT171 LIKE clause in table definition\n\n\nTODO\n\n\n\n\n\n\n\n\nT172 AS subquery clause in table definition\n\n\nTODO\n\n\n\n\n\n\n\n\nT173 Extended LIKE clause in table definition\n\n\nTODO\n\n\n\n\n\n\n\n\nT174 Identity columns\n\n\nTODO\n\n\n\n\n\n\n\n\nT175 Generated columns\n\n\nTODO\n\n\n\n\n\n\n\n\nT176 Sequence generator support\n\n\nTODO\n\n\n\n\n\n\n\n\nT177 Sequence generator support: simple restart option\n\n\nTODO\n\n\n\n\n\n\n\n\nT178 Identity columns: simple restart option\n\n\nTODO\n\n\n\n\n\n\n\n\nT180 System-versioned tables\n\n\nTODO\n\n\n\n\n\n\n\n\nT181 Application-time period tables\n\n\nTODO\n\n\n\n\n\n\n\n\nT191 Referential action RESTART\n\n\nTODO\n\n\n\n\n\n\n\n\nT201 Comparable data types for referential constraints\n\n\nTODO\n\n\n\n\n\n\n\n\nT211 Basic trigger capability\n\n\nTODO\n\n\n\n\n\n\n\n\nT212 Enhanced trigger capability\n\n\nTODO\n\n\n\n\n\n\n\n\nT213 INSTEAD OF triggers\n\n\nTODO\n\n\n\n\n\n\n\n\nT231 Sensitive cursors\n\n\nTODO\n\n\n\n\n\n\n\n\nT241 START TRANSACTION statement\n\n\nWONTFIX\n\n\nUse the backend library\n\n\n\n\n\n\nT251 SET TRANSACTION option: LOCAL option\n\n\nWONTFIX\n\n\nUse the backend library\n\n\n\n\n\n\nT261 Chained transactions\n\n\nN/A\n\n\n\n\n\n\n\n\nT271 Savepoints\n\n\nN/A\n\n\n\n\n\n\n\n\nT272 Enhanced savepoint management\n\n\nN/A\n\n\n\n\n\n\n\n\nT281 SELECT privilege with column granularity\n\n\nN/A\n\n\n\n\n\n\n\n\nT285 Enhanced derived column names\n\n\nN/A\n\n\n\n\n\n\n\n\nT301 Functional dependencies\n\n\nTODO\n\n\n\n\n\n\n\n\nT312 OVERLAY function\n\n\nTODO\n\n\n\n\n\n\n\n\nT321 Basic SQL-invoked routines\n\n\nTODO\n\n\n\n\n\n\n\n\nT323 Explicit security for external routines\n\n\nTODO\n\n\n\n\n\n\n\n\nT324 Explicit security for SQL routines\n\n\nTODO\n\n\n\n\n\n\n\n\nT325 Qualified SQL parameter references\n\n\nN/A\n\n\nBeam will likely use the qualified ones by default. Likely not exposed to user\n\n\n\n\n\n\nT326 Table functions\n\n\nTODO\n\n\n\n\n\n\n\n\nT331 Basic roles\n\n\nN/A\n\n\n\n\n\n\n\n\nT332 Extended roles\n\n\nN/A\n\n\n\n\n\n\n\n\nT341 Overleading of SQL-invoked functions and procodures\n\n\nWONTFIX\n\n\nHaskell doesn't allow overloading, and this seems complicated and unnecessary\n\n\n\n\n\n\nT351 Bracketed comments\n\n\nN/A\n\n\n\n\n\n\n\n\nT431 Extended grouping capabalities\n\n\nTODO\n\n\n\n\n\n\n\n\nT432 Nested and concatenated GROUPING SETs\n\n\nTODO\n\n\n\n\n\n\n\n\nT433 Multiargument GROUPING function\n\n\nTODO\n\n\n\n\n\n\n\n\nT434 GROUP BY DISTINCT\n\n\nTODO\n\n\n\n\n\n\n\n\nT441 ABS and MOD functions\n\n\nFULL\n\n\n\n\n\n\n\n\nT461 Symmetric BETWEEN predicate\n\n\nFULL\n\n\nBeam doesn't check this\n\n\n\n\n\n\nT471 Result sets return value\n\n\nTODO\n\n\n\n\n\n\n\n\nT472 DESCRIBE CURSOR\n\n\nN/A\n\n\nUse the backend library\n\n\n\n\n\n\nT491 LATERAL derived table\n\n\nTODO\n\n\n\n\n\n\n\n\nT495 Combined data change and retrieval\n\n\nTODO\n\n\n\n\n\n\n\n\nT501 Enhanced EXISTS predicate\n\n\nTODO\n\n\n\n\n\n\n\n\nT502 Period predicates\n\n\nTODO\n\n\n\n\n\n\n\n\nT511 Transaction counts\n\n\nTODO\n\n\n\n\n\n\n\n\nT521 Nested arguments in CALL statement\n\n\nTODO\n\n\n\n\n\n\n\n\nT522 Default values for IN parameters of SQL-invoked procs\n\n\nTODO\n\n\n\n\n\n\n\n\nT551 Optional key words for DEFAULT syntax\n\n\nTODO\n\n\n\n\n\n\n\n\nT561 Holdable locators\n\n\nTODO\n\n\n\n\n\n\n\n\nT571 Array-returning SQL-invoked functions\n\n\nTODO\n\n\nWill be supported once SQL-invoked functions are\n\n\n\n\n\n\nT572 Multiset-returning SQL-invoked functions\n\n\nTODO\n\n\n\n\n\n\n\n\nT581 Regular expression substring function\n\n\nTODO\n\n\n\n\n\n\n\n\nT591 UNIQUE constraints of possible NULL columns\n\n\nTODO\n\n\n\n\n\n\n\n\nT601 Local cursor references\n\n\nN/A\n\n\n\n\n\n\n\n\nT611 Elementary OLAP operations\n\n\nPARTIAL\n\n\nSee \nwithWindow_\n, \nwindow functions\n. Need \nnullsFirst_\n and \nnullsLast_\n\n\n\n\n\n\nT612 Advanced OLAP operations\n\n\nPARTIAL\n\n\nNeed \npercentRank_\n, \ncumeDist_\n, no exclusions yet\n\n\n\n\n\n\nT613 Sampling\n\n\nTODO\n\n\n\n\n\n\n\n\nT614 NTILE function\n\n\nTODO\n\n\n\n\n\n\n\n\nT615 LEAD and LAG function\n\n\nTODO\n\n\n\n\n\n\n\n\nT616 Null treatment for LEAD and LAG functions\n\n\nTODO\n\n\n\n\n\n\n\n\nT617 FIRST_VALUE and LAST_VALUE function\n\n\nTODO\n\n\n\n\n\n\n\n\nT618 NTH_VALUE function\n\n\nTODO\n\n\n\n\n\n\n\n\nT619 Nested window function\n\n\nTODO\n\n\n\n\n\n\n\n\nT620 WINDOW clause: GROUPS option\n\n\nTODO\n\n\n\n\n\n\n\n\nT621 Enhanced numeric functions\n\n\nTODO\n\n\n\n\n\n\n\n\nT641 Multiple column assignment\n\n\nTODO\n\n\n\n\n\n\n\n\nT651 SQL-schema statements in SQL routines\n\n\nTODO\n\n\n\n\n\n\n\n\nT652 SQL-dynamic statements in SQL routines\n\n\nTODO\n\n\n\n\n\n\n\n\nT653 SQL-schema statements in external routines\n\n\nTODO\n\n\n\n\n\n\n\n\nT654 SQL-dynamic statements in external routines\n\n\nTODO\n\n\n\n\n\n\n\n\nT655 Cyclically dependent routines\n\n\nTODO", 
            "title": "Compatibility Matrix"
        }, 
        {
            "location": "/about/license/", 
            "text": "The MIT License (MIT)\n\n\nCopyright \u00a9 2017 Travis Athougies\n\n\nPermission is hereby granted, free of charge, to any person\nobtaining a copy of this software and associated documentation\nfiles (the \u201cSoftware\u201d), to deal in the Software without\nrestriction, including without limitation the rights to use,\ncopy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the\nSoftware is furnished to do so, subject to the following\nconditions:\n\n\nThe above copyright notice and this permission notice shall be\nincluded in all copies or substantial portions of the Software.\n\n\nTHE SOFTWARE IS PROVIDED \u201cAS IS\u201d, WITHOUT WARRANTY OF ANY KIND,\nEXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES\nOF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND\nNONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT\nHOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY,\nWHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING\nFROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR\nOTHER DEALINGS IN THE SOFTWARE.", 
            "title": "License"
        }, 
        {
            "location": "/about/license/#the-mit-license-mit", 
            "text": "Copyright \u00a9 2017 Travis Athougies  Permission is hereby granted, free of charge, to any person\nobtaining a copy of this software and associated documentation\nfiles (the \u201cSoftware\u201d), to deal in the Software without\nrestriction, including without limitation the rights to use,\ncopy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the\nSoftware is furnished to do so, subject to the following\nconditions:  The above copyright notice and this permission notice shall be\nincluded in all copies or substantial portions of the Software.  THE SOFTWARE IS PROVIDED \u201cAS IS\u201d, WITHOUT WARRANTY OF ANY KIND,\nEXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES\nOF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND\nNONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT\nHOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY,\nWHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING\nFROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR\nOTHER DEALINGS IN THE SOFTWARE.", 
            "title": "The MIT License (MIT)"
        }, 
        {
            "location": "/about/release-notes/", 
            "text": "Beam Release Notes\n\n\n0.5.0.0\n\n\n\n\nMove to using finally tagless style for SQL generation\n\n\nSplit out backends from \nbeam-core\n\n\nAllow non-table entities to be stored in databases\n\n\nBasic migrations support", 
            "title": "Release Notes"
        }, 
        {
            "location": "/about/release-notes/#beam-release-notes", 
            "text": "", 
            "title": "Beam Release Notes"
        }, 
        {
            "location": "/about/release-notes/#0500", 
            "text": "Move to using finally tagless style for SQL generation  Split out backends from  beam-core  Allow non-table entities to be stored in databases  Basic migrations support", 
            "title": "0.5.0.0"
        }
    ]
}